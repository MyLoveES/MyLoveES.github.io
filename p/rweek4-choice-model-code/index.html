<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content=" R: 4.3.2 (2023-10-31) R studio: 2023.12.1+402 (2023.12.1+402)\nChoice modeling ÊòØ‰∏ÄÁßçÂ∏ÇÂú∫Á†îÁ©∂ÊñπÊ≥ïÔºåÁî®‰∫éÁêÜËß£Ê∂àË¥πËÄÖÂú®Ë¥≠‰π∞ÂÜ≥Á≠ñ‰∏≠ÂÅöÂá∫ÈÄâÊã©ÁöÑËøáÁ®ã„ÄÇËøôÁßçÊñπÊ≥ï‰ΩøÁî®Êï∞Â≠¶Ê®°ÂûãÊù•ÂàÜÊûêÊ∂àË¥πËÄÖÂ¶Ç‰ΩïÊ†πÊçÆ‰∏çÂêåÁöÑ‰∫ßÂìÅÊàñÊúçÂä°ÁâπÂæÅÂÅöÂá∫ÈÄâÊã©ÔºåÂπ∂ÈáèÂåñËøô‰∫õÈÄâÊã©ÁöÑÊ¶ÇÁéá„ÄÇÈÄöÂ∏∏ÔºåÁ†îÁ©∂ËÄÖ‰ºöËÆæËÆ°ÂÆûÈ™åÊàñË∞ÉÊü•Êù•Êî∂ÈõÜÂÖ≥‰∫éÊ∂àË¥πËÄÖÂØπ‰∏çÂêå‰∫ßÂìÅÊàñÊúçÂä°ÁöÑÂÅèÂ•ΩÂíåÈÄâÊã©ÁöÑÊï∞ÊçÆÔºåÁÑ∂Âêé‰ΩøÁî®ÈÄâÊã©Ê®°ÂûãÊù•Ëß£ÈáäËøô‰∫õÊï∞ÊçÆ„ÄÇ\nÈÄâÊã©Ê®°ÂûãÂèØ‰ª•ÊòØÂü∫‰∫éÂèÇÊï∞ÁöÑÔºåÊØîÂ¶ÇÊ¶ÇÁéáÊ®°ÂûãÔºàÂ¶ÇÈÄªËæëÂõûÂΩíÔºâ„ÄÅÂÅèÂ•ΩÊ®°ÂûãÔºàÂ¶ÇÂÅèÂ•ΩÂáΩÊï∞ÔºâÊàñÊïàÁî®Ê®°ÂûãÔºàÂ¶ÇÁ¶èÂà©ÂáΩÊï∞ÔºâÁ≠âÔºõ‰πüÂèØ‰ª•ÊòØÈùûÂèÇÊï∞ÁöÑÔºåÊØîÂ¶ÇÂÜ≥Á≠ñÊ†ë„ÄÅÈöèÊú∫Ê£ÆÊûóÁ≠âÊú∫Âô®Â≠¶‰π†ÊñπÊ≥ï„ÄÇËøô‰∫õÊ®°ÂûãËÉΩÂ§üÂ∏ÆÂä©‰ºÅ‰∏ö‰∫ÜËß£Ê∂àË¥πËÄÖÂØπ‰∫ßÂìÅÊàñÊúçÂä°ÁöÑÂÅèÂ•ΩÔºå‰ªéËÄåÊåáÂØº‰∫ßÂìÅÂÆö‰ª∑„ÄÅÂ∏ÇÂú∫ÂÆö‰Ωç„ÄÅÂπøÂëäÁ≠ñÁï•Á≠âÂÜ≥Á≠ñ„ÄÇ\nMarketers often observe yes/no outcomes:\n‚Ä¢ Did a customer purchase a product?\n‚Ä¢ Did a customer take a test drive?\n‚Ä¢ Did a customer sign up for a credit card, renew her subscription, or respond to a promotion?\nAll of these kinds of outcomes are binary because they have only two possible overserved states: yes or no. A logistic model is used to fit such outcomes.\n"><title>R[week4] Choice model code</title><link rel=canonical href=https://MyLoveES.github.io/p/rweek4-choice-model-code/><link rel=stylesheet href=/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="R[week4] Choice model code"><meta property='og:description' content=" R: 4.3.2 (2023-10-31) R studio: 2023.12.1+402 (2023.12.1+402)\nChoice modeling ÊòØ‰∏ÄÁßçÂ∏ÇÂú∫Á†îÁ©∂ÊñπÊ≥ïÔºåÁî®‰∫éÁêÜËß£Ê∂àË¥πËÄÖÂú®Ë¥≠‰π∞ÂÜ≥Á≠ñ‰∏≠ÂÅöÂá∫ÈÄâÊã©ÁöÑËøáÁ®ã„ÄÇËøôÁßçÊñπÊ≥ï‰ΩøÁî®Êï∞Â≠¶Ê®°ÂûãÊù•ÂàÜÊûêÊ∂àË¥πËÄÖÂ¶Ç‰ΩïÊ†πÊçÆ‰∏çÂêåÁöÑ‰∫ßÂìÅÊàñÊúçÂä°ÁâπÂæÅÂÅöÂá∫ÈÄâÊã©ÔºåÂπ∂ÈáèÂåñËøô‰∫õÈÄâÊã©ÁöÑÊ¶ÇÁéá„ÄÇÈÄöÂ∏∏ÔºåÁ†îÁ©∂ËÄÖ‰ºöËÆæËÆ°ÂÆûÈ™åÊàñË∞ÉÊü•Êù•Êî∂ÈõÜÂÖ≥‰∫éÊ∂àË¥πËÄÖÂØπ‰∏çÂêå‰∫ßÂìÅÊàñÊúçÂä°ÁöÑÂÅèÂ•ΩÂíåÈÄâÊã©ÁöÑÊï∞ÊçÆÔºåÁÑ∂Âêé‰ΩøÁî®ÈÄâÊã©Ê®°ÂûãÊù•Ëß£ÈáäËøô‰∫õÊï∞ÊçÆ„ÄÇ\nÈÄâÊã©Ê®°ÂûãÂèØ‰ª•ÊòØÂü∫‰∫éÂèÇÊï∞ÁöÑÔºåÊØîÂ¶ÇÊ¶ÇÁéáÊ®°ÂûãÔºàÂ¶ÇÈÄªËæëÂõûÂΩíÔºâ„ÄÅÂÅèÂ•ΩÊ®°ÂûãÔºàÂ¶ÇÂÅèÂ•ΩÂáΩÊï∞ÔºâÊàñÊïàÁî®Ê®°ÂûãÔºàÂ¶ÇÁ¶èÂà©ÂáΩÊï∞ÔºâÁ≠âÔºõ‰πüÂèØ‰ª•ÊòØÈùûÂèÇÊï∞ÁöÑÔºåÊØîÂ¶ÇÂÜ≥Á≠ñÊ†ë„ÄÅÈöèÊú∫Ê£ÆÊûóÁ≠âÊú∫Âô®Â≠¶‰π†ÊñπÊ≥ï„ÄÇËøô‰∫õÊ®°ÂûãËÉΩÂ§üÂ∏ÆÂä©‰ºÅ‰∏ö‰∫ÜËß£Ê∂àË¥πËÄÖÂØπ‰∫ßÂìÅÊàñÊúçÂä°ÁöÑÂÅèÂ•ΩÔºå‰ªéËÄåÊåáÂØº‰∫ßÂìÅÂÆö‰ª∑„ÄÅÂ∏ÇÂú∫ÂÆö‰Ωç„ÄÅÂπøÂëäÁ≠ñÁï•Á≠âÂÜ≥Á≠ñ„ÄÇ\nMarketers often observe yes/no outcomes:\n‚Ä¢ Did a customer purchase a product?\n‚Ä¢ Did a customer take a test drive?\n‚Ä¢ Did a customer sign up for a credit card, renew her subscription, or respond to a promotion?\nAll of these kinds of outcomes are binary because they have only two possible overserved states: yes or no. A logistic model is used to fit such outcomes.\n"><meta property='og:url' content='https://MyLoveES.github.io/p/rweek4-choice-model-code/'><meta property='og:site_name' content='Kunkka'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='R-Language'><meta property='article:published_time' content='2024-03-25T00:00:00+00:00'><meta property='article:modified_time' content='2024-03-25T00:00:00+00:00'><meta name=twitter:title content="R[week4] Choice model code"><meta name=twitter:description content=" R: 4.3.2 (2023-10-31) R studio: 2023.12.1+402 (2023.12.1+402)\nChoice modeling ÊòØ‰∏ÄÁßçÂ∏ÇÂú∫Á†îÁ©∂ÊñπÊ≥ïÔºåÁî®‰∫éÁêÜËß£Ê∂àË¥πËÄÖÂú®Ë¥≠‰π∞ÂÜ≥Á≠ñ‰∏≠ÂÅöÂá∫ÈÄâÊã©ÁöÑËøáÁ®ã„ÄÇËøôÁßçÊñπÊ≥ï‰ΩøÁî®Êï∞Â≠¶Ê®°ÂûãÊù•ÂàÜÊûêÊ∂àË¥πËÄÖÂ¶Ç‰ΩïÊ†πÊçÆ‰∏çÂêåÁöÑ‰∫ßÂìÅÊàñÊúçÂä°ÁâπÂæÅÂÅöÂá∫ÈÄâÊã©ÔºåÂπ∂ÈáèÂåñËøô‰∫õÈÄâÊã©ÁöÑÊ¶ÇÁéá„ÄÇÈÄöÂ∏∏ÔºåÁ†îÁ©∂ËÄÖ‰ºöËÆæËÆ°ÂÆûÈ™åÊàñË∞ÉÊü•Êù•Êî∂ÈõÜÂÖ≥‰∫éÊ∂àË¥πËÄÖÂØπ‰∏çÂêå‰∫ßÂìÅÊàñÊúçÂä°ÁöÑÂÅèÂ•ΩÂíåÈÄâÊã©ÁöÑÊï∞ÊçÆÔºåÁÑ∂Âêé‰ΩøÁî®ÈÄâÊã©Ê®°ÂûãÊù•Ëß£ÈáäËøô‰∫õÊï∞ÊçÆ„ÄÇ\nÈÄâÊã©Ê®°ÂûãÂèØ‰ª•ÊòØÂü∫‰∫éÂèÇÊï∞ÁöÑÔºåÊØîÂ¶ÇÊ¶ÇÁéáÊ®°ÂûãÔºàÂ¶ÇÈÄªËæëÂõûÂΩíÔºâ„ÄÅÂÅèÂ•ΩÊ®°ÂûãÔºàÂ¶ÇÂÅèÂ•ΩÂáΩÊï∞ÔºâÊàñÊïàÁî®Ê®°ÂûãÔºàÂ¶ÇÁ¶èÂà©ÂáΩÊï∞ÔºâÁ≠âÔºõ‰πüÂèØ‰ª•ÊòØÈùûÂèÇÊï∞ÁöÑÔºåÊØîÂ¶ÇÂÜ≥Á≠ñÊ†ë„ÄÅÈöèÊú∫Ê£ÆÊûóÁ≠âÊú∫Âô®Â≠¶‰π†ÊñπÊ≥ï„ÄÇËøô‰∫õÊ®°ÂûãËÉΩÂ§üÂ∏ÆÂä©‰ºÅ‰∏ö‰∫ÜËß£Ê∂àË¥πËÄÖÂØπ‰∫ßÂìÅÊàñÊúçÂä°ÁöÑÂÅèÂ•ΩÔºå‰ªéËÄåÊåáÂØº‰∫ßÂìÅÂÆö‰ª∑„ÄÅÂ∏ÇÂú∫ÂÆö‰Ωç„ÄÅÂπøÂëäÁ≠ñÁï•Á≠âÂÜ≥Á≠ñ„ÄÇ\nMarketers often observe yes/no outcomes:\n‚Ä¢ Did a customer purchase a product?\n‚Ä¢ Did a customer take a test drive?\n‚Ä¢ Did a customer sign up for a credit card, renew her subscription, or respond to a promotion?\nAll of these kinds of outcomes are binary because they have only two possible overserved states: yes or no. A logistic model is used to fit such outcomes.\n"><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu_c68a00bbf16dac8.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>üç•</span></figure><div class=site-meta><h1 class=site-name><a href=/>Kunkka</a></h1><h2 class=site-description>wind rises</h2></div></header><ol class=menu-social><li><a href=https://github.com/MyLoveES target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=/index.xml target=_blank title=RSS rel=me><svg class="icon icon-tabler icon-tabler-rss" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="5" cy="19" r="1"/><path d="M4 4a16 16 0 0116 16"/><path d="M4 11a9 9 0 019 9"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/about><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>About</span></a></li><li><a href=/archives><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/all-categories><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg>
<span>Categories</span></a></li><li><a href=/links><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li><a href=/all-tags><svg class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11 3l9 9a1.5 1.5.0 010 2l-6 6a1.5 1.5.0 01-2 0L3 11V7a4 4 0 014-4h4"/><circle cx="9" cy="9" r="2"/></svg>
<span>Tags</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#31-the-logit-model>3.1 The Logit Model</a></li><li><a href=#32-predicting-probabilities>3.2 Predicting probabilities</a></li><li><a href=#33-predicting-behaviour>3.3 Predicting behaviour</a></li><li><a href=#34-evaluating-the-model>3.4 Evaluating the model</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/r-language/>R-Language</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/rweek4-choice-model-code/>R[week4] Choice model code</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Mar 25, 2024</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>12 minute read</time></div></footer></div></header><section class=article-content><blockquote><p>R: 4.3.2 (2023-10-31)
R studio: 2023.12.1+402 (2023.12.1+402)</p></blockquote><p>Choice modeling ÊòØ‰∏ÄÁßçÂ∏ÇÂú∫Á†îÁ©∂ÊñπÊ≥ïÔºåÁî®‰∫éÁêÜËß£Ê∂àË¥πËÄÖÂú®Ë¥≠‰π∞ÂÜ≥Á≠ñ‰∏≠ÂÅöÂá∫ÈÄâÊã©ÁöÑËøáÁ®ã„ÄÇËøôÁßçÊñπÊ≥ï‰ΩøÁî®Êï∞Â≠¶Ê®°ÂûãÊù•ÂàÜÊûêÊ∂àË¥πËÄÖÂ¶Ç‰ΩïÊ†πÊçÆ‰∏çÂêåÁöÑ‰∫ßÂìÅÊàñÊúçÂä°ÁâπÂæÅÂÅöÂá∫ÈÄâÊã©ÔºåÂπ∂ÈáèÂåñËøô‰∫õÈÄâÊã©ÁöÑÊ¶ÇÁéá„ÄÇÈÄöÂ∏∏ÔºåÁ†îÁ©∂ËÄÖ‰ºöËÆæËÆ°ÂÆûÈ™åÊàñË∞ÉÊü•Êù•Êî∂ÈõÜÂÖ≥‰∫éÊ∂àË¥πËÄÖÂØπ‰∏çÂêå‰∫ßÂìÅÊàñÊúçÂä°ÁöÑÂÅèÂ•ΩÂíåÈÄâÊã©ÁöÑÊï∞ÊçÆÔºåÁÑ∂Âêé‰ΩøÁî®ÈÄâÊã©Ê®°ÂûãÊù•Ëß£ÈáäËøô‰∫õÊï∞ÊçÆ„ÄÇ</p><p>ÈÄâÊã©Ê®°ÂûãÂèØ‰ª•ÊòØÂü∫‰∫éÂèÇÊï∞ÁöÑÔºåÊØîÂ¶ÇÊ¶ÇÁéáÊ®°ÂûãÔºàÂ¶ÇÈÄªËæëÂõûÂΩíÔºâ„ÄÅÂÅèÂ•ΩÊ®°ÂûãÔºàÂ¶ÇÂÅèÂ•ΩÂáΩÊï∞ÔºâÊàñÊïàÁî®Ê®°ÂûãÔºàÂ¶ÇÁ¶èÂà©ÂáΩÊï∞ÔºâÁ≠âÔºõ‰πüÂèØ‰ª•ÊòØÈùûÂèÇÊï∞ÁöÑÔºåÊØîÂ¶ÇÂÜ≥Á≠ñÊ†ë„ÄÅÈöèÊú∫Ê£ÆÊûóÁ≠âÊú∫Âô®Â≠¶‰π†ÊñπÊ≥ï„ÄÇËøô‰∫õÊ®°ÂûãËÉΩÂ§üÂ∏ÆÂä©‰ºÅ‰∏ö‰∫ÜËß£Ê∂àË¥πËÄÖÂØπ‰∫ßÂìÅÊàñÊúçÂä°ÁöÑÂÅèÂ•ΩÔºå‰ªéËÄåÊåáÂØº‰∫ßÂìÅÂÆö‰ª∑„ÄÅÂ∏ÇÂú∫ÂÆö‰Ωç„ÄÅÂπøÂëäÁ≠ñÁï•Á≠âÂÜ≥Á≠ñ„ÄÇ</p><p>Marketers often observe yes/no outcomes:<br>‚Ä¢ Did a customer purchase a product?<br>‚Ä¢ Did a customer take a test drive?<br>‚Ä¢ Did a customer sign up for a credit card, renew her subscription, or respond to a promotion?<br>All of these kinds of outcomes are binary because they have only two possible overserved states: yes or no. A logistic model is used to fit such outcomes.</p><p><strong>Ëøô‰∫õÁ±ªÂûãÁöÑÁªìÊûúÈÉΩÊòØ‰∫åÂÖÉÁöÑÔºåÂÆÉ‰ª¨Âè™Êúâ‰∏§ÁßçÂèØËÉΩÁöÑÁä∂ÊÄÅÔºöÊòØÊàñÂê¶„ÄÇ logisticÊ®°ÂûãË¢´Áî®Êù•ÊãüÂêàËøôÊ†∑ÁöÑÁªìÊûú„ÄÇ</strong></p><h1 id=1-basics-of-logistic-regression>1. Basics of logistic regression</h1><p>The core feature of a logistic model is that it relates the probability of an outcome to an exponential function of a predictor variable.<br>By modelling the probability of an outcome, a logistic model accomplishes two things:<br>‚Ä¢ First, it more directly models what we are interested in, which is a probability or proportion, such as the likelihood of a given customer to purchase a product or the expected proportion of a segment who will respond to a promotion.<br>‚Ä¢ Second, it limits the model to the appropriate range for a proportion, which is [0, 1]. A basic linear model, as generated with lm(), does not have such a limit. The equation for the logistic function is:</p>$$
p(y) = \frac{e^{v_x}}{e^{v_x} + 1}
$$<p>LogisticÊ®°ÂûãÁöÑÊ†∏ÂøÉÁâπÂæÅÊòØÂÆÉÂ∞ÜÁªìÊûúÁöÑÊ¶ÇÁéá‰∏éÈ¢ÑÊµãÂèòÈáèÁöÑÊåáÊï∞ÂáΩÊï∞Áõ∏ÂÖ≥ËÅî„ÄÇ<br>ÈÄöËøáÂØπÁªìÊûúÁöÑÊ¶ÇÁéáÂª∫Ê®°ÔºålogisticÊ®°ÂûãÂÆûÁé∞‰∫Ü‰∏§‰∏™ÁõÆÊ†á„ÄÇ<br>‚Ä¢ È¶ñÂÖàÔºåÂÆÉÊõ¥Áõ¥Êé•Âú∞ÂØπÊàë‰ª¨ÊÑüÂÖ¥Ë∂£ÁöÑÂÜÖÂÆπËøõË°åÂª∫Ê®°ÔºåÂç≥Ê¶ÇÁéáÊàñÊØî‰æãÔºå‰æãÂ¶ÇÁªôÂÆöÂÆ¢Êà∑Ë¥≠‰π∞‰∫ßÂìÅÁöÑÂèØËÉΩÊÄßÊàñÂ∞ÜÂØπ‰øÉÈîÄÊ¥ªÂä®ÂÅöÂá∫ÂõûÂ∫îÁöÑÁªÜÂàÜÈ¢ÑÊúüÊØî‰æã„ÄÇ<br>‚Ä¢ ÂÖ∂Ê¨°ÔºåÂÆÉÂ∞ÜÊ®°ÂûãÈôêÂà∂Âú®ÊØî‰æãÁöÑÈÄÇÂΩìËåÉÂõ¥ÂÜÖÔºåÂç≥[0,1]„ÄÇÂü∫Êú¨ÁöÑÁ∫øÊÄßÊ®°ÂûãÔºåÂ¶Çlm()ÁîüÊàêÁöÑÊ®°ÂûãÔºåÊ≤°ÊúâËøôÊ†∑ÁöÑÈôêÂà∂„ÄÇ</p><p>In this equation, the outcome of interest is y, and we compute its likelihood p(y) as a function of vx. We typically estimate vx as a function of the features (x) of a product, such as price. vx can take any real value, so we are able to treat it as a continuous function in a linear model. In that case, vx is composed of one or more coefficients of the model and indicates the importance of the corresponding features of the product.</p><p>Âú®Ëøô‰∏™ÊñπÁ®ã‰∏≠ÔºåÊàë‰ª¨ÊÑüÂÖ¥Ë∂£ÁöÑÁªìÊûúÊòØyÔºåÊàë‰ª¨ËÆ°ÁÆóÂÖ∂Ê¶ÇÁéáp(y)‰Ωú‰∏∫vxÁöÑÂáΩÊï∞„ÄÇÊàë‰ª¨ÈÄöÂ∏∏Â∞Üvx‰º∞ËÆ°‰∏∫‰∫ßÂìÅÁâπÂæÅÔºàxÔºâÁöÑÂáΩÊï∞Ôºå‰æãÂ¶Ç‰ª∑Ê†º„ÄÇvxÂèØ‰ª•Âèñ‰ªª‰ΩïÂÆûÊï∞ÂÄºÔºåÂõ†Ê≠§Êàë‰ª¨ÂèØ‰ª•Â∞ÜÂÖ∂ËßÜ‰∏∫Á∫øÊÄßÊ®°Âûã‰∏≠ÁöÑËøûÁª≠ÂáΩÊï∞„ÄÇÂú®ËøôÁßçÊÉÖÂÜµ‰∏ãÔºåvxÁî±Ê®°ÂûãÁöÑ‰∏Ä‰∏™ÊàñÂ§ö‰∏™Á≥ªÊï∞ÁªÑÊàêÔºåÂπ∂ÊåáÁ§∫‰∫ßÂìÅÁõ∏Â∫îÁâπÂæÅÁöÑÈáçË¶ÅÊÄß„ÄÇ</p><p>The formula gives a value between [0, 1]. The likelihood of y is less than 50% when vx is negative, is 50% when vx = 0 and is above 50% when vx is positive. We compute this first by hand and then switch to the equivalent plogis() function:</p><p>Ëøô‰∏™ÂÖ¨ÂºèÁªôÂá∫‰∫Ü‰∏Ä‰∏™Âú®[0, 1]‰πãÈó¥ÁöÑÂÄº„ÄÇÂΩìvx‰∏∫Ë¥üÊó∂ÔºåyÁöÑÊ¶ÇÁéáÂ∞è‰∫é50ÔºÖÔºåÂΩìvx = 0Êó∂ÔºåÊ¶ÇÁéá‰∏∫50ÔºÖÔºåÂΩìvx‰∏∫Ê≠£Êó∂ÔºåÊ¶ÇÁéáÂ§ß‰∫é50ÔºÖ„ÄÇÊàë‰ª¨È¶ñÂÖàÊâãÂ∑•ËÆ°ÁÆóËøô‰∏™ÂÄºÔºåÁÑ∂ÂêéÂàáÊç¢Âà∞Á≠âÊïàÁöÑplogis()ÂáΩÊï∞Ôºö</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>&gt; exp(0) / exp(0)+1 # computing logistic by hand, or using plogis()
</span></span><span class=line><span class=cl>‚Ä¢[1] 2
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl># plogisÂèÇÊï∞ÂÖ∂ÂÆûÂ∞±ÊòØp(y)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>&gt; plogis(-Inf) #infinitely low = likelihood 0
</span></span><span class=line><span class=cl>[1] 0
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>&gt; plogis(2) #moderate probability = 88% chance of outcome
</span></span><span class=line><span class=cl>[1] 0.8807971
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>&gt; plogis(-0.2) # weak likelihood
</span></span><span class=line><span class=cl>[1] 0.450166
</span></span></code></pre></td></tr></table></div></div><div style=background-color:#f0f0f0;padding:10px><blockquote><p>plogis():</p></blockquote><p><code>plogis()</code> ÂáΩÊï∞ÊòØ R ËØ≠Ë®Ä‰∏≠Áî®‰∫éËÆ°ÁÆóÈÄªËæëÂáΩÊï∞Ôºàlogistic functionÔºâÁöÑÂáΩÊï∞„ÄÇ</p><p>ÈÄªËæëÂáΩÊï∞ÁöÑÂÆö‰πâÂ¶Ç‰∏ãÊâÄÁ§∫Ôºö</p>$$
\text{logistic}(x) = \frac{1}{1 + e^{-x}}
$$<p>ÂÖ∂‰∏≠Ôºå\(x\) ÊòØÈÄªËæëÂáΩÊï∞ÁöÑËæìÂÖ•ÂÄº„ÄÇ<code>plogis()</code> ÂáΩÊï∞Êé•Âèó‰∏Ä‰∏™ÂèÇÊï∞ \(x\)ÔºåË°®Á§∫ÈÄªËæëÂáΩÊï∞ÁöÑËæìÂÖ•ÂÄºÔºåÁÑ∂ÂêéËøîÂõûÈÄªËæëÂáΩÊï∞ÁöÑÂÄº„ÄÇËøô‰∏™ÂáΩÊï∞ÈÄöÂ∏∏Áî®‰∫éÈÄªËæëÂõûÂΩíÊ®°Âûã‰∏≠ÔºåÂ∞ÜÁ∫øÊÄßÈ¢ÑÊµãÂÄºËΩ¨Êç¢‰∏∫0Âà∞1‰πãÈó¥ÁöÑÊ¶ÇÁéáÂÄº„ÄÇ</p><p>Âú® R ‰∏≠Ôºå‰Ω†ÂèØ‰ª•‰ΩøÁî® <code>plogis()</code> ÂáΩÊï∞Êù•ËÆ°ÁÆóÈÄªËæëÂáΩÊï∞ÁöÑÂÄº„ÄÇ‰æãÂ¶ÇÔºö</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl># ËÆ°ÁÆóÈÄªËæëÂáΩÊï∞ÂÄº
</span></span><span class=line><span class=cl>x &lt;- 2
</span></span><span class=line><span class=cl>probability &lt;- plogis(x)
</span></span><span class=line><span class=cl>print(probability)
</span></span></code></pre></td></tr></table></div></div></div><p>Such a model is known as a logit model, which determines the value of vx from the logarithm of the relative probability of occurence of y:</p>$$
v_x = \log \left( \frac{p(y)}{1 - p(y)} \right)
$$<div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>&gt; log(0.88 / (1-0.88)) # moderate high likelihood
</span></span><span class=line><span class=cl>[1] 1.99243
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>&gt; qlogis(0.88) # equivalent to hand computation
</span></span><span class=line><span class=cl>[1] 1.99243
</span></span></code></pre></td></tr></table></div></div><div style=background-color:#f0f0f0;padding:10px><blockquote><p>qlogis()</p></blockquote><p><code>qlogis()</code> ÂáΩÊï∞ÊòØ R ËØ≠Ë®Ä‰∏≠Áî®‰∫éËÆ°ÁÆóÈÄªËæëÂáΩÊï∞ÁöÑÂèçÂáΩÊï∞ÁöÑÂáΩÊï∞„ÄÇ</p><p>ÈÄªËæëÂáΩÊï∞ÁöÑÂèçÂáΩÊï∞ÈÄöÂ∏∏Áß∞‰∏∫ÈÄÜÈÄªËæëÂáΩÊï∞ÔºåÂÖ∂ÂÆö‰πâÂ¶Ç‰∏ãÊâÄÁ§∫Ôºö</p>$$
\text{logit}(p) = \log\left(\frac{p}{1 - p}\right)
$$<p>ÂÖ∂‰∏≠Ôºå\( p \) ÊòØÈÄªËæëÂáΩÊï∞ÁöÑËæìÂá∫ÂÄºÔºåÂç≥Ê¶ÇÁéáÂÄº„ÄÇ</p><p><code>qlogis()</code> ÂáΩÊï∞Êé•Âèó‰∏Ä‰∏™ÂèÇÊï∞ \( p \)ÔºåË°®Á§∫ÈÄªËæëÂáΩÊï∞ÁöÑËæìÂá∫ÂÄºÔºàÂç≥Ê¶ÇÁéáÂÄºÔºâÔºåÁÑ∂ÂêéËøîÂõûÈÄÜÈÄªËæëÂáΩÊï∞ÁöÑÂÄº„ÄÇËøô‰∏™ÂáΩÊï∞ÈÄöÂ∏∏Áî®‰∫é‰ªéÈÄªËæëÂáΩÊï∞ÁöÑÊ¶ÇÁéáÂÄº‰∏≠ÂèçÊé®Âá∫Á∫øÊÄßÈ¢ÑÊµãÂÄº„ÄÇ</p><p>Âú® R ‰∏≠Ôºå‰Ω†ÂèØ‰ª•‰ΩøÁî® <code>qlogis()</code> ÂáΩÊï∞Êù•ËÆ°ÁÆóÈÄÜÈÄªËæëÂáΩÊï∞ÁöÑÂÄº„ÄÇ‰æãÂ¶ÇÔºö</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-R data-lang=R><span class=line><span class=cl><span class=c1># ËÆ°ÁÆóÈÄÜÈÄªËæëÂáΩÊï∞ÂÄº</span>
</span></span><span class=line><span class=cl><span class=n>p</span> <span class=o>&lt;-</span> <span class=m>0.7</span>
</span></span><span class=line><span class=cl><span class=n>linear_pred</span> <span class=o>&lt;-</span> <span class=nf>qlogis</span><span class=p>(</span><span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nf>print</span><span class=p>(</span><span class=n>linear_pred</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div></div><h1 id=2-generalised-linear-model-glm>2. Generalised linear model (GLM)</h1><p>A logistic regression model in R is fitted as a generalised linear model (GLM) using a process similar to linear regression with lm(), but with the difference that a GLM can handle dependent variables that are not normally distributed. Thus, GLM can be used to model data counts (such as the number of purchases), time intervals (such as time spent on a website), or binary variables (e.g., did/didn‚Äôt purchase). The common feature of all GLM models is that they relate normally distributed predictors to a non-normal outcome using a function known as a link. This means that they are able to fit models for many different distributions using a single, consistent framework.</p><p>Âú®R‰∏≠ÔºåÈÄªËæëÂõûÂΩíÊ®°ÂûãÊòØ‰Ωú‰∏∫Âπø‰πâÁ∫øÊÄßÊ®°ÂûãÔºàGLMÔºâËøõË°åÊãüÂêàÁöÑÔºå‰ΩøÁî®ÁöÑËøáÁ®ãÁ±ª‰ºº‰∫é‰ΩøÁî®lm()ËøõË°åÁ∫øÊÄßÂõûÂΩíÔºå‰ΩÜ‰∏çÂêå‰πãÂ§ÑÂú®‰∫éGLMÂèØ‰ª•Â§ÑÁêÜ‰∏çÁ¨¶ÂêàÊ≠£ÊÄÅÂàÜÂ∏ÉÁöÑÂõ†ÂèòÈáè„ÄÇÂõ†Ê≠§ÔºåGLMÂèØÁî®‰∫éÂØπÊï∞ÊçÆËÆ°Êï∞Ôºà‰æãÂ¶ÇË¥≠‰π∞Ê¨°Êï∞Ôºâ„ÄÅÊó∂Èó¥Èó¥ÈöîÔºà‰æãÂ¶ÇÂú®ÁΩëÁ´ô‰∏äÁöÑÂÅúÁïôÊó∂Èó¥ÔºâÊàñ‰∫åÂÖÉÂèòÈáèÔºà‰æãÂ¶ÇÊòØÂê¶Ë¥≠‰π∞ÔºâÂª∫Ê®°„ÄÇÊâÄÊúâGLMÊ®°ÂûãÁöÑÂÖ±ÂêåÁâπÁÇπÊòØÂÆÉ‰ª¨Â∞ÜÊ≠£ÊÄÅÂàÜÂ∏ÉÁöÑÈ¢ÑÊµãÂèòÈáè‰∏é‰∏Ä‰∏™ÈùûÊ≠£ÊÄÅÁöÑÁªìÊûúÁõ∏ÂÖ≥ËÅîÔºå‰ΩøÁî®ÁöÑÂáΩÊï∞Áß∞‰∏∫ÈìæÊé•ÂáΩÊï∞„ÄÇËøôÊÑèÂë≥ÁùÄÂÆÉ‰ª¨ËÉΩÂ§ü‰ΩøÁî®Âçï‰∏Ä„ÄÅ‰∏ÄËá¥ÁöÑÊ°ÜÊû∂ÊãüÂêàËÆ∏Â§ö‰∏çÂêåÂàÜÂ∏ÉÁöÑÊ®°Âûã„ÄÇ</p><div style=background-color:#f0f0f0;padding:10px><p>Âπø‰πâÁ∫øÊÄßÊ®°ÂûãÔºàGeneralized Linear ModelÔºåGLMÔºâÊòØ‰∏ÄÁßçÂπøÊ≥õÂ∫îÁî®‰∫éÁªüËÆ°ÂàÜÊûê‰∏≠ÁöÑÊ®°ÂûãÔºåÂÆÉÂ∞ÜÁ∫øÊÄßÊ®°ÂûãÊâ©Â±ïÂà∞‰∫ÜÊõ¥ÂπøÊ≥õÁöÑÊï∞ÊçÆÁ±ªÂûãÂíåÂàÜÂ∏É„ÄÇGLMÂèØ‰ª•Â§ÑÁêÜ‰∏çÂêåÁ±ªÂûãÁöÑÂìçÂ∫îÂèòÈáèÔºåÂåÖÊã¨‰∫åÈ°πÂàÜÂ∏É„ÄÅÊ≥äÊùæÂàÜÂ∏É„ÄÅÊ≠£ÊÄÅÂàÜÂ∏ÉÁ≠âÔºåÂπ∂‰∏îÂèØ‰ª•Â§ÑÁêÜ‰∏çÂêåÁöÑÈìæÊé•ÂáΩÊï∞ÔºåÂ¶ÇÊÅíÁ≠âÂáΩÊï∞„ÄÅÂØπÊï∞ÂáΩÊï∞„ÄÅÈÄªËæëÊñØËíÇÂáΩÊï∞Á≠â„ÄÇ</p><p>GLMÁöÑÂü∫Êú¨ÂΩ¢ÂºèÂ¶Ç‰∏ãÔºö</p><ol><li>Á∫øÊÄßÈÉ®ÂàÜÔºö</li></ol>$$
\eta = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p
$$<p>ËøôÈÉ®ÂàÜ‰∏éÂ§öÂÖÉÁ∫øÊÄßÂõûÂΩíÊ®°ÂûãÁõ∏‰ººÔºåÂÖ∂‰∏≠ $ \eta $ ÊòØÁ∫øÊÄßÈ¢ÑÊµãÂÄºÔºå$ \beta_0 $ , $ \beta_1 $ , $ \ldots $, $ \beta_p $ ÊòØÁ≥ªÊï∞Ôºå$ x_1 $ , $ x_2 $ , $ \ldots $, $ x_p $ ÊòØÈ¢ÑÊµãÂèòÈáè„ÄÇ</p><ol start=2><li>ÈìæÊé•ÂáΩÊï∞Ôºö</li></ol>$$ g(\mu) = \eta $$<p>ËøôÈáåÁöÑ $ g(\cdot) $ ÊòØÈìæÊé•ÂáΩÊï∞ÔºåÂÆÉÂÆö‰πâ‰∫ÜÈ¢ÑÊµãÂèòÈáè $ \eta $ ‰∏éÂìçÂ∫îÂèòÈáè $ \mu $ ‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÈìæÊé•ÂáΩÊï∞ÈÄöÂ∏∏Ê†πÊçÆÂìçÂ∫îÂèòÈáèÁöÑÁ±ªÂûãÈÄâÊã©ÔºåÂ¶ÇÂØπÊï∞ÈìæÊé•ÂáΩÊï∞Áî®‰∫éÂ§ÑÁêÜÊ≥äÊùæÂàÜÂ∏ÉÁöÑÂìçÂ∫îÂèòÈáèÔºåÈÄªËæëÊñØËíÇÈìæÊé•ÂáΩÊï∞Áî®‰∫éÂ§ÑÁêÜ‰∫åÈ°πÂàÜÂ∏ÉÁöÑÂìçÂ∫îÂèòÈáèÁ≠â„ÄÇ</p><ol start=3><li>ÂàÜÂ∏ÉÊóèÔºö</li></ol>$$ Y \sim \text{Dist}(\mu) $$<p>ËøôÈáåÁöÑ $ \text{Dist}(\mu) $ Ë°®Á§∫ÂìçÂ∫îÂèòÈáè Y ÁöÑÂàÜÂ∏ÉÊóèÔºå$ \mu $ ÊòØÂìçÂ∫îÂèòÈáèÁöÑÂùáÂÄº„ÄÇ</p><p>GLMÁöÑ‰ºòÂäøÂú®‰∫éÂÆÉÁöÑÁÅµÊ¥ªÊÄßÂíåÈÄÇÁî®ÊÄßÔºåÂèØ‰ª•ÈÄÇÂ∫î‰∏çÂêåÁ±ªÂûãÂíåÂàÜÂ∏ÉÁöÑÊï∞ÊçÆÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÂØπÂèÇÊï∞ÁöÑËß£ÈáäÊÄß„ÄÇÂÆÉÂú®ËÆ∏Â§öÈ¢ÜÂüüÈÉΩÂæóÂà∞‰∫ÜÂπøÊ≥õÂ∫îÁî®ÔºåÂåÖÊã¨ÁîüÁâ©ÁªüËÆ°Â≠¶„ÄÅÂåªÂ≠¶„ÄÅÁ§æ‰ºöÁßëÂ≠¶Á≠â„ÄÇ</p></div><h1 id=3-rfm-recency-frequency-monetary>3. RFM (recency, frequency, monetary)</h1><p>RFM is a method used for analyzing customer value. RFM stands for the three dimensions: Recency: How recently did the customer purchase? Frequency: How often do they purchase? Monetary Value: How much do they spend?</p><p>RFMÊòØÁî®‰∫éÂàÜÊûêÂÆ¢Êà∑‰ª∑ÂÄºÁöÑ‰∏ÄÁßçÊñπÊ≥ï„ÄÇRFM‰ª£Ë°®‰∏â‰∏™Áª¥Â∫¶ÔºöRecencyÔºàÊúÄËøëË¥≠‰π∞Êó∂Èó¥ÔºâÔºöÂÆ¢Êà∑ÊúÄËøë‰∏ÄÊ¨°Ë¥≠‰π∞ÊòØÂú®Â§ö‰πÖ‰πãÂâçÔºüFrequencyÔºàË¥≠‰π∞È¢ëÁéáÔºâÔºö‰ªñ‰ª¨Ë¥≠‰π∞ÁöÑÈ¢ëÁéáÂ¶Ç‰ΩïÔºüMonetary ValueÔºàË¥≠‰π∞ÈáëÈ¢ùÔºâÔºö‰ªñ‰ª¨ÁöÑÊ∂àË¥πÈáëÈ¢ùÊòØÂ§öÂ∞ëÔºü</p><h2 id=31-the-logit-model>3.1 The Logit Model</h2><p>The logit model restricts the output values to lie in [0, 1] intervals.<br>Specifically, it expresses the probability of purchase by customer i as a function of coefficients Œ≤0:3 and variables in the following manner:</p><p>ÈÄªËæëÊñØËíÇÊ®°ÂûãÂ∞ÜËæìÂá∫ÂÄºÈôêÂà∂Âú®[0, 1]ÁöÑÂå∫Èó¥ÂÜÖ„ÄÇ
ÂÖ∑‰ΩìËÄåË®ÄÔºåÂÆÉÂ∞ÜÂÆ¢Êà∑iÁöÑË¥≠‰π∞Ê¶ÇÁéáË°®Ëææ‰∏∫Á≥ªÊï∞Œ≤0:3Âíå‰ª•‰∏ãÂèòÈáèÁöÑÂáΩÊï∞Ôºö</p>$$
P(Purchase_i) = \frac{exp(\beta_0 + \beta_1 \text{Recency}_i + \beta_2 \text{Frequency}_i + \beta_3 \text{Monetary}_i)}{exp(\beta_0 + \beta_1 \text{Recency}_i + \beta_2 \text{Frequency}_i + \beta_3 \text{Monetary}_i) + 1}
$$<div style=background-color:#f0f0f0;padding:10px><p>Ëøô‰∏™ÂÖ¨ÂºèÊòØ‰∏Ä‰∏™ÈÄªËæëÂõûÂΩíÊ®°Âûã‰∏≠Áî®‰∫éËÆ°ÁÆóË¥≠‰π∞Ê¶ÇÁéáÁöÑÊñπÁ®ã„ÄÇÂú®Ëøô‰∏™ÊñπÁ®ã‰∏≠Ôºö</p><ul><li>$ P(Purchase_i) $ Ë°®Á§∫Á¨¨ i ‰∏™‰∏™‰ΩìË¥≠‰π∞ÁöÑÊ¶ÇÁéá„ÄÇ</li><li>$ \beta_0 $, $ \beta_1 $, $ \beta_2 $, $ \beta_3 $ ÊòØÊ®°ÂûãÁöÑÂèÇÊï∞ÔºåÂàÜÂà´Ë°®Á§∫Êà™Ë∑ùÂíå‰∏éÊØè‰∏™È¢ÑÊµãÂèòÈáèÔºàRecency„ÄÅFrequency„ÄÅMonetaryÔºâÁõ∏ÂÖ≥ÁöÑÁ≥ªÊï∞„ÄÇ</li><li>$ \text{Recency}_i $, $ \text{Frequency}_i $, $ \text{Monetary}_i $ ÊòØÁ¨¨ i ‰∏™‰∏™‰ΩìÁöÑÈ¢ÑÊµãÂèòÈáèÂÄºÔºåÂàÜÂà´Ë°®Á§∫ÊúÄËøë‰∏ÄÊ¨°Ë¥≠‰π∞Ë∑ùÁ¶ª„ÄÅË¥≠‰π∞È¢ëÁéáÂíåË¥≠‰π∞ÈáëÈ¢ù„ÄÇ</li></ul><p>ÂÖ¨ÂºèÁöÑÂàÜÂ≠êÈÉ®ÂàÜË°®Á§∫‰∫Ü‰∏Ä‰∏™Á∫øÊÄßÁªÑÂêà$ (\beta_0 + \beta_1 \text{Recency}_i + \beta_2 \text{Frequency}_i + \beta_3 \text{Monetary}_i) $ ÁöÑÊåáÊï∞ÂΩ¢ÂºèÔºåÂç≥ÊåáÊï∞ÂáΩÊï∞ $ \text{exp}(\ldots) $ Ôºå‰ª£Ë°®‰∫ÜË¥≠‰π∞ÁöÑÂèØËÉΩÊÄß„ÄÇ</p><p>ÂàÜÊØçÈÉ®ÂàÜÊòØÂàÜÂ≠êÈÉ®ÂàÜÂä†‰∏ä1ÔºåËøôÊòØÁî±‰∫éÈÄªËæëÂõûÂΩíÊ®°ÂûãÁöÑÂΩ¢ÂºèÔºå‰øùËØÅ‰∫ÜÊ¶ÇÁéáÂÄºÂú®0Âíå1‰πãÈó¥„ÄÇÊï¥‰∏™ÊñπÁ®ãÂÆûÈôÖ‰∏äÊòØÈÄªËæëÂõûÂΩíÊ®°ÂûãÁöÑÈÄªËæëÂáΩÊï∞Ôºàlogistic functionÔºâÔºåÂÆÉÂ∞ÜÁ∫øÊÄßÈ¢ÑÊµãÂÄºËΩ¨Êç¢‰∏∫0Âà∞1‰πãÈó¥ÁöÑÊ¶ÇÁéáÂÄºÔºåËøôË°®Á§∫‰∏™‰ΩìË¥≠‰π∞ÁöÑÊ¶ÇÁéá„ÄÇ</p></div><p>Intuitively, the utility of choosing to buy is:</p>$$ V_{bi} = \beta_0 + \beta_1 \text{Recency}_i + \beta_2 \text{Frequency}_i + \beta_3 \text{Monetary}_i $$<div style=background-color:#f0f0f0;padding:10px><p>Ëøô‰∏™ÂÖ¨ÂºèË°®Á§∫‰∫Ü‰∏Ä‰∏™Á∫øÊÄßÊ®°ÂûãÔºåÁî®‰∫éÈ¢ÑÊµã‰∏™‰Ωì i ÁöÑ $ V $ ÂÄº„ÄÇÂú®Ëøô‰∏™ÂÖ¨Âºè‰∏≠Ôºö</p><ul><li>$ V_{bi} $ Ë°®Á§∫‰∏™‰Ωì $ i $ ÁöÑ $ V $ ÂÄº„ÄÇ</li><li>$ \beta_0, \beta_1, \beta_2, \beta_3 $ ÊòØÊ®°ÂûãÁöÑÂèÇÊï∞ÔºåÂàÜÂà´Ë°®Á§∫Êà™Ë∑ùÂíå‰∏éÊØè‰∏™È¢ÑÊµãÂèòÈáèÔºàRecency„ÄÅFrequency„ÄÅMonetaryÔºâÁõ∏ÂÖ≥ÁöÑÁ≥ªÊï∞„ÄÇ</li><li>$ \text{Recency}_i, \text{Frequency}_i, \text{Monetary}_i $ Á¨¨ $ i $ ‰∏™‰∏™‰ΩìÁöÑÈ¢ÑÊµãÂèòÈáèÂÄºÔºåÂàÜÂà´Ë°®Á§∫ÊúÄËøë‰∏ÄÊ¨°Ë¥≠‰π∞Ë∑ùÁ¶ª„ÄÅË¥≠‰π∞È¢ëÁéáÂíåË¥≠‰π∞ÈáëÈ¢ù„ÄÇ</li></ul><p>Ëøô‰∏™Ê®°ÂûãÁöÑÁõÆÁöÑÊòØÈÄöËøá‰∏™‰ΩìÁöÑË¥≠‰π∞Ë°å‰∏∫ÁöÑÁõ∏ÂÖ≥ÁâπÂæÅÔºàRecency„ÄÅFrequency„ÄÅMonetaryÔºâÊù•È¢ÑÊµã‰ªñ‰ª¨ÁöÑ $ V $ ÂÄº„ÄÇËøô‰∏™ $ V $ ÂÄºÂèØËÉΩË°®Á§∫‰∏™‰ΩìÁöÑÊΩúÂú®‰ª∑ÂÄºÊàñÂÖ∂‰ªñÁõ∏ÂÖ≥ÁöÑÊåáÊ†á„ÄÇ</p><p>whereas utility of choosing not to buy is normalized to zero $ V_ni = 0 $, so $ exp(V_n) = exp(0) = 1 $ in the fraction above.<br>With the given formulation, we can estimate values $ \beta_0:3 $ that fit the data best. We use glm() of family=‚Äúbinomial‚Äù.</p><p>ÈÄâÊã©‰∏çË¥≠‰π∞ÁöÑÊïàÁî®Ë¢´ÂΩí‰∏ÄÂåñ‰∏∫Èõ∂ÔºåÂç≥ Vni = 0ÔºåÂõ†Ê≠§Âú®‰∏äËø∞ÂàÜÊï∞‰∏≠ exp(Vn) = exp(0) = 1„ÄÇ<br>ÈÄöËøáÁªôÂÆöÁöÑÂÖ¨ÂºèÔºåÊàë‰ª¨ÂèØ‰ª•‰º∞ËÆ°ÊúÄÈÄÇÂêàÊï∞ÊçÆÁöÑ Œ≤0:3 ÂÄº„ÄÇÊàë‰ª¨‰ΩøÁî® glm() ‰∏≠ÁöÑ family=&ldquo;binomial&rdquo;„ÄÇ</p></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>&gt; RFMdata &lt;- read.csv(file = &#34;RFMData.csv&#34;,row.names=1) 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>&gt; head(RFMdata,5)
</span></span><span class=line><span class=cl>  Recency Frequency Monetary Purchase
</span></span><span class=line><span class=cl>1     120         7    41.66        0
</span></span><span class=line><span class=cl>2      90         9    46.71        0
</span></span><span class=line><span class=cl>3     120         6   103.99        1
</span></span><span class=line><span class=cl>4     270        17    37.13        1
</span></span><span class=line><span class=cl>5      60         5    88.92        0
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>&gt; model &lt;- glm(Purchase~Recency+Frequency+Monetary, data=RFMdata, family = &#34;binomial&#34;) 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>&gt; output &lt;- cbind(coef(summary(model))[, 1:4],exp(coef(model)))
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>&gt; colnames(output) &lt;- c(&#34;beta&#34;,&#34;SE&#34;,&#34;z val.&#34;,&#34;Pr(&gt;|z|)&#34;,&#39;exp(beta)&#39;) 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>&gt; kable(output,caption = &#34;Logistic regression estimates&#34;)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Table: Logistic regression estimates
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>|            |        beta|        SE|    z val.| Pr(&gt;&amp;#124;z&amp;#124;)| exp(beta)|
</span></span><span class=line><span class=cl>|:-----------|-----------:|---------:|---------:|------------------:|---------:|
</span></span><span class=line><span class=cl>|(Intercept) | -30.2976692| 8.5522913| -3.542638|          0.0003961|  0.000000|
</span></span><span class=line><span class=cl>|Recency     |   0.1114175| 0.0309797|  3.596464|          0.0003226|  1.117862|
</span></span><span class=line><span class=cl>|Frequency   |   0.5941268| 0.2429393|  2.445577|          0.0144620|  1.811448|
</span></span><span class=line><span class=cl>|Monetary    |   0.1677054| 0.0465645|  3.601572|          0.0003163|  1.182588|
</span></span></code></pre></td></tr></table></div></div><blockquote><p>glm()</p></blockquote><div style=background-color:#f0f0f0;padding:10px>glm() ÂáΩÊï∞ÊòØ R ËØ≠Ë®Ä‰∏≠ÁöÑ‰∏Ä‰∏™ÈáçË¶ÅÂáΩÊï∞ÔºåÁî®‰∫éÊãüÂêàÂπø‰πâÁ∫øÊÄßÊ®°ÂûãÔºàGeneralized Linear ModelsÔºâ„ÄÇÂπø‰πâÁ∫øÊÄßÊ®°ÂûãÊòØÁ∫øÊÄßÊ®°ÂûãÁöÑÊâ©Â±ïÔºåÂÖÅËÆ∏Âõ†ÂèòÈáèÊúç‰ªé‰∏çÂêåÁöÑÂàÜÂ∏ÉÔºåËÄå‰∏ç‰ªÖ‰ªÖÊòØÊ≠£ÊÄÅÂàÜÂ∏É„ÄÇËøô‰ΩøÂæóÂπø‰πâÁ∫øÊÄßÊ®°ÂûãÈÄÇÁî®‰∫éÊõ¥ÂπøÊ≥õÁöÑÊï∞ÊçÆÁ±ªÂûãÔºåÂåÖÊã¨‰∫åÈ°πÂàÜÂ∏ÉÔºà‰∫åÂÖÉÈÄªËæëÂõûÂΩíÔºâ„ÄÅÊ≥äÊùæÂàÜÂ∏ÉÔºàËÆ°Êï∞Êï∞ÊçÆÔºâ„ÄÅÂ§öÈ°πÂàÜÂ∏ÉÔºàÂ§öÁ±ªÂà´ÂàÜÁ±ªÔºâÁ≠â„ÄÇ</div><blockquote><p>cbind()</p></blockquote><div style=background-color:#f0f0f0;padding:10px>cbind() ÂáΩÊï∞ÊòØ R ËØ≠Ë®Ä‰∏≠ÁöÑ‰∏Ä‰∏™Âü∫Á°ÄÂáΩÊï∞ÔºåÁî®‰∫éÊåâÂàóÂêàÂπ∂Â§ö‰∏™ÂØπË±°ÔºàÈÄöÂ∏∏ÊòØÂêëÈáè„ÄÅÁü©ÈòµÊàñÊï∞ÊçÆÊ°ÜÔºâ„ÄÇcbind ÊòØ "column bind" ÁöÑÁº©ÂÜôÔºåË°®Á§∫ÊåâÂàóÂêàÂπ∂„ÄÇ</div><blockquote><p>kable()</p></blockquote><div style=background-color:#f0f0f0;padding:10px>kable() ÂáΩÊï∞ÊòØ R ËØ≠Ë®Ä‰∏≠ knitr Âíå rmarkdown ÂåÖ‰∏≠ÁöÑ‰∏Ä‰∏™ÂáΩÊï∞ÔºåÁî®‰∫éÁîüÊàêÁæéËßÇÁöÑË°®Ê†ºËæìÂá∫„ÄÇÂÆÉËÉΩÂ§üÂ∞Ü R ‰∏≠ÁöÑÊï∞ÊçÆÊ°Ü„ÄÅÁü©ÈòµÊàñË°®Ê†ºËΩ¨Êç¢‰∏∫ Markdown Êàñ LaTeX Ê†ºÂºèÁöÑË°®Ê†ºÔºå‰ªéËÄåÊñπ‰æøÂú∞Â∞ÜÂÖ∂ÊèíÂÖ•Âà∞ R Markdown ÊñáÊ°£Êàñ HTML È°µÈù¢‰∏≠„ÄÇ</div><p>We also run the likelihood ratio test with H0 : Œ≤1 = Œ≤2 = Œ≤3 = 0 ‚Äì to make sure our full logit model offers a significantly better fit than the model with just an intercept. We find that œá2 = 107.14 and P(> |Chi|) ‚âà 0, so we reject H0.</p><p>Êàë‰ª¨ËøòËøõË°å‰∫Ü‰ººÁÑ∂ÊØîÊ£ÄÈ™åÔºåÂÅáËÆæ H0ÔºöŒ≤1 = Œ≤2 = Œ≤3 = 0Ôºå‰ª•Á°Æ‰øùÊàë‰ª¨ÁöÑÂÆåÊï¥ logit Ê®°ÂûãÊèê‰æõ‰∫ÜÊòæÁùÄÊõ¥Â•ΩÁöÑÊãüÂêàÊïàÊûúÔºåËÄå‰∏ç‰ªÖ‰ªÖÊòØ‰∏Ä‰∏™Êà™Ë∑ùÊ®°Âûã„ÄÇÊàë‰ª¨ÂèëÁé∞ œá2 = 107.14ÔºåP(> |Chi|) ‚âà 0ÔºåÂõ†Ê≠§Êàë‰ª¨ÊãíÁªù H0„ÄÇ</p><div style=background-color:#f0f0f0;padding:10px><p>ËøôÂè•ËØùË°®Êòé‰∫ÜÂØπÂÖ®Ê®°ÂûãÔºàÂê´ÊúâRecency„ÄÅFrequency„ÄÅMonetaryÈ¢ÑÊµãÂèòÈáèÔºâÂíåÂè™ÊúâÊà™Ë∑ùÈ°πÁöÑÊ®°Âûã‰πãÈó¥ËøõË°å‰∫Ü‰ººÁÑ∂ÊØîÊ£ÄÈ™å„ÄÇÂú®‰ººÁÑ∂ÊØîÊ£ÄÈ™å‰∏≠ÔºåÂéüÂÅáËÆæ $ H_0 $ ÊòØÊ®°Âûã‰∏≠ÊâÄÊúâÈ¢ÑÊµãÂèòÈáèÁöÑÁ≥ªÊï∞ÈÉΩ‰∏∫Èõ∂ÔºåÂç≥ $ \beta_1 = \beta_2 = \beta_3 = 0 $ÔºåÂç≥Âè™ÊúâÊà™Ë∑ùÈ°π„ÄÇÂ§áÊã©ÂÅáËÆæ $ H_1 $ ÊòØËá≥Â∞ëÊúâ‰∏Ä‰∏™È¢ÑÊµãÂèòÈáèÁöÑÁ≥ªÊï∞‰∏ç‰∏∫Èõ∂ÔºåÂç≥ÂÖ®Ê®°Âûã„ÄÇ</p><p>ÈÄöËøá‰ººÁÑ∂ÊØîÊ£ÄÈ™åÔºåÂèØ‰ª•Á°ÆÂÆöÊòØÂê¶ÂÖ®Ê®°ÂûãÁõ∏ÂØπ‰∫éÂè™ÊúâÊà™Ë∑ùÈ°πÁöÑÊ®°ÂûãÊèê‰æõ‰∫ÜÊõ¥Â•ΩÁöÑÊãüÂêà„ÄÇÂú®ËøôÁßçÊÉÖÂÜµ‰∏ãÔºåÈÄöËøáËÆ°ÁÆóÂæóÂà∞ÁöÑÂç°ÊñπÁªüËÆ°Èáè $ \chi^2 $ ‰∏∫107.14ÔºåÂØπÂ∫îÁöÑPÂÄºÈùûÂ∏∏Êé•Ëøë‰∫é0ÔºåÈÄöÂ∏∏Â∞è‰∫éÊòæËëóÊÄßÊ∞¥Âπ≥Ôºà‰æãÂ¶Ç0.05Ôºâ„ÄÇÁî±‰∫éPÂÄºÂ∞è‰∫éÊòæËëóÊÄßÊ∞¥Âπ≥ÔºåÊàë‰ª¨ÊãíÁªùÂéüÂÅáËÆæ $ H_0 $ÔºåÂç≥ËÆ§‰∏∫ÂÖ®Ê®°ÂûãÁöÑÊãüÂêàÊïàÊûúÊòæËëóÂú∞‰ºò‰∫éÂè™ÊúâÊà™Ë∑ùÈ°πÁöÑÊ®°Âûã„ÄÇ</p></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>&gt; # likelihood ratio test
</span></span><span class=line><span class=cl>&gt; reduced.model &lt;- glm(Purchase ~ 1, data=RFMdata, family = &#34;binomial&#34;) 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>&gt; kable(xtable(anova(reduced.model, model, test = &#34;Chisq&#34;)),caption = &#34;Likelihood ratio test&#34;)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Table: Likelihood ratio test
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>| Resid. Df| Resid. Dev| Df| Deviance| Pr(&gt;Chi)|
</span></span><span class=line><span class=cl>|---------:|----------:|--:|--------:|--------:|
</span></span><span class=line><span class=cl>|        99|  137.62776| NA|       NA|       NA|
</span></span><span class=line><span class=cl>|        96|   30.48715|  3| 107.1406|        0|
</span></span></code></pre></td></tr></table></div></div><h2 id=32-predicting-probabilities>3.2 Predicting probabilities</h2><p>Now we calculate $ P(Purchase_i) $ for each individual in the data set.</p><p>Áé∞Âú®Êàë‰ª¨ËÆ°ÁÆóÊï∞ÊçÆÈõÜ‰∏≠ÊØè‰∏™‰∏™‰ΩìÁöÑË¥≠‰π∞Ê¶ÇÁéá$ P(Purchase_i) $„ÄÇ</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>&gt; # calculate logit probabilities
</span></span><span class=line><span class=cl>&gt; RFMdata$Base.Probability &lt;- predict(model, RFMdata, type=&#34;response&#34;) 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>&gt; kable(head(RFMdata,5),row.names = TRUE)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>|   | Recency| Frequency| Monetary| Purchase| Base.Probability|
</span></span><span class=line><span class=cl>|:--|-------:|---------:|--------:|--------:|----------------:|
</span></span><span class=line><span class=cl>|1  |     120|         7|    41.66|        0|        0.0030728|
</span></span><span class=line><span class=cl>|2  |      90|         9|    46.71|        0|        0.0008332|
</span></span><span class=line><span class=cl>|3  |     120|         6|   103.99|        1|        0.9833225|
</span></span><span class=line><span class=cl>|4  |     270|        17|    37.13|        1|        0.9999999|
</span></span><span class=line><span class=cl>|5  |      60|         5|    88.92|        0|        0.0032378|
</span></span></code></pre></td></tr></table></div></div><blockquote><p>predict()</p></blockquote><div style=background-color:#f0f0f0;padding:10px>predict() ÂáΩÊï∞ÊòØ R ËØ≠Ë®Ä‰∏≠ÁöÑ‰∏Ä‰∏™Â∏∏Áî®ÂáΩÊï∞ÔºåÁî®‰∫éÂØπÂ∑≤ÊãüÂêàÁöÑÊ®°ÂûãËøõË°åÈ¢ÑÊµã„ÄÇÂÆÉÂèØ‰ª•ÂØπÊñ∞ÁöÑËßÇÊµãÊï∞ÊçÆÂ∫îÁî®Â∑≤ÁªèÊãüÂêàÂ•ΩÁöÑÊ®°ÂûãÔºå‰ªéËÄåÁîüÊàêÈ¢ÑÊµãÂÄº„ÄÇ</div><h2 id=33-predicting-behaviour>3.3 Predicting behaviour</h2><p>We also calculate an indicator variable for whether individuals will purchase or not based on their predicted probabilities</p><p>Êàë‰ª¨ËøòÊ†πÊçÆ‰ªñ‰ª¨ÁöÑÈ¢ÑÊµãÊ¶ÇÁéáËÆ°ÁÆóÂá∫‰∏™‰ΩìÊòØÂê¶‰ºöË¥≠‰π∞ÁöÑÊåáÁ§∫ÂèòÈáè„ÄÇ</p><p>Á¨¶Âè∑ &ldquo;‚äÆ&rdquo; ‰ª£Ë°®ÈÄªËæëÈùûÔºànegationÔºâÊàñËÄÖ‚ÄúÈùû‚Äù„ÄÇÊâÄ‰ª•Êï¥‰∏™ÂÖ¨ÂºèÁöÑÂê´‰πâÊòØÔºöË¥≠‰π∞Ê¶ÇÁéá‰∏çÂ§ß‰∫éÊàñÁ≠â‰∫é0.5„ÄÇ</p>$$
\neg [P(Purchase_i) \geq 0.5]
$$<p>If individual‚Äôs predicted probability is greater or equal to 0.5, we predict he will make a purchase.<br>Â¶ÇÊûú‰∏™‰ΩìÁöÑÈ¢ÑÊµãÊ¶ÇÁéáÂ§ß‰∫éÊàñÁ≠â‰∫é0.5ÔºåÂàôÊàë‰ª¨È¢ÑÊµã‰ªñ‰ºöË¥≠‰π∞„ÄÇ</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>&gt; # purchase vs. no purchase &lt;-&gt; p&gt;0.5 or p&lt;0.5
</span></span><span class=line><span class=cl>&gt; RFMdata$Predicted.Purchase &lt;- 1*(RFMdata$Base.Probability&gt;=0.5) 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>&gt; kable(head(RFMdata,5),row.names = TRUE)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>|   | Recency| Frequency| Monetary| Purchase| Base.Probability| Predicted.Purchase|
</span></span><span class=line><span class=cl>|:--|-------:|---------:|--------:|--------:|----------------:|------------------:|
</span></span><span class=line><span class=cl>|1  |     120|         7|    41.66|        0|        0.0030728|                  0|
</span></span><span class=line><span class=cl>|2  |      90|         9|    46.71|        0|        0.0008332|                  0|
</span></span><span class=line><span class=cl>|3  |     120|         6|   103.99|        1|        0.9833225|                  1|
</span></span><span class=line><span class=cl>|4  |     270|        17|    37.13|        1|        0.9999999|                  1|
</span></span><span class=line><span class=cl>|5  |      60|         5|    88.92|        0|        0.0032378|                  0|
</span></span></code></pre></td></tr></table></div></div><h2 id=34-evaluating-the-model>3.4 Evaluating the model</h2><p>Now, we compute a confusion matrix between predicted purchases and actual purchase behaviour.</p><p>Áé∞Âú®ÔºåÊàë‰ª¨ËÆ°ÁÆóÈ¢ÑÊµãË¥≠‰π∞ÂíåÂÆûÈôÖË¥≠‰π∞Ë°å‰∏∫‰πãÈó¥ÁöÑÊ∑∑Ê∑ÜÁü©Èòµ„ÄÇ</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>&gt; confusionMatrix(table(RFMdata$Predicted.Purchase,RFMdata$Purchase),positive = &#34;1&#34;)
</span></span><span class=line><span class=cl>Confusion Matrix and Statistics
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>   
</span></span><span class=line><span class=cl>     0  1
</span></span><span class=line><span class=cl>  0 51  2
</span></span><span class=line><span class=cl>  1  4 43
</span></span><span class=line><span class=cl>                                         
</span></span><span class=line><span class=cl>               Accuracy : 0.94           
</span></span><span class=line><span class=cl>                 95% CI : (0.874, 0.9777)
</span></span><span class=line><span class=cl>    No Information Rate : 0.55           
</span></span><span class=line><span class=cl>    P-Value [Acc &gt; NIR] : &lt;2e-16         
</span></span><span class=line><span class=cl>                                         
</span></span><span class=line><span class=cl>                  Kappa : 0.8793         
</span></span><span class=line><span class=cl>                                         Now we calculate
</span></span><span class=line><span class=cl> Mcnemar&#39;s Test P-Value : 0.6831         
</span></span><span class=line><span class=cl>                                         
</span></span><span class=line><span class=cl>            Sensitivity : 0.9556         
</span></span><span class=line><span class=cl>            Specificity : 0.9273         
</span></span><span class=line><span class=cl>         Pos Pred Value : 0.9149         
</span></span><span class=line><span class=cl>         Neg Pred Value : 0.9623         
</span></span><span class=line><span class=cl>             Prevalence : 0.4500         
</span></span><span class=line><span class=cl>         Detection Rate : 0.4300         
</span></span><span class=line><span class=cl>   Detection Prevalence : 0.4700         
</span></span><span class=line><span class=cl>      Balanced Accuracy : 0.9414         
</span></span><span class=line><span class=cl>                                         
</span></span><span class=line><span class=cl>       &#39;Positive&#39; Class : 1   
</span></span></code></pre></td></tr></table></div></div><blockquote><p>confusionMatrix()</p></blockquote><div style=background-color:#f0f0f0;padding:10px>confusionMatrix() ÂáΩÊï∞ÊòØ caret ÂåÖ‰∏≠ÁöÑ‰∏Ä‰∏™ÂáΩÊï∞ÔºåÁî®‰∫éËÆ°ÁÆóÂàÜÁ±ªÊ®°ÂûãÁöÑÊ∑∑Ê∑ÜÁü©Èòµ‰ª•ÂèäÂêÑÁßçÂàÜÁ±ªÊåáÊ†áÔºåÂ¶ÇÂáÜÁ°ÆÁéá„ÄÅÁÅµÊïèÂ∫¶„ÄÅÁâπÂºÇÊÄßÁ≠â„ÄÇÊ∑∑Ê∑ÜÁü©ÈòµÊòØ‰∏ÄÁßçÁî®‰∫éËØÑ‰º∞ÂàÜÁ±ªÊ®°ÂûãÊÄßËÉΩÁöÑË°®Ê†ºÔºåÂÖ∂‰∏≠Ë°åË°®Á§∫ÁúüÂÆûÁ±ªÂà´ÔºåÂàóË°®Á§∫È¢ÑÊµãÁ±ªÂà´„ÄÇ<p>Âú®Â∏ÇÂú∫Ëê•ÈîÄÂú∫ÊôØ‰∏≠ÔºåconfusionMatrix() ÂáΩÊï∞ÂèØ‰ª•Áî®‰∫éËØÑ‰º∞ÂàÜÁ±ªÊ®°ÂûãÂú®È¢ÑÊµãÂÆ¢Êà∑Ë°å‰∏∫ÊñπÈù¢ÁöÑÊÄßËÉΩ„ÄÇ‰ª•‰∏ãÊòØ‰∏Ä‰∫õÂÆûÈôÖÂ∫îÁî®Âú∫ÊôØÔºö</p><p>‚Ä¢ ÂÆ¢Êà∑ÊµÅÂ§±È¢ÑÊµãÔºöÂÅáËÆæ‰Ω†Ê≠£Âú®ÂºÄÂ±ïÂÆ¢Êà∑ÊµÅÂ§±È¢ÑÊµãÈ°πÁõÆ„ÄÇ‰Ω†ÂèØ‰ª•‰ΩøÁî®ÂéÜÂè≤Êï∞ÊçÆËÆ≠ÁªÉ‰∏Ä‰∏™ÊµÅÂ§±È¢ÑÊµãÊ®°ÂûãÔºåÂπ∂‰ΩøÁî® confusionMatrix() ÂáΩÊï∞Êù•ËØÑ‰º∞ËØ•Ê®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÈÄöËøáÊ∑∑Ê∑ÜÁü©ÈòµÔºå‰Ω†ÂèØ‰ª•‰∫ÜËß£Ê®°ÂûãÊ≠£Á°ÆÈ¢ÑÊµãÊµÅÂ§±ÂÆ¢Êà∑ÂíåÊú™ÊµÅÂ§±ÂÆ¢Êà∑ÁöÑÊÉÖÂÜµÔºåÂπ∂ËÆ°ÁÆóÂá∫ÂáÜÁ°ÆÁéá„ÄÅÁÅµÊïèÂ∫¶Á≠âÊåáÊ†á„ÄÇ</p><p>‚Ä¢ Ëê•ÈîÄÊ¥ªÂä®ÂèçÈ¶àÔºöÂ¶ÇÊûú‰Ω†ËøõË°å‰∫Ü‰∏ÄÈ°πÂ∏ÇÂú∫Ëê•ÈîÄÊ¥ªÂä®Ôºå‰æãÂ¶ÇÂèëÈÄÅÁîµÂ≠êÈÇÆ‰ª∂ÊàñÁü≠‰ø°Ëê•ÈîÄÊ¥ªÂä®Ôºå‰Ω†ÂèØ‰ª•‰ΩøÁî® confusionMatrix() ÂáΩÊï∞Êù•ËØÑ‰º∞Ê¥ªÂä®ÁöÑÊïàÊûú„ÄÇ‰Ω†ÂèØ‰ª•Â∞ÜÂÆûÈôÖÊ¥ªÂä®ÁöÑÁªìÊûú‰∏éÈ¢ÑÊúüÁªìÊûúËøõË°åÊØîËæÉÔºå‰ª•‰∫ÜËß£Ê¥ªÂä®ÁöÑÊàêÂäüÁéá‰ª•ÂèäÊòØÂê¶ÊúâÂøÖË¶ÅË∞ÉÊï¥‰Ω†ÁöÑËê•ÈîÄÁ≠ñÁï•„ÄÇ</p><p>‚Ä¢ ÂÆ¢Êà∑ÂàÜÁæ§ÔºöÂú®ÂÆ¢Êà∑ÂàÜÁæ§È°πÁõÆ‰∏≠Ôºå‰Ω†ÂèØËÉΩ‰ΩøÁî®ËÅöÁ±ªÁÆóÊ≥ïÂ∞ÜÂÆ¢Êà∑ÂàÜÊàê‰∏çÂêåÁöÑÁæ§‰Ωì„ÄÇ‰Ω†ÂèØ‰ª•‰ΩøÁî® confusionMatrix() ÂáΩÊï∞Êù•ËØÑ‰º∞Ê®°ÂûãÁöÑÊÄßËÉΩÔºå‰æãÂ¶ÇÊØîËæÉËÅöÁ±ªÁªìÊûú‰∏éÁúüÂÆûÁöÑÂÆ¢Êà∑ÁâπÂæÅ‰πãÈó¥ÁöÑÂåπÈÖçÁ®ãÂ∫¶„ÄÇËøôÂèØ‰ª•Â∏ÆÂä©‰Ω†Á°ÆÂÆöÊòØÂê¶ÈúÄË¶ÅÈáçÊñ∞Ë∞ÉÊï¥ÂàÜÁæ§ÊñπÊ≥ïÊàñËÄÖÊîπËøõÊï∞ÊçÆË¥®Èáè„ÄÇ</p><p>‚Ä¢ ‰∫ßÂìÅÊé®ËçêÁ≥ªÁªüÔºöÂ¶ÇÊûú‰Ω†Ê≠£Âú®ÂºÄÂèë‰∏Ä‰∏™‰∫ßÂìÅÊé®ËçêÁ≥ªÁªüÔºå‰Ω†ÂèØ‰ª•‰ΩøÁî® confusionMatrix() ÂáΩÊï∞Êù•ËØÑ‰º∞Á≥ªÁªüÁöÑÂáÜÁ°ÆÊÄß„ÄÇ‰Ω†ÂèØ‰ª•ÊØîËæÉÁ≥ªÁªüÊé®ËçêÁöÑ‰∫ßÂìÅ‰∏éÁî®Êà∑ÂÆûÈôÖË¥≠‰π∞ÁöÑ‰∫ßÂìÅ‰πãÈó¥ÁöÑÂåπÈÖçÊÉÖÂÜµÔºåÂπ∂ËÆ°ÁÆóÂá∫ÂáÜÁ°ÆÁéá„ÄÅÂè¨ÂõûÁéáÁ≠âÊåáÊ†áÔºå‰ª•‰∫ÜËß£Á≥ªÁªüÁöÑÊÄßËÉΩÂíåÁî®Êà∑ÁöÑÊª°ÊÑèÂ∫¶„ÄÇ</p></div><p>We can also plot the receiver operating characteristic (ROC) curve, which illustrates the diagnostic ability of a binary logit model. It is created by plotting the true positive rate (TPR) against the false positive rate (FPR) ‚Äì at various decision threshold values for prediction.<br>ROC curve can be quickly evaluated using the area under the curve (AUC) metric, which captures the overall quality of the classifier. The greater the AUC, the better. AUC of 1.0 represents a perfect classifier, AUC of 0.5 (diagonal line) represents a worthless classifier. As we see, the binary logit classifier does a good job of predicting purchases on the training data.</p><p>Êàë‰ª¨ËøòÂèØ‰ª•ÁªòÂà∂Êé•Êî∂ËÄÖÊìç‰ΩúÁâπÂæÅÊõ≤Á∫øÔºàROCÊõ≤Á∫øÔºâÔºåÂÆÉÂ±ïÁ§∫‰∫Ü‰∫åÂÖÉLogitÊ®°ÂûãÁöÑËØäÊñ≠ËÉΩÂäõ„ÄÇROCÊõ≤Á∫øÈÄöËøáÂú®‰∏çÂêåÁöÑÈ¢ÑÊµãÂÜ≥Á≠ñÈòàÂÄº‰∏ãÁªòÂà∂ÁúüÊ≠£ÁéáÔºàTPRÔºâ‰∏éÂÅáÊ≠£ÁéáÔºàFPRÔºâ‰πãÈó¥ÁöÑÂÖ≥Á≥ªÊù•ÂàõÂª∫„ÄÇ<br>ROCÊõ≤Á∫øÂèØ‰ª•ÈÄöËøáÊõ≤Á∫ø‰∏ãÈù¢ÁßØÔºàAUCÔºâÊåáÊ†áËøõË°åÂø´ÈÄüËØÑ‰º∞ÔºåËØ•ÊåáÊ†áÊçïÊçâ‰∫ÜÂàÜÁ±ªÂô®ÁöÑÊï¥‰ΩìË¥®Èáè„ÄÇAUCË∂äÂ§ßÔºåÂàÜÁ±ªÂô®ÁöÑÊÄßËÉΩË∂äÂ•Ω„ÄÇAUC‰∏∫1.0Ë°®Á§∫ÂÆåÁæéÁöÑÂàÜÁ±ªÂô®ÔºåAUC‰∏∫0.5ÔºàÂØπËßíÁ∫øÔºâË°®Á§∫‰∏Ä‰∏™ÊØ´Êó†‰ª∑ÂÄºÁöÑÂàÜÁ±ªÂô®„ÄÇÊ≠£Â¶ÇÊàë‰ª¨ÊâÄÁúãÂà∞ÁöÑÔºå‰∫åÂÖÉLogitÂàÜÁ±ªÂô®Âú®ËÆ≠ÁªÉÊï∞ÊçÆ‰∏äÈ¢ÑÊµãË¥≠‰π∞Ë°å‰∏∫ÁöÑÊïàÊûúËâØÂ•Ω„ÄÇ</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>&gt; rocobj &lt;- roc(RFMdata$Purchase, RFMdata$Base.Probability)
</span></span><span class=line><span class=cl>Setting levels: control = 0, case = 1
</span></span><span class=line><span class=cl>Setting direction: controls &lt; cases
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>&gt; {plot(rocobj,legacy.axes=TRUE)
</span></span><span class=line><span class=cl>  text(0.5, 0.8, labels = sprintf(&#34;AUC = %.5f&#34;,rocobj$auc))}
</span></span></code></pre></td></tr></table></div></div><blockquote><p>roc()</p></blockquote><div style=background-color:#f0f0f0;padding:10px>roc() ÂáΩÊï∞ÊòØ pROC ÂåÖ‰∏≠ÁöÑ‰∏Ä‰∏™ÂáΩÊï∞ÔºåÁî®‰∫éËÆ°ÁÆóÊé•Êî∂ËÄÖÊìç‰ΩúÁâπÂæÅÔºàReceiver Operating CharacteristicÔºåROCÔºâÊõ≤Á∫ø‰ª•ÂèäËÆ°ÁÆóÊõ≤Á∫ø‰∏ãÈù¢ÁßØÔºàArea Under the CurveÔºåAUCÔºâ„ÄÇROC Êõ≤Á∫øÊòØ‰∏ÄÁßçÁî®‰∫éËØÑ‰º∞‰∫åÂÖÉÂàÜÁ±ªÂô®ÊÄßËÉΩÁöÑÂõæÂΩ¢Â∑•ÂÖ∑ÔºåÂÆÉÊòæÁ§∫‰∫ÜÂú®‰∏çÂêåÂàÜÁ±ªÈòàÂÄº‰∏ãÁúüÊ≠£‰æãÁéáÔºàTrue Positive RateÔºåTPRÔºåÂèàÁß∞‰∏∫ÁÅµÊïèÂ∫¶Ôºâ‰∏éÂÅáÊ≠£‰æãÁéáÔºàFalse Positive RateÔºåFPRÔºâ‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇ</div><p>{% asset_image week4_plot.png %}</p><p>Finally, we predict new probabilities under a hypothetical scenario that everyone‚Äôs Monetary variable went up by one unit.</p>$$
V_{\text{new}} = \beta_0 + \beta_1 \text{Recency} + \beta_2 \text{Frequency} + \beta_3 (\text{Monetary} + 1)
$$<div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>&gt; # calculate new logit probabilities (Monetary+1)
</span></span><span class=line><span class=cl>&gt; RFMdata_new &lt;- RFMdata
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>&gt; RFMdata_new$Monetary &lt;- RFMdata_new$Monetary + 1 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>&gt; RFMdata$New.Probability &lt;- predict(model, RFMdata_new, type=&#34;response&#34;)
</span></span></code></pre></td></tr></table></div></div><p>We compare mean new probability across individuals to the mean of old probabilities, and also calculate the lift metric.</p><p>Êàë‰ª¨ÊØîËæÉÂêÑ‰∏™‰∏™‰ΩìÁöÑÊñ∞Ê¶ÇÁéáÂùáÂÄº‰∏éÊóßÊ¶ÇÁéáÂùáÂÄºÔºåÂπ∂ËÆ°ÁÆóÊèêÂçáÂ∫¶Èáè„ÄÇ</p>$$ P(Purchase_i) = \frac{1}{N} \sum_{i=1}^{N} \frac{\exp(V_{bi})}{\exp(V_{bi}) + 1} $$$$ P(Purchase_{\text{new}}) = \frac{1}{N} \sum_{i=1}^{N} \frac{\exp(V_{\text{new}})}{\exp(V_{\text{new}}) + 1} $$$$ \text{Lift} = \frac{p_{\text{new}} - p_{\text{old}}}{p_{\text{old}}} $$<div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-gdscript3 data-lang=gdscript3><span class=line><span class=cl><span class=o>&gt;</span> <span class=c1># mean predicted base probability</span>
</span></span><span class=line><span class=cl><span class=o>&gt;</span> <span class=n>mean</span><span class=p>(</span><span class=n>RFMdata</span><span class=o>$</span><span class=n>Base</span><span class=o>.</span><span class=n>Probability</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=mf>0.45</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>&gt;</span> <span class=c1># mean new predicted probability</span>
</span></span><span class=line><span class=cl><span class=o>&gt;</span> <span class=n>mean</span><span class=p>(</span><span class=n>RFMdata</span><span class=o>$</span><span class=n>New</span><span class=o>.</span><span class=n>Probability</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=mf>0.4578851</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>&gt;</span> <span class=c1># lift</span>
</span></span><span class=line><span class=cl><span class=o>&gt;</span> <span class=p>(</span><span class=n>mean</span><span class=p>(</span><span class=n>RFMdata</span><span class=o>$</span><span class=n>New</span><span class=o>.</span><span class=n>Probability</span><span class=p>)</span> <span class=o>-</span> <span class=n>mean</span><span class=p>(</span><span class=n>RFMdata</span><span class=o>$</span><span class=n>Base</span><span class=o>.</span><span class=n>Probability</span><span class=p>))</span><span class=o>/</span><span class=n>mean</span><span class=p>(</span><span class=n>RFMdata</span><span class=o>$</span><span class=n>Base</span><span class=o>.</span><span class=n>Probability</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=mf>0.01752255</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>&gt;</span> <span class=c1># remove predicted purchase variable</span>
</span></span><span class=line><span class=cl><span class=o>&gt;</span> <span class=n>RFMdata</span><span class=o>$</span><span class=n>Predicted</span><span class=o>.</span><span class=n>Purchase</span> <span class=o>&lt;-</span> <span class=n>NULL</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>&gt;</span> <span class=c1># data</span>
</span></span><span class=line><span class=cl><span class=o>&gt;</span> <span class=n>kable</span><span class=p>(</span><span class=n>head</span><span class=p>(</span><span class=n>RFMdata</span><span class=p>,</span><span class=mi>5</span><span class=p>),</span><span class=n>row</span><span class=o>.</span><span class=n>names</span> <span class=o>=</span> <span class=n>TRUE</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>|</span>   <span class=o>|</span> <span class=n>Recency</span><span class=o>|</span> <span class=n>Frequency</span><span class=o>|</span> <span class=n>Monetary</span><span class=o>|</span> <span class=n>Purchase</span><span class=o>|</span> <span class=n>Base</span><span class=o>.</span><span class=n>Probability</span><span class=o>|</span> <span class=n>New</span><span class=o>.</span><span class=n>Probability</span><span class=o>|</span>
</span></span><span class=line><span class=cl><span class=o>|</span><span class=p>:</span><span class=o>--|-------</span><span class=p>:</span><span class=o>|---------</span><span class=p>:</span><span class=o>|--------</span><span class=p>:</span><span class=o>|--------</span><span class=p>:</span><span class=o>|----------------</span><span class=p>:</span><span class=o>|---------------</span><span class=p>:</span><span class=o>|</span>
</span></span><span class=line><span class=cl><span class=o>|</span><span class=mi>1</span>  <span class=o>|</span>     <span class=mi>120</span><span class=o>|</span>         <span class=mi>7</span><span class=o>|</span>    <span class=mf>41.66</span><span class=o>|</span>        <span class=mi>0</span><span class=o>|</span>        <span class=mf>0.0030728</span><span class=o>|</span>       <span class=mf>0.0036319</span><span class=o>|</span>
</span></span><span class=line><span class=cl><span class=o>|</span><span class=mi>2</span>  <span class=o>|</span>      <span class=mi>90</span><span class=o>|</span>         <span class=mi>9</span><span class=o>|</span>    <span class=mf>46.71</span><span class=o>|</span>        <span class=mi>0</span><span class=o>|</span>        <span class=mf>0.0008332</span><span class=o>|</span>       <span class=mf>0.0009852</span><span class=o>|</span>
</span></span><span class=line><span class=cl><span class=o>|</span><span class=mi>3</span>  <span class=o>|</span>     <span class=mi>120</span><span class=o>|</span>         <span class=mi>6</span><span class=o>|</span>   <span class=mf>103.99</span><span class=o>|</span>        <span class=mi>1</span><span class=o>|</span>        <span class=mf>0.9833225</span><span class=o>|</span>       <span class=mf>0.9858611</span><span class=o>|</span>
</span></span><span class=line><span class=cl><span class=o>|</span><span class=mi>4</span>  <span class=o>|</span>     <span class=mi>270</span><span class=o>|</span>        <span class=mi>17</span><span class=o>|</span>    <span class=mf>37.13</span><span class=o>|</span>        <span class=mi>1</span><span class=o>|</span>        <span class=mf>0.9999999</span><span class=o>|</span>       <span class=mf>0.9999999</span><span class=o>|</span>
</span></span><span class=line><span class=cl><span class=o>|</span><span class=mi>5</span>  <span class=o>|</span>      <span class=mi>60</span><span class=o>|</span>         <span class=mi>5</span><span class=o>|</span>    <span class=mf>88.92</span><span class=o>|</span>        <span class=mi>0</span><span class=o>|</span>        <span class=mf>0.0032378</span><span class=o>|</span>       <span class=mf>0.0038267</span><span class=o>|</span>
</span></span></code></pre></td></tr></table></div></div><h1 id=4-recap>4. Recap</h1><p>‚Ä¢ Logistic regression is a powerful method and a particularly good fit for many marketing problems with binary outcomes. We will cover the choice model later for modelling product choice among sets of alternatives.<br>‚Ä¢ Logistic regression relates a binary outcome such as purchase to predictors that may include continuous and factor variable by modelling the variable‚Äôs association with the probability of the outcome.</p><blockquote><p>Although we performed logistic regression here with categorical predictors (factor variables) due to the structure of the amusement park sales data, we could also use continuous predictors in glm(). Just add those to the right-hand side of the model formula as we did with lm()<br>‚Ä¢ A logistic regression model, also known as a logit model, is a member of the generalized linear model family and is fit using glm( , family = binomial).<br>‚Ä¢ Coefficient in a logit model can be interpreted in terms of odds ratios, the degree to which they are associated with the increased or decreased likelihood of an outcome. This is done simply by exponentiating the coefficients with exp().<br>‚Ä¢ A statistically significant result does not always mean that the model is appropriate. It is important to explore data thoroughly and construct models on the basis of careful consideration.<br>We saw that the estimated effect of promotion was positive when we estimated one model yet negative when we estimated another. This shows that it is crucial to explore data thoroughly before modelling or interpreting a model. For most marketing data, no model is ever definitive. However, through careful data exploration and consideration of multiple models, we may increase our confidence in our models and the inferences drawn from them.</p></blockquote><p>‚Ä¢ ÈÄªËæëÂõûÂΩíÊòØ‰∏ÄÁßçÂº∫Â§ßÁöÑÊñπÊ≥ïÔºåÁâπÂà´ÈÄÇÁî®‰∫éËÆ∏Â§öÂÖ∑Êúâ‰∫åÂÖÉÁªìÊûúÁöÑËê•ÈîÄÈóÆÈ¢ò„ÄÇÊàë‰ª¨Á®çÂêéÂ∞Ü‰ªãÁªçÈÄâÊã©Ê®°ÂûãÔºåÁî®‰∫éÂØπ‰∏ÄÁªÑÊõø‰ª£ÂìÅ‰∏≠ÁöÑ‰∫ßÂìÅÈÄâÊã©ËøõË°åÂª∫Ê®°„ÄÇ<br>‚Ä¢ ÈÄªËæëÂõûÂΩíÂ∞Ü‰∫åÂÖÉÁªìÊûúÔºàÂ¶ÇË¥≠‰π∞Ôºâ‰∏éÂèØËÉΩÂåÖÊã¨ËøûÁª≠ÂíåÂõ†Â≠êÂèòÈáèÁöÑÈ¢ÑÊµãÂèòÈáèÂÖ≥ËÅîËµ∑Êù•ÔºåÊñπÊ≥ïÊòØÈÄöËøáÊ®°ÊãüÂèòÈáè‰∏éÁªìÊûúÁöÑÊ¶ÇÁéá‰πãÈó¥ÁöÑÂÖ≥ËÅî„ÄÇ</p><blockquote><p>Â∞ΩÁÆ°Êàë‰ª¨Âú®ËøôÈáå‰ΩøÁî®‰∫ÜÂõ†Â≠êÂèòÈáèÔºàÂàÜÁ±ªÂèòÈáèÔºâÊâßË°åÈÄªËæëÂõûÂΩíÔºåÂõ†‰∏∫Â®±‰πêÂõ≠ÈîÄÂîÆÊï∞ÊçÆÁöÑÁªìÊûÑÂ¶ÇÊ≠§Ôºå‰ΩÜÊàë‰ª¨‰πüÂèØ‰ª•Âú®glm()‰∏≠‰ΩøÁî®ËøûÁª≠È¢ÑÊµãÂèòÈáè„ÄÇÂè™ÈúÄÂÉèÊàë‰ª¨Âú®lm()‰∏≠ÈÇ£Ê†∑Â∞ÜÂÆÉ‰ª¨Ê∑ªÂä†Âà∞Ê®°ÂûãÂÖ¨ÂºèÁöÑÂè≥‰æßÂç≥ÂèØ„ÄÇ<br>‚Ä¢ ÈÄªËæëÂõûÂΩíÊ®°ÂûãÔºå‰πüÁß∞‰∏∫logitÊ®°ÂûãÔºåÊòØÂπø‰πâÁ∫øÊÄßÊ®°ÂûãÂÆ∂ÊóèÁöÑ‰∏ÄÂëòÔºå‰ΩøÁî®glm()ÊãüÂêàÔºåÂÆ∂ÊóèËÆæÁΩÆ‰∏∫binomial„ÄÇ<br>‚Ä¢ Âú®logitÊ®°Âûã‰∏≠ÔºåÁ≥ªÊï∞ÂèØ‰ª•ÈÄöËøáÂ∞ÜÂÖ∂ÊåáÊï∞Âåñ‰∏∫exp()Êù•Ëß£Èáä‰∏∫Âá†ÁéáÊØîÔºåÂç≥ÂÆÉ‰ª¨‰∏éÁªìÊûúÁöÑÂ¢ûÂä†ÊàñÂáèÂ∞ëÂèØËÉΩÊÄßÁöÑÁõ∏ÂÖ≥Á®ãÂ∫¶„ÄÇ<br>‚Ä¢ ÁªüËÆ°ÊòæËëóÁªìÊûúÂπ∂‰∏çÊÄªÊÑèÂë≥ÁùÄÊ®°ÂûãÊòØÈÄÇÂΩìÁöÑ„ÄÇÂú®Âª∫Ê®°ÊàñËß£ÈáäÊ®°Âûã‰πãÂâçÔºåÂΩªÂ∫ïÊé¢Á¥¢Êï∞ÊçÆÂπ∂Âü∫‰∫éÊ∑±ÊÄùÁÜüËôëÊûÑÂª∫Ê®°ÂûãÊòØÈùûÂ∏∏ÈáçË¶ÅÁöÑ„ÄÇ<br>Êàë‰ª¨ÁúãÂà∞ÔºåÂΩìÊàë‰ª¨‰º∞ËÆ°‰∏Ä‰∏™Ê®°ÂûãÊó∂Ôºå‰øÉÈîÄÁöÑ‰º∞ËÆ°ÊïàÊûúÊòØÊ≠£ÂêëÁöÑÔºå‰ΩÜÂΩìÊàë‰ª¨‰º∞ËÆ°Âè¶‰∏Ä‰∏™Ê®°ÂûãÊó∂ÔºåÊïàÊûúÊòØË¥üÂêëÁöÑ„ÄÇËøôË°®ÊòéÔºåÂú®Âª∫Ê®°ÊàñËß£ÈáäÊ®°Âûã‰πãÂâçÔºåÂΩªÂ∫ïÊé¢Á¥¢Êï∞ÊçÆÊòØËá≥ÂÖ≥ÈáçË¶ÅÁöÑ„ÄÇÂØπ‰∫éÂ§ßÂ§öÊï∞Ëê•ÈîÄÊï∞ÊçÆÔºåÊ≤°Êúâ‰∏Ä‰∏™Ê®°ÂûãÊòØÁªùÂØπÁöÑ„ÄÇÁÑ∂ËÄåÔºåÈÄöËøáÂØπÊï∞ÊçÆËøõË°åÊ∑±ÂÖ•Êé¢Á¥¢Âπ∂ËÄÉËôëÂ§ö‰∏™Ê®°ÂûãÔºåÊàë‰ª¨ÂèØËÉΩ‰ºöÂ¢ûÂä†ÂØπÊ®°ÂûãÂèäÂÖ∂Êé®Êñ≠ÁöÑ‰ø°ÂøÉ„ÄÇ</p></blockquote></section><footer class=article-footer><section class=article-tags><a href=/tags/r-language/>R-Language</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//weasley.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 Kunkka</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>