[{"content":"荤菜 红烧排骨 西红柿炒鸡蛋 西葫芦炒蛋 素菜 红烧茄子 汤 冬瓜排骨汤 凉菜 凉拌苦菊 主食 暂无内容\n外卖 暂无内容\n","date":"2025-09-07T23:38:00Z","permalink":"https://MyLoveES.github.io/p/god-of-cookery-home-edition/","title":"God of Cookery - Home Edition"},{"content":"Immich 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 # # WARNING: To install Immich, follow our guide: https://immich.app/docs/install/docker-compose # # Make sure to use the docker-compose.yml of the current release: # # https://github.com/immich-app/immich/releases/latest/download/docker-compose.yml # # The compose file on main may not be compatible with the latest release. name: immich services: immich-server: container_name: immich_server image: ghcr.io/immich-app/immich-server:v1.135.3 # extends: # file: hwaccel.transcoding.yml # service: cpu # set to one of [nvenc, quicksync, rkmpp, vaapi, vaapi-wsl] for accelerated transcoding volumes: # Do not edit the next line. If you want to change the media storage location on your system, edit the value of UPLOAD_LOCATION in the .env file - /share/Container/container-station-data/application/immich/server/library:/usr/src/app/upload - /share/CACHEDEV1_DATA/Weasley/Pictures:/external/Weasley/Pictures:ro - /etc/localtime:/etc/localtime:ro environment: - TZ=Asia/Shanghai - DB_USERNAME=postgres - DB_PASSWORD=postgres - DB_DATABASE_NAME=immich ports: - \u0026#39;2283:2283\u0026#39; depends_on: - redis - database networks: - immich restart: always healthcheck: disable: false immich-machine-learning: container_name: immich_machine_learning # For hardware acceleration, add one of -[armnn, cuda, rocm, openvino, rknn] to the image tag. # Example tag: ${IMMICH_VERSION:-release}-cuda image: ghcr.io/immich-app/immich-machine-learning:v1.135.3 # extends: # uncomment this section for hardware acceleration - see https://immich.app/docs/features/ml-hardware-acceleration # file: hwaccel.ml.yml # service: cpu # set to one of [armnn, cuda, rocm, openvino, openvino-wsl, rknn] for accelerated inference - use the `-wsl` version for WSL2 where applicable volumes: - /share/Container/container-station-data/application/immich/machine_learning/model_cache:/cache restart: always networks: - immich healthcheck: disable: false redis: container_name: immich_redis image: docker.io/valkey/valkey:8-bookworm networks: - immich healthcheck: test: redis-cli ping || exit 1 restart: always database: container_name: immich_postgres image: ghcr.io/immich-app/postgres:14-vectorchord0.4.3-pgvectors0.2.0 environment: POSTGRES_PASSWORD: postgres POSTGRES_USER: postgres POSTGRES_DB: immich POSTGRES_INITDB_ARGS: \u0026#39;--data-checksums\u0026#39; # Uncomment the DB_STORAGE_TYPE: \u0026#39;HDD\u0026#39; var if your database isn\u0026#39;t stored on SSDs # DB_STORAGE_TYPE: \u0026#39;HDD\u0026#39; networks: - immich volumes: # Do not edit the next line. If you want to change the database storage location on your system, edit the value of DB_DATA_LOCATION in the .env file - /share/Container/container-station-data/application/immich/Postgres/data:/var/lib/postgresql/data restart: always volumes: model-cache: networks: immich: 1. 模型下载 NAS上由于网络问题，无法从hf上加载对应的模型，需要先下载到本地，再手动上传到NAS。\n1 2 3 4 5 6 7 8 9 # 下载模型 https://huggingface.co/immich-app ## buffalo_l 用于人脸识别 huggingface-cli download immich-app/buffalo_l --local-dir immich-app-buffalo_l ## XLM-Roberta-Large-Vit-B-16Plus用于智能搜索 huggingface-cli download immich-app/XLM-Roberta-Large-Vit-B-16Plus --local-dir immich-app-XLM-Roberta-Large-Vit-B-16Plus NAS上模型目录结构\n1 2 3 4 5 6 7 8 9 10 11 12 . |____model_cache | |____clip | | |____XLM-Roberta-Large-Vit-B-16Plus | | | |____textual | | | |____visual | | | |____.cache | |____facial-recognition | | |____buffalo_l | | | |____recognition | | | |____.cache | | | |____detection 2. immich 配置 ","date":"2025-07-07T00:00:00Z","permalink":"https://MyLoveES.github.io/p/disconas-immich/","title":"DiscoNAS Immich"},{"content":"Media Library 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 version: \u0026#34;3\u0026#34; services: jellyfin: image: nyanmisaka/jellyfin:250627-amd64 container_name: media_library_jellyfin environment: - PUID=0 - PGID=0 - TZ=Asia/Shanghai volumes: - /share/CACHEDEV1_DATA/Container/container-station-data/application/media_library/jellyfin/config:/config - /share/CACHEDEV1_DATA/Container/container-station-data/application/media_library/jellyfin/cache:/cache - /share/CACHEDEV1_DATA/Weasley/Videos/:/videos ports: - 8096:8096 - 8920:8920 devices: - /dev/dri:/dev/dri restart: unless-stopped nastool: # image: hsuyelin/nas-tools:3.4.1 image: razeencheng/nastool:2.9.1 container_name: media_library_nastool environment: - PUID=0 - PGID=0 - UMASK=000 - TZ=Asia/Shanghai - NASTOOL_AUTO_UPDATE=false - REPO_URL=https://ghproxy.com/https://github.com/hsuyelin/nas-tools.git # - ALPINE_MIRROR=mirrors.ustc.edu.cn # - LANG=C.UTF-8 # - NASTOOL_AUTO_UPDATE=false # - NASTOOL_CN_UPDATE=false # - NASTOOL_CONFIG=/config/config.yaml # - NASTOOL_VERSION=master # - PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin # - PYPI_MIRROR=https://pypi.tuna.tsinghua.edu.cn/simple # - REPO_URL=https://github.com/jxxghp/nas-tools.git # - WORKDIR=/nas-tools volumes: - /share/CACHEDEV1_DATA/Container/container-station-data/application/media_library/nastool/config:/config - /share/CACHEDEV1_DATA/Weasley/Videos/:/videos ports: - 3000:3000 restart: unless-stopped jackett: image: linuxserver/jackett:amd64-0.22.2132 container_name: media_library_jackett volumes: - /share/CACHEDEV1_DATA/Container/container-station-data/application/media_library/jackett/config:/config - /share/CACHEDEV1_DATA/Container/container-station-data/application/media_library/jackett/downloads:/downloads environment: - PUID=1000 - PGID=1000 - TZ=Asia/Shanghai - AUTO_UPDATE=true # - HOME=/root # - LSIO_FIRST_PARTY=true # - PATH=/lsiopy/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin # - S6_CMD_WAIT_FOR_SERVICES_MAXTIME=0 # - S6_STAGE2_HOOK=/docker-mods # - S6_VERBOSITY=1 # - TERM=xterm # - VIRTUAL_ENV=/lsiopy # - XDG_CONFIG_HOME=/config # - XDG_DATA_HOME=/config ports: - 9117:9117 restart: unless-stopped qBittorrent: image: superng6/qbittorrentee:5.1.1.10 container_name: media_library_qBittorrent environment: - PUID=1026 - PGID=100 - TZ=Asia/Shanghai - WEBUIPORT=8080 - ENABLE_DOWNLOADS_PERM_FIX=true restart: unless-stopped volumes: - /share/CACHEDEV1_DATA/Container/container-station-data/application/media_library/qBittorrent/config:/config - /share/CACHEDEV1_DATA/Weasley/Videos/:/downloads ports: - 8080:8080 - 26881:6881 - 26881:6881/udp 1. qBittorrent 默认账号：admin 默认密码：需要查看 qBittorrent 容器日志 1.1 修改密码 1.2 修改端口号 1.3 修改下载路径 1.4 修改语言 1.5 添加分类 2. Jackett 2.1 记住 API KEY 后面需要配置到 NASTOOL 中\n2.2 添加公开的 INDEXER 需要等待几分钟，才能添加完\n3. Jellyfin 3.1 添加媒体库 选择 NASTOOL 链接后的目录 3.2 生成API密钥 后面给 NASTOOL 用\n3.3 指定转码 4. NASTOOL 4.1 TMDB https://www.themoviedb.org/\n4.2 媒体库 注意要用NASTOOL链接后的目录 4.3 目录同步 从 下载目录 到 转换后的目录 4.4 索引器 4.5 下载器 4.6 媒体播放器 4.7 服务 可以手动目录同步，或者清理缓存 ","date":"2025-06-07T00:00:00Z","permalink":"https://MyLoveES.github.io/p/disconas-media-library/","title":"DiscoNAS Media Library"},{"content":"日本关西：京都与大阪 大家好，今天想跟大家聊聊我之前去日本京都和大阪玩的一些体验。整体感觉挺舒服的，两个城市风格很不同，一个安静有古韵，一个热闹充满活力，而且交通、吃喝都特别方便。我就按我们走的顺序，跟大家分享一下旅程。\n🚉 京都：走走停停的慢时光 清水寺 清水寺作为日本最具代表性的佛教古寺之一，历史可追溯至公元778年。寺庙供奉的是千手观音，而龙在佛教中常被视为守护神兽，镇守山门，驱邪避灾。 清水舞台 京都的标志之一。悬空的大木台（叫“清水舞台”）真的很特别，是古人用木头巧妙搭建起来的，没用钉子，站在上面看下面的景色，视野非常开阔，能看到大半个京都。我们去的时候虽然不是樱花或红叶季，但满眼的绿色也很养眼。感觉它受欢迎，一是因为历史悠久、建筑独特，二是因为景色确实好，拍照也好看。\n另外，这也是情侣照名打卡地，因为《名侦探柯南》里新兰出现过这里。\n凌云之龙 造型气势磅礴、极富动感，权力的象征。\n「音羽瀑布」 接水祈福。\n二、三年坂 二年坂与三年坂是京都清水寺通往街道的两条传统坡道，以古色古香的石板路、和风茶屋与手工店铺闻名，是体验京都古都风情的热门步行街。传说中在三年坂跌倒会有三年厄运，也为其增添一丝神秘色彩。\n伏见稻荷大社 伏见稻荷大社是位于京都的著名神社，以数千座朱红色鸟居构成的“千本鸟居”闻名，供奉掌管五谷丰登与商业繁荣的稻荷神，是日本最受欢迎的参拜胜地之一。 千本鸟居 这个神社最特别的就是那条长长的“千本鸟居”了。一座接一座的红色鸟居沿着山路往上排，形成了一条望不到头的红色通道。走进去感觉挺神奇的，越往山上走人越少，也越安静。爬到一半回头看，还能看到京都的城市风景。每个鸟居后面都写着捐赠者的名字和心愿，挺有意思的。\n这里拍照确实好看，电影《艺伎回忆录》也在这里取过景。\n登顶，非常简陋的打卡点\n使者 狐狸 狐狸因其常出没于田间地头，被认为是稻荷神的“神使”。相传狐狸拥有神秘力量，能穿梭阴阳界，帮助传达神谕、驱邪保平安。因此神社内常见手持钥匙、宝珠、卷轴、稻穗的狐狸像。\n鸭川 \u0026amp; 八坂神社 傍晚在鸭川边散步是我们在京都最放松的时刻之一。河水很干净，两边修了很宽的步行道。当地人三三两两地坐在河边的石阶上，聊天、看风景、等日落，感觉特别生活化，节奏很慢。\n沿着鸭川走不远就是八坂神社。这个神社好像大门一直开着，白天挺热闹，晚上去就安静多了，灯笼点起来，氛围很宁静，跟白天很不一样。\n八坂神社是京都最重要的神社之一，位于祇园地区，以祈求厄除、健康与良缘闻名，也是每年祇园祭的主办神社，深受当地人和游客喜爱。 金阁寺 金阁寺（正式名称为鹿苑寺）是京都著名的世界文化遗产，以其外墙贴满金箔的华丽三重楼阁倒映在镜湖池中的绝美景色而闻名，象征着日本室町时代的极致审美与权力荣耀。 同样，柯南中也有它的身影\n御守 虽门票附赠\n白蛇冢 我称之为 雷峰塔京都分塔=MINI版\n贵船神社 如果觉得市区人多，可以抽半天去北边的贵船神社，躲个清净。它藏在山里，一路绿树成荫，夏天去特别凉快。最吸引人的是入口那条石阶路，两边排满了红灯笼，傍晚亮灯时或者雨天，氛围感特别好，很适合拍照。\n这里据说求姻缘挺灵的，还可以玩一种“水占卜”，签纸放水里会显出字来。\n京都车站 京都车站本身也挺有看头的，是个很现代的大建筑。里面什么都有：商店、餐厅、还有个大阶梯广场让人休息。\n同样，这里也是《迷宫的十字路口》的取景地。\n动漫 环境 非常干净。你可以说意林，读者在美化国外，但是，京都真的非常非常非常非常干净。为什么呢？街道上不允许设置垃圾桶，这能不干净吗？\n🌆 大阪：热闹与美食的聚集地 从京都坐车很快到大阪，感觉一下子热闹起来了，特别是晚上，灯火通明，很有活力。\n环球影城 (USJ)： 大阪环球影城确实好玩，尤其是马里奥园区，做得太用心了。戴上AR手环，那些游戏里的场景和道具真的能互动，感觉自己就在游戏世界里。库巴城堡里的赛车游戏尤其刺激。哈利波特园区也还原得很好。不过热门项目排队时间确实长，建议提前研究下快速票（Express Pass）。为什么大家愿意排长队？我觉得主要是沉浸感做得特别好，让你不只是看，而是能“参与”进去，像真的进入了电影或游戏的世界，大人小孩都能玩得很开心。\n神户 须磨山 如果时间充裕，可以从大阪坐车去神户的须磨山（大约1小时）。坐缆车上山，站在山顶能同时看到濑户内海和壮观的明石海峡大桥，视野非常开阔。山上人不多，还有个动物园和植物园，山下有海滩，是个可以远离城市喧嚣、轻松逛逛的好地方。\n心斋桥 \u0026amp; 道顿堀 这里就是大阪夜生活的中心了，特别热闹。满街都是小吃摊和商店。章鱼烧外皮焦脆里面软糯，各种串烧香气扑鼻，可丽饼甜度刚好不腻，边走边吃很满足。除了吃的，药妆店也特别多，像眼药水、常用药、面膜、日用品什么的，种类全，价格也比国内便宜不少，看到很多人都是一袋子一袋子地买。晚上这里人虽然多，但秩序挺好，是一种热闹但不混乱的感觉。这里受欢迎很简单：东西好吃不贵，拍照有特色（比如那个著名的格力高跑步人招牌），买东西也实惠方便。\n🚇 交通与便利：旅行中的小确幸 公共交通 日本的电车（包括地铁和火车）系统很发达，但也确实有点复杂。同一个站台可能有不同速度的车（普通、快速、急行、特急），上车前一定要看清电子屏上的目的地和车种。换乘有时需要走点路，甚至上下楼。用Suica或ICOCA这种交通卡最方便，刷卡进出，自动扣费，省去买票的麻烦。强烈推荐用Google Maps查路线，会告诉你坐哪趟车、在几号站台、几点发车，非常准。\n新干线 城市间移动坐新干线很快很舒服。车厢干净，座位宽敞，时间也极准。我们坐了几次，体验都很好。车上很安静，不太有人大声说话或打电话。车站便利店卖的便当选择多，味道也不错，可以带上车吃。\n便利店 日本的便利店（7-11、Lawson、FamilyMart）真是旅行中的救星。早上买个热咖啡加饭团，中午饿了来个便当或面包，晚上回去前买点零食或饮料，一日三餐都能解决。它们还能打印、取票、寄快递、取钱（ATM支持银联卡），甚至能买景点门票，营业时间长，到处都有，干净又方便。\n🍱 吃吃喝喝：简单但满足 一人食友好 在日本吃饭，一个人特别自在。很多餐厅都有吧台位，菜单上定食套餐（一份主菜+米饭+汤+小菜）就是为一个人设计的，分量合适，点餐也方便，不少店门口还有自助点餐机，社恐友好。吃饭时没人催你，可以慢慢吃。\n印象深刻的美食 豆腐套餐\n和牛饭：\n居酒屋烧鸟：\n和牛自助：\n🎌 一点点感受 这次关西之行，从京都的古寺鸟居，到大阪的环球影城和夜市，感觉挺充实的。印象最深的是，在这里旅行很方便也很舒适。便利店解决了基本需求，电车和新干线准时高效，美食选择多而且对独自旅行的人很友好。\n京都适合慢慢走，感受历史和宁静；大阪则是放开了玩和吃的地方。如果你也打算去，不妨试试：\n早上在便利店解决早餐，方便快捷。 坐一次非特急的普通或快速电车，感受下当地人的通勤。 在清水寺这样的地方，回想下看过的动漫场景（如果你是动漫迷的话）。 找个地方，一个人安静地吃一份定食，慢慢感受。 总的来说，这是一次节奏比较舒缓但体验很丰富的旅行。希望以后有机会还能再去。也欢迎大家分享你们的日本旅行经验！\n","date":"2025-05-07T00:00:00Z","permalink":"https://MyLoveES.github.io/p/kyoto-osaka/","title":"kyoto \u0026 osaka"},{"content":"事件循环 一、事件循环的本质：从快递分拣中心说起 1.1 现实世界类比 1 2 3 4 5 6 7 8 9 10 假设你经营一家快递分拣中心： - **传统阻塞模型**（多线程）： 每个包裹（请求）分配一个工人（线程） 工人必须全程跟踪包裹：接收 → 分拣 → 装车 即使工人90%时间在等待货车，也不能处理其他包裹 - **事件循环模型**： 少数高效工人（事件循环线程） 每个工人管理多个传送带（Socket通道） 工人只处理就绪的包裹（就绪的I/O事件） 1.2 传统模型的瓶颈 事件循环（Event Loop）： 一种程序结构，通过无限循环持续监听并分发I/O事件，其核心组件包括：\n事件队列：存储待处理事件（新连接、数据到达等） 事件分发器：检测哪些通道（Channel）已就绪 事件处理器：处理具体I/O操作的回调函数 二、多路复用技术：事件循环的\u0026quot;火眼金睛\u0026quot; 2.1 从BIO到NIO的演进 模型 工作方式 资源消耗 适用场景 BIO 1线程1连接（阻塞等待） 随连接数线性增长 低并发场景 select 遍历所有fd检查状态 O(n)时间复杂度 少量连接 epoll 回调通知就绪事件 O(1)时间复杂度 万级连接 2.2 epoll的三大核心能力 2.2.1 红黑树管理文件描述符集 1 2 3 4 5 6 7 8 // 创建epoll实例 int epoll_fd = epoll_create1(0); // 添加监控描述符 struct epoll_event event; event.events = EPOLLIN | EPOLLET; // 边缘触发模式 event.data.fd = socket_fd; epoll_ctl(epoll_fd, EPOLL_CTL_ADD, socket_fd, \u0026amp;event); 2.2.1 就绪列表与事件回调 1 2 3 4 5 6 7 8 9 // 等待事件发生（毫秒级超时） int num_events = epoll_wait(epoll_fd, events, MAX_EVENTS, -1); // 处理就绪事件 for (int i = 0; i \u0026lt; num_events; i++) { if (events[i].events \u0026amp; EPOLLIN) { handle_read(events[i].data.fd); } } 2.2.3 触发模式选择 水平触发（LT）：只要缓冲区有数据就会持续通知 边缘触发（ET）：仅在状态变化时通知一次（性能更优） 三、Netty的事件循环实现 3.1 核心组件关系图 1 2 3 4 5 6 7 8 9 10 11 12 ┌───────────────────────────┐ │ EventLoopGroup │ │ ┌─────────────────────┐ │ │ │ NioEventLoop[] │ │ │ │ ┌─────────────────┐ │ │ │ │ │ Selector │ │ │ │ │ │ (epoll实例) │ │ │ │ │ └─────────────────┘ │ │ │ │ │ Task Queue │ │ │ │ │ └─────────────────┘ │ │ │ └─────────────────────┘ │ └───────────────────────────┘ 3.2 事件循环线程的生命周期 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 // 简化版事件循环伪代码 public void run() { while (!terminated) { // 阶段1：检测I/O事件 int readyChannels = selector.select(timeout); // 阶段2：处理I/O事件 if (readyChannels \u0026gt; 0) { Set\u0026lt;SelectionKey\u0026gt; keys = selector.selectedKeys(); for (SelectionKey key : keys) { if (key.isReadable()) { handleRead(key); } if (key.isWritable()) { handleWrite(key); } } keys.clear(); } // 阶段3：处理异步任务 runAllTasks(); } } 3.3 关键性能优化手段 I/O比例控制：通过ioRatio参数平衡I/O与任务处理时间 1 2 // 默认配置：I/O操作占用50%时间 NioEventLoopGroup group = new NioEventLoopGroup(4, new DefaultThreadFactory(), 50); 直接内存分配：使用ByteBuf避免JVM堆内存拷贝 零拷贝技术：文件传输通过FileRegion直接操作内核缓冲区 四、性能对比：理论 vs 现实 4.1 理论计算模型 C10K问题公式推导： 传统模型所需线程数 = 并发连接数 × (平均等待时间 / 平均处理时间) 假设：\n10,000并发连接 每个请求95%时间在等待（19ms等待 + 1ms处理） 传统模型线程数 = 10,000 × (19/1) = 190,000 → 完全不可行\n事件循环模型线程数 = CPU核心数（如4线程）→ 轻松应对\n五、从内核到应用：全链路视角看事件循环 5.1 Linux内核的工作流程 1 2 3 4 5 6 7 应用层（Java NIO） ↓ 系统调用（epoll_ctl/epoll_wait） VFS（虚拟文件系统层） ↓ 回调注册 网卡驱动 ↓ 硬件中断 DMA缓冲区 → 数据就绪 → 触发epoll回调 5.2 现代网络栈优化 RSS（接收端扩展）：多队列网卡分散中断压力 SO_REUSEPORT：允许多个Socket监听同一端口 Kernel Bypass：DPDK/XDK等用户态网络方案 六、最佳实践：如何最大化事件循环效率 6.1 配置原则 1 2 3 4 5 6 7 8 # 推荐Netty配置 server: netty: event-loop: boss-count: 1 # 接收连接线程数 worker-count: cpu_cores * 2 # 处理I/O线程数 max-initial-line-length: 8192 so-backlog: 1024 # 等待连接队列大小 6.2 禁忌事项 ❌ 在事件循环线程执行阻塞操作 ❌ 忘记释放ByteBuf导致内存泄漏 ❌ 在Handler中处理耗时业务逻辑 6.3 监控指标 1 2 3 4 关键Metric： reactor.netty.io.allocated.direct：直接内存使用量 reactor.netty.io.pending.tasks：待处理任务数 reactor.netty.io.select.latency：select操作延迟 ","date":"2025-03-10T00:00:00Z","permalink":"https://MyLoveES.github.io/p/event-loop/","title":"Event Loop"},{"content":"响应式编程 vs 传统同步模型 一、传统模型的性能困境 1.1 现实中的例子 1 2 3 4 假设一个银行有10个柜台（线程池大小为10）： - 每个客户（请求）需要占用柜台5分钟（包含等待IO的时间） - 当同时有100个客户时，90个客户必须排队等待 - 即使柜员实际工作时间只有1分钟（CPU计算），其他4分钟都在等待（IO阻塞） 1.2 传统模型的瓶颈 1 2 3 4 5 6 7 8 9 ┌───────────┐ ┌───────────┐ 请求队列 → │ 线程池 │ → 1:1 → │ 阻塞IO │ │ (200线程) │ │ (DB/HTTP) │ └───────────┘ └───────────┘ 关键问题： - 假设CPU处理本身需要10ms，而下游响应延迟从10ms升至500ms时： 吞吐量从 200/(0.01+0.01)=10,000 QPS 暴跌至 200/(0.5+0.01)=392 QPS 线程资源浪费：大量时间浪费在等待IO（数据库、网络调用） 高并发场景下：线程池爆满，请求排队，响应延迟飙升 例如：Tomcat默认200线程池，下游ASR响应慢时，线程池耗尽，新的请求无法接受，触发Pod重启。\n二、WebFlux：事件驱动 2.1 架构对比 2.1.1 传统Servlet模型 1 2 3 4 5 6 7 8 9 10 [工作流程] 1. 接受请求 → 分配线程 2. 线程执行 → 阻塞等待 3. 获取结果 → 返回响应 4. 释放线程 [资源时间线示例] 线程1：█░░░░░░░░░（80%时间在等待） 线程2：███░░░░░░░ 线程3：░░░░░░░░░░ 2.1.2 WebFlux响应式模型 1 2 3 4 5 6 7 8 9 10 [工作流程] 1. 接受请求 → 注册回调 2. 立即释放线程 → 处理其他请求 3. 下游响应就绪 → 事件循环调度处理 4. 生成响应 → 无需等待 [资源时间线示例] 线程1：██████████（持续处理事件） 线程2：██████████ （仅需2-4个核心线程） 2.1.3 传统模型 vs WebFlux模型对比 维度 传统模型（Servlet） WebFlux（Reactive） 线程模型 1请求1线程（阻塞） 少量线程 + 事件循环（非阻塞） 资源消耗 高（线程数≈并发数） 低（线程数≈CPU核心数） 编程范式 同步阻塞（Imperative） 异步非阻塞（Declarative） 吞吐量瓶颈 线程池大小 CPU/网络带宽 2.2 响应式的形象化解释 1 2 3 4 5 6 7 // 传统模型：同步等待结果（线程被阻塞） String data = database.query(); // \u0026lt;- 线程在这里卡住！ response.send(data); // WebFlux：订阅数据流（线程立即释放） Mono\u0026lt;String\u0026gt; mono = database.reactiveQuery(); mono.subscribe(data -\u0026gt; response.send(data)); 关键机制：\n事件循环（Event Loop） 少数线程轮询事件队列 当IO就绪时触发回调，不空等 背压（Backpressure） 生产者（Publisher）根据消费者（Subscriber）的处理能力动态调整数据流速 数据流操作符 使用Flux（0-N个元素）和Mono（0-1个元素）组合异步操作 1 2 3 4 5 6 7 webClient.get() .uri(\u0026#34;/api/users\u0026#34;) .retrieve() .bodyToFlux(User.class) .filter(user -\u0026gt; user.age \u0026gt; 18) .take(10) .subscribe(System.out::println); 三、WebFlux的优势与代价 3.1 优势：何时选择WebFlux 场景 传统模型 WebFlux 原因 流式数据传输（日志推送） ❌ ✅ 天然支持SSE/WebSocket CPU密集型任务 ✅ ❌ 非阻塞模型无优势 微服务网关（聚合请求） ⚠️ ✅ 异步组合多个服务响应 3.2 代价：使用WebFlux的挑战 3.2.1 编程习惯 从\u0026quot;按步骤执行\u0026quot;到\u0026quot;定义数据流管道\u0026quot; 调试困难：堆栈跟踪包含大量反应式操作符 不方便的“上下文传递”（需要使用Reactor的Context，以及自定义WebFilter等） 1 2 3 4 5 6 7 8 9 10 11 Mono\u0026lt;String\u0026gt; data = webClient.get() .uri(\u0026#34;/api\u0026#34;) .retrieve() .bodyToMono(String.class) .flatMap(response -\u0026gt; Mono.deferContextual(ctx -\u0026gt; { String requestId = ctx.get(\u0026#34;requestId\u0026#34;); // 从上下文中获取 return processResponse(response, requestId); }) ) .contextWrite(Context.of(\u0026#34;requestId\u0026#34;, \u0026#34;12345\u0026#34;)); // 写入上下文 3.2.2 生态限制 支持响应式的组件 传统阻塞组件 R2DBC JDBC WebClient RestTemplate 达梦数据库R2DBC大坑： https://note.youdao.com/s/K3VE8Se9 3.2.3 代码示例 四、性能测试 五、总结：WebFlux不是银弹 5.1 结论 适合：IO密集型、高并发、延迟敏感型系统 不适合：简单CRUD应用、强依赖阻塞生态的场景 核心价值：资源利用率提升，而非绝对性能 5.2 决策 Checklist 是否“真的”有高并发需求 能否使用响应式数据库驱动 是否承担调试和维护成本 ","date":"2025-03-09T00:00:00Z","permalink":"https://MyLoveES.github.io/p/why-reactor/","title":"Why Reactor"},{"content":"一、项目生成 1、Spring Boot CLI 首先，你需要安装 Spring Boot CLI。你可以参考 Spring Boot 官方文档中的安装指南：https://docs.spring.io/spring-boot/docs/current/reference/html/getting-started.html#getting-started.installing.cli 1 2 brew tap spring-io/tap brew install spring-boot 创建一个基于 Gradle 的 Spring Boot 项目： 1 spring init --build gradle-project --groupId=com.weasley --artifactId=mrsix --name=mrsix --description=\u0026#34;MrSix: Pipeline framework\u0026#34; mrsix 二、子模块 1) submodule-common 1 2 3 1) mkdir submodule-common 2) mkdir -p src/main/java/com/weasley/common 3) mkdir -p src/main/resources 2) submodule-domain 1 2 3 1) mkdir submodule-domain 2) mkdir -p src/main/java/com/weasley/domain 3) mkdir -p src/main/resources 3) submodule-application 1 2 3 1) mkdir submodule-application 2) mkdir -p src/main/java/com/weasley/application 3) mkdir -p src/main/resources 4) submodule-interface 1 2 3 1) mkdir submodule-interface 2) mkdir -p src/main/java/com/weasley/interface 3) mkdir -p src/main/resources 5) submodule-infrastructure 1 2 3 1) mkdir submodule-infrastructure 2) mkdir -p src/main/java/com/weasley/infrastructure 3) mkdir -p src/main/resources 2.1 gradle 根目录 setting.gradle\n1 2 3 4 5 include \u0026#39;submodule-common\u0026#39; include \u0026#39;submodule-domain\u0026#39; include \u0026#39;submodule-application\u0026#39; include \u0026#39;submodule-interface\u0026#39; include \u0026#39;submodule-infrastructure\u0026#39; 根目录 build.gradle\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 // 定义项目构建所需的仓库和依赖项 // 定义项目构建所需的仓库和依赖项 buildscript { repositories { mavenCentral() } dependencies { classpath(\u0026#39;org.springframework.boot:spring-boot-gradle-plugin:3.0.4\u0026#39;) } } plugins { id \u0026#39;java\u0026#39; id \u0026#39;groovy\u0026#39; id \u0026#39;java-library\u0026#39; id \u0026#39;org.springframework.boot\u0026#39; version \u0026#39;3.0.4\u0026#39; id \u0026#39;io.spring.dependency-management\u0026#39; version \u0026#39;1.1.0\u0026#39; } allprojects{ group = \u0026#39;com.weasley\u0026#39; version = \u0026#39;0.0.1-SNAPSHOT\u0026#39; sourceCompatibility = \u0026#39;17\u0026#39; repositories { mavenCentral() } dependencyManagement { imports { mavenBom \u0026#39;org.springframework.boot:spring-boot-dependencies:3.0.4\u0026#39; } } } subprojects { apply plugin: \u0026#39;java\u0026#39; apply plugin: \u0026#39;java-library\u0026#39; apply plugin: \u0026#39;groovy\u0026#39; apply plugin: \u0026#39;org.springframework.boot\u0026#39; apply plugin: \u0026#39;io.spring.dependency-management\u0026#39; dependencies {} tasks.named(\u0026#39;test\u0026#39;) { useJUnitPlatform() } } 2.2 maven 1 mvn archetype:generate -DgroupId=com.weasley -DartifactId=sdk-common -Dversion=1.0.0 -DinteractiveMode=false 三、增加通用依赖 1. spring-boot-devtools 1 developmentOnly \u0026#39;org.springframework.boot:spring-boot-devtools\u0026#39; 2. lombok 1 2 compileOnly \u0026#39;org.projectlombok:lombok:1.18.20\u0026#39; annotationProcessor \u0026#39;org.projectlombok:lombok:1.18.20\u0026#39; 3. spring-boot-configuration-processor 1 annotationProcessor \u0026#34;org.springframework.boot:spring-boot-configuration-processor\u0026#34; 4. validation 1 implementation group: \u0026#39;org.springframework.boot\u0026#39;, name: \u0026#39;spring-boot-starter-validation\u0026#39;, version: \u0026#39;3.0.4\u0026#39; 5. mapstruct 1 2 implementation \u0026#39;org.mapstruct:mapstruct:1.5.3.Final\u0026#39; annotationProcessor \u0026#39;org.mapstruct:mapstruct-processor:1.5.3.Final\u0026#39; 6. knife4j 1 implementation group: \u0026#39;com.github.xiaoymin\u0026#39;, name: \u0026#39;knife4j-openapi3-jakarta-spring-boot-starter\u0026#39;, version: \u0026#39;4.0.0\u0026#39; 7. mybatis plus 1 2 3 4 implementation group: \u0026#39;com.baomidou\u0026#39;, name: \u0026#39;mybatis-plus-boot-starter\u0026#39;, version: \u0026#39;3.5.3.1\u0026#39; implementation group: \u0026#39;com.baomidou\u0026#39;, name: \u0026#39;mybatis-plus-generator\u0026#39;, version: \u0026#39;3.5.3.1\u0026#39; implementation group: \u0026#39;org.freemarker\u0026#39;, name: \u0026#39;freemarker\u0026#39;, version: \u0026#39;2.3.31\u0026#39; implementation group: \u0026#39;mysql\u0026#39;, name: \u0026#39;mysql-connector-java\u0026#39;, version: \u0026#39;8.0.32\u0026#39; 8. spock 1 2 testImplementation group: \u0026#39;org.spockframework\u0026#39;, name: \u0026#39;spock-core\u0026#39;, version: \u0026#39;2.3-groovy-4.0\u0026#39; testImplementation group: \u0026#39;org.spockframework\u0026#39;, name: \u0026#39;spock-spring\u0026#39;, version: \u0026#39;2.3-groovy-4.0\u0026#39; 9. h2 1 testImplementation group: \u0026#39;com.h2database\u0026#39;, name: \u0026#39;h2\u0026#39;, version: \u0026#39;2.1.214\u0026#39; 10. embedded-redis 1 2 3 testImplementation (group: \u0026#39;it.ozimov\u0026#39;, name: \u0026#39;embedded-redis\u0026#39;, version: \u0026#39;0.7.3\u0026#39;) { exclude group: \u0026#39;org.slf4j\u0026#39;, module: \u0026#39;slf4j-simple\u0026#39; } 11. actuator 1 implementation group: \u0026#39;org.springframework.boot\u0026#39;, name: \u0026#39;spring-boot-starter-actuator\u0026#39;, version: \u0026#39;3.0.5\u0026#39; 12. redission 1 implementation group: \u0026#39;org.redisson\u0026#39;, name: \u0026#39;redisson-spring-boot-starter\u0026#39;, version: \u0026#39;3.20.0\u0026#39; 13. caffeine 1 implementation group: \u0026#39;com.github.ben-manes.caffeine\u0026#39;, name: \u0026#39;caffeine\u0026#39;, version: \u0026#39;3.1.5\u0026#39; 14. hutool-core 1 2 implementation group: \u0026#39;cn.hutool\u0026#39;, name: \u0026#39;hutool-core\u0026#39;, version: \u0026#39;5.8.15\u0026#39; implementation group: \u0026#39;cn.hutool\u0026#39;, name: \u0026#39;hutool-extra\u0026#39;, version: \u0026#39;5.8.16\u0026#39; 15. minio 1 implementation group: \u0026#39;io.minio\u0026#39;, name:\u0026#39;minio\u0026#39;, version: \u0026#39;8.4.3\u0026#39; 16. vavr 1 implementation group: \u0026#39;io.vavr\u0026#39;, name: \u0026#39;vavr\u0026#39;, version: \u0026#39;0.10.4\u0026#39; 17. apm 1 implementation group: \u0026#39;org.apache.skywalking\u0026#39;, name: \u0026#39;apm-toolkit-logback-1.x\u0026#39;, version: \u0026#39;8.15.0\u0026#39; 18. json-path 1 implementation group: \u0026#39;com.jayway.jsonpath\u0026#39;, name: \u0026#39;json-path\u0026#39;, version: \u0026#39;2.8.0\u0026#39; 添加数据库表 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 import com.baomidou.mybatisplus.generator.FastAutoGenerator; import com.baomidou.mybatisplus.generator.config.OutputFile; import com.baomidou.mybatisplus.generator.engine.FreemarkerTemplateEngine; import org.junit.jupiter.api.Test; import java.util.Collections; public class CodeGenerator { @Test public void generator() { FastAutoGenerator.create(\u0026#34;jdbc:mysql://localhost:3306/file_center?serverTimezone=Asia/Shanghai\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;\u0026#34;) .globalConfig(builder -\u0026gt; { builder.author(\u0026#34;baomidou\u0026#34;) // 设置作者 .enableSwagger() // 开启 swagger 模式 .fileOverride() // 覆盖已生成文件 .outputDir(\u0026#34;/Users/disco/Downloads/outputFileCenter\u0026#34;); // 指定输出目录 }) .packageConfig(builder -\u0026gt; { builder.parent(\u0026#34;com.baomidou.mybatisplus.samples.generator\u0026#34;) // 设置父包名 .moduleName(\u0026#34;system\u0026#34;) // 设置父包模块名 .pathInfo(Collections.singletonMap(OutputFile.xml, \u0026#34;/Users/disco/Downloads/outputFileCenter\u0026#34;)); // 设置mapperXml生成路径 }) .strategyConfig(builder -\u0026gt; { builder.addInclude(Collections.emptyList()); // 设置需要生成的表名 // .addTablePrefix(\u0026#34;t_\u0026#34;, \u0026#34;c_\u0026#34;); // 设置过滤表前缀 }) .templateEngine(new FreemarkerTemplateEngine()) // 使用Freemarker引擎模板，默认的是Velocity引擎模板 .execute(); } } ","date":"2025-03-06T00:00:00Z","permalink":"https://MyLoveES.github.io/p/build-spring-web-project/","title":"Build spring web project"},{"content":" R: 4.3.2 (2023-10-31)\nR studio: 2023.12.1+402 (2023.12.1+402)\n市场分析师经常研究不同群体之间的差异:\n按人群对数据进行分组：男性或女性更倾向于订阅我们的服务？哪个人口统计分段最有能力购买我们的产品？该产品更受房主还是租户的青睐？ 按地理位置对数据进行分组：A地区的表现是否优于B地区？ 按实验操作对数据进行分组：广告版本A的转化率是否高于广告版本B？ 按时间对数据进行分组：类似邮件或促销活动后，同店销售是否增加了？\n在所有这些情况下，我们将一个数据组与另一个数据组进行比较，以识别效应。本教程探讨了市场营销中经常出现的这些比较类型。 1 Descriptives by group 我们使用消费者细分数据。我们对两个新开发的广告（叙事型 vs. 信息型）的效果感兴趣，并收集了来自300名受访者的数据，包括他们的年龄、性别、子女数量、是否喜欢广告以及他们在页面上花费的时间。在这些数据中，每个受访者来自我们的四个消费者细分之一：郊区混合、都市潮流、旅行者或晋升中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \u0026gt; ad.df \u0026lt;- read.csv(\u0026#34;Data_Compare_Groups.csv\u0026#34;, stringsAsFactors = TRUE) \u0026gt; summary(ad.df) condition like seconds_spent age gender kids segment control :159 likeNo :260 Min. : 0.69 Min. :19.00 Female:157 Min. :0.00 Moving up : 70 treatment:141 likeYes: 40 1st Qu.: 39.66 1st Qu.:33.00 Male :143 1st Qu.:0.00 Suburb mix:100 Median : 52.02 Median :39.50 Median :1.00 Travelers : 80 Mean : 50.98 Mean :41.17 Mean :1.27 Urban hip : 50 3rd Qu.: 61.41 3rd Qu.:48.00 3rd Qu.:2.00 Max. :114.28 Max. :80.00 Max. :7.00 \u0026gt; str(ad.df) \u0026#39;data.frame\u0026#39;:\t300 obs. of 7 variables: $ condition : Factor w/ 2 levels \u0026#34;control\u0026#34;,\u0026#34;treatment\u0026#34;: 1 2 2 1 2 2 1 1 1 2 ... $ like : Factor w/ 2 levels \u0026#34;likeNo\u0026#34;,\u0026#34;likeYes\u0026#34;: 1 1 1 1 1 1 1 1 1 1 ... $ seconds_spent: num 49.5 35.5 44.2 81 79.3 ... $ age : int 47 31 43 37 41 43 38 28 44 35 ... $ gender : Factor w/ 2 levels \u0026#34;Female\u0026#34;,\u0026#34;Male\u0026#34;: 2 2 2 1 1 2 2 2 1 1 ... $ kids : int 2 1 0 1 3 4 3 0 1 0 ... $ segment : Factor w/ 4 levels \u0026#34;Moving up\u0026#34;,\u0026#34;Suburb mix\u0026#34;,..: 2 2 2 2 2 2 2 2 2 2 ... 我们对广告的两个版本在诸如浏览时长和喜欢程度等指标上的变化感兴趣。\n一种临时的方法是使用数据框索引：找到符合某些条件的行，然后取另一个统计量的平均值。例如，要找出叙事型广告（处理组）的平均总浏览时长。\n1 2 3 4 5 6 \u0026gt; mean(ad.df$seconds_spent[ad.df$condition == \u0026#34;treatment\u0026#34;]) [1] 55.01809 \u0026gt; # We could further narrow the cases to Moving up respondents who are in the treatment condition: \u0026gt; mean(ad.df$seconds_spent[ad.df$condition == \u0026#34;trea ...\u0026#34; ... [TRUNCATED] [1] 50.21652 1.1 aggregate() 当你想要找到多个组的值时，一个更通用的方法是使用aggregate()函数。\n1 2 3 4 \u0026gt; aggregate(ad.df$seconds_spent, list(ad.df$condition), mean) Group.1 x 1 control 47.39132 2 treatment 55.01809 1.2 Basic formula syntax R通过公式规范提供了描述变量之间关系的标准方法。一个公式使用波浪线（~）运算符将左侧的响应变量与右侧的解释变量分开。基本形式是：\n1 y ~ x（简单公式） 这在R的许多情境中使用，其中响应和解释的含义取决于情况。例如，在线性回归中，上述简单公式将y建模为x的线性函数。在aggregate()命令的情况下，效果是根据x的水平对y进行聚合。 让我们实践一下。不使用aggregate(ad.dfsecondsspent,list(ad.dfcondition), mean)的方式，而是使用通用形式aggregate(formula, data, FUN)。在我们的例子中，我们告诉R“在数据集ad.df中按照条件取seconds_spent，并对每个组应用平均函数”。\n1 2 3 4 \u0026gt; aggregate(seconds_spent ~ condition, data = ad.df, mean) condition seconds_spent 1 control 47.39132 2 treatment 55.01809 1.3 Descriptives for two-way groups 在市场营销中的一个常见任务是交叉制表，根据两个（或更多）因素将客户分成组。公式语法使得通过指定多个解释变量来计算交叉制表变得容易：\ny ~ x1 + x2 + . . . (Multiple variable formula)\n1.3.1 Means 利用这种格式与aggregate()，我们可以这样写：\n1 2 3 4 5 6 7 8 9 10 \u0026gt; aggregate(seconds_spent ~ segment + condition, data = ad.df, mean) segment condition seconds_spent 1 Moving up control 54.49787 2 Suburb mix control 54.93308 3 Travelers control 63.18900 4 Urban hip control 21.33800 5 Moving up treatment 50.21652 6 Suburb mix treatment 55.14312 7 Travelers treatment 62.08533 8 Urban hip treatment 23.05800 现在，我们为每个段和条件的组合都有一个单独的组，并且可以开始看到seconds_spent与段和条件变量之间的关系。\n我们可以将结果分配给一个数据框对象并进行索引：\n1 agg.data \u0026lt;- aggregate(seconds_spent ~ segment + condition, data = ad.df, mean) aggregate()命令允许我们计算连续变量的函数，例如seconds_spent或like的平均值，适用于任何因素的组合（如段、条件等）。这是市场研究中的一个常见任务，许多公司专门生产交叉制表。\n1.3.2 Frequencies 1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026gt; table(ad.df$condition, ad.df$like) likeNo likeYes control 137 22 treatment 123 18 \u0026gt; table(ad.df$segment, ad.df$like) likeNo likeYes Moving up 56 14 Suburb mix 94 6 Travelers 70 10 Urban hip 40 10 我们可以将计数相加以找到它们的总数。例如，kids是一个计数变量；如果一个受访者报告有3个孩子，那么这就是一个计数为3，我们可以将计数相加以得到每个段中报告的孩子总数。我们可以使用aggregate(. . . , sum)：\n1 2 3 4 5 6 \u0026gt; aggregate(kids ~ segment, data = ad.df, sum) segment kids 1 Moving up 134 2 Suburb mix 192 3 Travelers 0 4 Urban hip 55 结果显示，“都市上升”受访者报告了总共55个孩子，而“旅行者”没有报告。\n2 Visualization by group 2.1 Visualizing frequencies and proportions Lattice包提供了一个有用的解决方案：histogram (formula, data, type)。它理解公式的概念，包括对因子进行条件划分（\u0026quot; | \u0026ldquo;），这意味着根据因子将绘图分成多个面板。\n1 histogram(~ like | condition, data = ad.df) 结果显示，两个广告版本生成的喜欢程度相似。\n你会注意到在这个公式中，波浪线（~）前面没有响应变量，只有它后面的解释变量（condition）。histogram()默认情况下，假设我们想要绘制每个喜欢程度水平上的人群比例。我们在条件上进行了绘图，告诉histogram为每个段生成一个单独的直方图。 histogram()的默认方式是在每个组内绘制比例，使得值相对于组的大小。如果我们想要实际计数，可以包含参数type = \u0026ldquo;count\u0026rdquo;。\n{% asset_image R_week9_code_2.png %}\n我们可以通过使用“+”在多个因素上添加条件。例如，每个段内的订阅者比例如何根据住房所有权来计算？\n{% asset_image R_week9_code_3.png %}\n结果告诉我们，根据广告条件在段内的喜欢程度差异很小。这意味着两个广告版本在喜欢程度上产生的差异很小。\n2.2 Visualizing continuous data 在前面的部分中，我们看到了如何绘制计数和比例。那么连续数据呢？我们如何绘制我们数据中按条件分组的总浏览时间？一个简单的方法是使用aggregate()来找到平均总时间，然后使用lattice包中的barchart()来绘制计算出的值：\n1 2 3 \u0026gt; ad.mean \u0026lt;- aggregate(seconds_spent ~ condition, data = ad.df, mean) \u0026gt; barchart(seconds_spent ~ condition, data = ad.mean, col = \u0026#34;grey\u0026#34;) {% asset_image R_week9_code_4.png %}\n我们如何进一步按段拆分数据？首先，我们必须对数据进行聚合，以在公式中包括两个因素。然后，我们通过添加参数groups=factor，告诉barchart()使用段作为分组变量。\n1 2 3 \u0026gt; ad.seconds.agg \u0026lt;- aggregate (seconds_spent ~ condition + segment, data = ad.df, mean) \u0026gt; barchart(seconds_spent ~ condition, data = ad.seconds.agg ,groups = segment, auto.key=TRUE) {% asset_image R_week9_code_5.png %}\n用于比较不同组的连续数据值（如不同组的seconds_spent）的更具信息性的图表是箱线图。箱线图比柱状图更好，因为它显示了更多关于值分布的信息。\n{% asset_image R_week9_code_6.png %}\n2.2.1 bwplot() * 对于箱线图的更好选择是lattice包中的bwplot()命令，它提供了更漂亮的图表，并且允许多因素条件化。需要注意的一点是，bwplot()使用的模型公式方向与你可能期望的相反；你写的是condition ~ seconds_spent。我们按段绘制水平箱线图如下：\n1 bwplot(condition ~ seconds_spent, data = ad.df, horizontal = TRUE, xlab = \u0026#34;Total seconds spent\u0026#34;) {% asset_image R_week9_code_7.png %}\n我们可以将处理条件作为一个条件变量：\n1 bwplot(condition ~ seconds_spent | segment, data = ad.df, horizontal = TRUE, xlab = \u0026#34;seconds_spent\u0026#34;) {% asset_image R_week9_code_8.png %}\n对于按段和广告条件分组的总浏览时长的条件化图表显示，旅行者细分中处于处理组的人的总浏览时长分布比处于对照组的人的分布要宽得多。\n3 Statistical tests 除了使用上面描述的组平均值和交叉制表总结组间的差异外，一个优秀的分析师能够使用统计检验来确定差异是否真实，或者可能是由于数据中的微小变化（“噪音”）造成的。我们应该专注于帮助识别真实差异的统计检验。\n3.1 Testing group frequencies: chisq.test() 卡方检验用于频率计数，例如由table产生的计数。卡方检验确定单元格中的频率是否与基于它们的总计数的预期值显着不同。在R中，我们使用chisq.test()命令。一般来说，chisq.test()操作在一个表上。\n喜欢行为是否独立于条件？也就是说，在我们的数据中，受访者无论看到哪个版本的广告，他们都同样可能喜欢吗？我们构建一个二维表并对其进行测试：\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026gt; table(ad.df$like, ad.df$condition) control treatment likeNo 137 123 likeYes 22 18 \u0026gt; chisq.test(table(ad.df$like, ad.df$condition)) Pearson\u0026#39;s Chi-squared test with Yates\u0026#39; continuity correction data: table(ad.df$like, ad.df$condition) X-squared = 0.010422, df = 1, p-value = 0.9187 在这种情况下，零假设是因素之间没有关联。也就是说，单元格中的计数与边际比例一样符合预期。基于较高的p值，我们无法拒绝零假设，并得出结论：因素之间没有关联，喜欢与条件在我们的数据中是独立的。喜欢和条件之间没有关系。\n3.2 Testing group means: t.test() t检验比较一个样本的平均值与另一个样本的平均值（或与一个特定值，如0）之间的差异。重要的是，它比较了完全两组数据的平均值。例如，在数据中，我们可能会询问两种广告条件之间的总浏览时间是否有所不同。\n我们使用t.test(formula, data)测试两组之间的浏览时间（处理组 vs. 控制组）的差异：\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026gt; t.test(seconds_spent ~ condition, data = ad.df) Welch Two Sample t-test data: seconds_spent by condition t = -3.3297, df = 286.45, p-value = 0.0009832 alternative hypothesis: true difference in means between group control and group treatment is not equal to 0 95 percent confidence interval: -12.135137 -3.118392 sample estimates: mean in group control mean in group treatment 47.39132 55.01809 在t.test()输出中有几个重要的信息：\nt统计量为-3.12，p值为0.0010。这意味着拒绝了没有广告条件下浏览时间差异的零假设。这表明在处理组看到广告版本的人花费了更长的时间。 差异的95%置信区间为-12.14到-3.12。我们可以有95%的信心认为组之间的差异在这些值之间。 我们数据的样本均值：控制条件下的平均总浏览时间为47.39132，处理条件下为55.01809。 在旅行者细分中的差异如何？我们可以使用过滤数据=子集数据, 条件选择只有旅行者，并重复测试：\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026gt; t.test(seconds_spent ~ condition, data = subset(ad.df, segment == \u0026#34;Travelers\u0026#34;)) Welch Two Sample t-test data: seconds_spent by condition t = 0.22758, df = 52.758, p-value = 0.8209 alternative hypothesis: true difference in means between group control and group treatment is not equal to 0 95 percent confidence interval: -8.624575 10.831909 sample estimates: mean in group control mean in group treatment 63.18900 62.08533 置信区间从-8.62到10.83包含了0，并且p值为0.82。因此，我们得出结论，在我们的数据中，旅行者中两个条件之间的平均浏览时间没有显著差异。\n3.3 Testing multiple-group means: ANOVA 方差分析（ANOVA）比较多个组的平均值。零假设是多个平均值之间没有差异。\nANOVA可以处理单个因素（称为单因素ANOVA）、两个因素（双因素）、以及更高阶的因素，包括因素之间的交互作用。\nANOVA的基本R命令是aov(formula, data)来设置模型，然后是anova(model)来显示标准的ANOVA汇总。 例如，我们想要回答这个问题：浏览时间与条件、段成员资格或两者之间是否相关？我们可以将seconds_spent建模为对条件的响应：\n1 2 3 4 5 6 7 8 9 10 11 \u0026gt; ad.aov.con \u0026lt;- aov(seconds_spent ~ condition, data = ad.df) \u0026gt; anova(ad.aov.con) Analysis of Variance Table Response: seconds_spent Df Sum Sq Mean Sq F value Pr(\u0026gt;F) condition 1 4347 4346.9 11.195 0.0009251 *** Residuals 298 115706 388.3 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 在两个条件之间的浏览时间存在显著差异（与我们从t检验中得出的结论相同）。\n为了测试浏览时间是否同时受到条件和段的影响，我们可以将两个因素都添加到ANOVA模型中进行测试：\n1 2 3 4 5 6 7 8 9 10 \u0026gt; anova(aov(seconds_spent ~ segment + condition, data = ad.df)) # combine two commands Analysis of Variance Table Response: seconds_spent Df Sum Sq Mean Sq F value Pr(\u0026gt;F) segment 3 55237 18412.2 83.8824 \u0026lt;2e-16 *** condition 1 64 63.9 0.2913 0.5898 Residuals 295 64753 219.5 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 结果表明，当我们尝试用段和条件来解释总浏览时间的差异时，段是一个显著的预测因子，但条件不是一个显著的预测因子。然而，先前的结果表明条件是显著的。这种差异是什么？这意味着段和条件不是独立的，而且段成员资格单独就足以捕捉到效应。条件仅仅比段解释了稍微多一点。\n3.3.1 Visualize ANOVA result 可视化ANOVA结果的一个好方法是绘制组均值的置信区间。我们使用multcomp（多重比较）包和它的glht(model)（一般线性假设）命令。\n默认的aov()模型具有一个截距项（对应于一个段），而所有其他段相对于该截距项。这可能会让决策者或客户难以理解，因此我们发现最好通过在模型公式中添加“0”来移除截距项：\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026gt; ad.aov \u0026lt;- aov (seconds_spent ~ 0 + segment, data = ad.df) \u0026gt; glht(ad.aov) General Linear Hypotheses Linear Hypotheses: Estimate segmentMoving up == 0 53.09 segmentSuburb mix == 0 55.03 segmentTravelers == 0 62.36 segmentUrban hip == 0 21.68 在移除截距项后，glht()为我们提供了每个段的均值。我们使用plot()函数绘制它，使用par(mar =. . . )命令为长轴标签添加一些额外的边距：\n1 2 3 # cex.axis = 0.8 makes the axis labels smaller to 80%. \u0026gt; plot(glht(ad.aov), + xlab = \u0026#34;Total seconds spent\u0026#34;, main = \u0026#34;Average seconds spent by Segment (95% CI)\u0026#34;, cex.axis = 0.8) 点表示每个段的均值，条形图反映了置信区间。我们可以看到每个段中平均浏览时间的置信区间。很明显，都市潮流段成员的平均浏览时间明显低于其他三组。\n3.4 Testing group means: lm() 你也可以使用lm()回归来比较组，当响应变量是连续变量时，比如总浏览时间。 分组变量（因子）必须被重新编码为虚拟变量（即取0和1为值的变量）。例如，因子“条件”（包括“对照”和“处理”）应该被重新编码为“dummy_condition: 0 = 对照，1 = 处理”。\n具有多于两个水平的因子（n）应该使用多个虚拟变量（n-1）进行重新编码。例如，段有四个水平，可以使用三个虚拟变量进行重新编码：dummy_s，dummy_u，dummy_t：\n1 2 3 4 郊区混合：dummy_s = 1，dummy_u = 0，dummy_t = 0 都市潮流：dummy_s = 0，dummy_u = 1，dummy_t = 0 旅行者：dummy_s = 0，dummy_u = 0，dummy_t = 1 晋升中：dummy_s = 0，dummy_u = 0，dummy_t = 0（参考水平） 虚拟变量应该作为预测变量输入模型。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \u0026gt; ad.df.reg \u0026lt;- ad.df %\u0026gt;% + mutate(dummy_condition = ifelse(condition == \u0026#34;treatment\u0026#34;,1,0), + dummy_s = ifelse(segment == \u0026#34;Suburb mix\u0026#34;,1,0), .... [TRUNCATED] \u0026gt; regression \u0026lt;- lm(seconds_spent ~ dummy_condition, data = ad.df.reg) \u0026gt; summary(regression) Call: lm(formula = seconds_spent ~ dummy_condition, data = ad.df.reg) Residuals: Min 1Q Median 3Q Max -54.33 -11.02 0.97 12.10 66.07 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 47.391 1.563 30.327 \u0026lt; 2e-16 *** dummy_condition 7.627 2.279 3.346 0.000925 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 19.7 on 298 degrees of freedom Multiple R-squared: 0.03621,\tAdjusted R-squared: 0.03297 F-statistic: 11.2 on 1 and 298 DF, p-value: 0.0009251 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \u0026gt; regression \u0026lt;- lm(seconds_spent ~ dummy_s + dummy_u + dummy_t, data = ad.df.reg) \u0026gt; summary(regression) Call: lm(formula = seconds_spent ~ dummy_s + dummy_u + dummy_t, data = ad.df.reg) Residuals: Min 1Q Median 3Q Max -61.671 -7.059 -0.442 6.281 51.919 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 53.091 1.769 30.017 \u0026lt; 2e-16 *** dummy_s 1.943 2.306 0.842 0.400214 dummy_u -31.409 2.740 -11.463 \u0026lt; 2e-16 *** dummy_t 9.270 2.422 3.828 0.000158 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 14.8 on 296 degrees of freedom Multiple R-squared: 0.4601,\tAdjusted R-squared: 0.4546 F-statistic: 84.08 on 3 and 296 DF, p-value: \u0026lt; 2.2e-16 3.5 Difference-in-Difference 1 2 3 4 5 6 7 \u0026gt; panel.df.raw \u0026lt;- read.csv(\u0026#34;Data_Panel.csv\u0026#34;, stringsAsFactors = TRUE) \u0026gt; str(panel.df.raw) \u0026#39;data.frame\u0026#39;:\t70 obs. of 3 variables: $ market: Factor w/ 7 levels \u0026#34;A\u0026#34;,\u0026#34;B\u0026#34;,\u0026#34;C\u0026#34;,\u0026#34;D\u0026#34;,..: 1 1 1 1 1 1 1 1 1 1 ... $ year : int 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 ... $ profit: num 13428 -18997 -112 26458 30083 ... 创建一个虚拟变量来指示治疗开始的时间。假设治疗始于2014年。在这种情况下，2014年之前的年份将取值为0，2014年之后的年份将取值为1。\n1 2 \u0026gt; panel.df\u0026lt;-panel.df.raw %\u0026gt;% + mutate(time = ifelse(panel.df.raw$year \u0026gt;= 2014, 1, 0)) 然后，我们创建一个虚拟变量来识别接受治疗的群体。在这个例子中，市场5、6和7是治疗组（=1）。市场1-4没有受到影响（=0）。\n1 2 3 4 panel.df\u0026lt;-panel.df %\u0026gt;% mutate(treatment = ifelse(panel.df$market == \u0026#34;E\u0026#34; | panel.df$market== \u0026#34;F\u0026#34; | panel.df$market == \u0026#34;G\u0026#34;, 1, 0)) 创建一个时间和受治疗之间的交互项。我们将这个交互项称为“did”。\n1 2 3 4 5 6 7 8 9 10 \u0026gt; panel.df\u0026lt;-panel.df %\u0026gt;% mutate(did = time * treatment) \u0026gt; str(panel.df) \u0026#39;data.frame\u0026#39;:\t70 obs. of 6 variables: $ market : Factor w/ 7 levels \u0026#34;A\u0026#34;,\u0026#34;B\u0026#34;,\u0026#34;C\u0026#34;,\u0026#34;D\u0026#34;,..: 1 1 1 1 1 1 1 1 1 1 ... $ year : int 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 ... $ profit : num 13428 -18997 -112 26458 30083 ... $ time : num 0 0 0 0 1 1 1 1 1 1 ... $ treatment: num 0 0 0 0 0 0 0 0 0 0 ... $ did : num 0 0 0 0 0 0 0 0 0 0 ... 3.5.1 Estimating the DID estimator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \u0026gt; didreg \u0026lt;- lm(profit ~ treatment + time + did, data = panel.df) \u0026gt; summary(didreg) Call: lm(formula = profit ~ treatment + time + did, data = panel.df) Residuals: Min 1Q Median 3Q Max -97675 -16228 1167 13928 68071 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 3581 7382 0.485 0.6292 treatment 17760 11276 1.575 0.1200 time 22894 9530 2.402 0.0191 * did -25195 14557 -1.731 0.0882 . --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 29530 on 66 degrees of freedom Multiple R-squared: 0.08273,\tAdjusted R-squared: 0.04104 F-statistic: 1.984 on 3 and 66 DF, p-value: 0.1249 “did”的系数是差异中的差异估计量。治疗效应在10%的显著性水平下具有边际意义，治疗效应为负。\n4 Takeaways 在描述和可视化组数据时：\naggregate()更强大；它理解公式模型，并生成一个可重用的、可索引的对象来存储结果。 频率可以使用table()函数找到。 对于某个因子的比例和出现频率的图表，lattice包中的histogram()命令非常合适。 对于因子的连续数据绘图，可以使用barchart()，或者更好的是使用box-and-whiskers plots（箱线图）和boxplot()。lattice包通过公式规范和bwplot()命令扩展了这样的绘图到多个因子。 在进行组间差异的统计测试时：\nchisq.test()函数用于在表格数据上找到置信区间，并进行假设检验。 t.test()是检验两组（或一组和一个固定值）均值差异的常见方法。 方差分析（ANOVA）是一种更一般的方法，用于测试由一个或多个因素识别的多个组之间的均值差异。基本模型使用aov()拟合，常见的汇总统计量使用anova()报告。 anova()命令还可以用于比较两个或更多ANOVA模型或其他线性模型，前提是它们是嵌套模型。 从multicomp包中绘制glht()对象是可视化ANOVA模型置信区间的好方法。 线性回归lm()是检查组间差异的更一般的方法。对于分组变量，虚拟编码很重要。 差异-差异是通过检查治疗和时间之间的交互项系数来测试的。 ","date":"2024-04-03T00:00:00Z","permalink":"https://MyLoveES.github.io/p/rweek9-managing-resource-trade-offs-ii-code/","title":"R[week9] Managing Resource Trade-Offs II Code"},{"content":" R: 4.3.2 (2023-10-31)\nR studio: 2023.12.1+402 (2023.12.1+402)\n1 Marketing Effectiveness and Resource Allocation Leverage historical data to quantify the effectiveness of marketing actions.\n1 2 3 4 5 6 7 8 9 \u0026gt; spending.data \u0026lt;- read_xls(\u0026#34;advertising spending 1.xls\u0026#34;) \u0026gt; str(spending.data) tibble [200 × 5] (S3: tbl_df/tbl/data.frame) $ week : num [1:200] 1 2 3 4 5 6 7 8 9 10 ... $ tv : num [1:200] 230.1 44.5 17.2 151.5 180.8 ... $ radio : num [1:200] 37.8 39.3 45.9 41.3 10.8 48.9 32.8 19.6 2.1 2.6 ... $ newspaper: num [1:200] 69.2 45.1 69.3 58.5 58.4 75 23.5 11.6 1 21.2 ... $ sales : num [1:200] 22.1 10.4 9.3 18.5 12.9 7.2 11.8 13.2 4.8 10.6 ... 数据包含每周的销售额以及电视、广播和报纸广告费用（以千英镑计）。\n1.1 Does radio advertising affect sales for an electronics brand? Visualizing the data\n1 2 # scatter plot plot(spending.data$radio, spending.data$sales) {% asset_image R_week8_code_1.png %}\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \u0026gt; # Correlation between radio advertising and sales \u0026gt; cor(spending.data$radio, spending.data$sales) [1] 0.5762226 \u0026gt; regression \u0026lt;- lm(sales ~ radio, data = spending.data) \u0026gt; summary(regression) Call: lm(formula = sales ~ radio, data = spending.data) Residuals: Min 1Q Median 3Q Max -15.7305 -2.1324 0.7707 2.7775 8.1810 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 9.31164 0.56290 16.542 \u0026lt;2e-16 *** radio 0.20250 0.02041 9.921 \u0026lt;2e-16 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 4.275 on 198 degrees of freedom Multiple R-squared: 0.332,\tAdjusted R-squared: 0.3287 F-statistic: 98.42 on 1 and 198 DF, p-value: \u0026lt; 2.2e-16 问题：如果我在广播广告上花费 40,000 英镑，我将卖出多少？答案：17.31 = 9.31 +0.20 * 40\n1.2 Accounting for multiple predictors: multiple linear regression 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 \u0026gt; regression \u0026lt;- lm(sales ~ radio + tv, data=spending.data) \u0026gt; summary(regression) Call: lm(formula = sales ~ radio + tv, data = spending.data) Residuals: Min 1Q Median 3Q Max -8.7977 -0.8752 0.2422 1.1708 2.8328 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 2.92110 0.29449 9.919 \u0026lt;2e-16 *** radio 0.18799 0.00804 23.382 \u0026lt;2e-16 *** tv 0.04575 0.00139 32.909 \u0026lt;2e-16 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 1.681 on 197 degrees of freedom Multiple R-squared: 0.8972,\tAdjusted R-squared: 0.8962 F-statistic: 859.6 on 2 and 197 DF, p-value: \u0026lt; 2.2e-16 1.3 Allocating marketing budgets 1.3.1 Ratio of elasticities method What is elasticity? % change in the response variable for a 1% change in the predictor variable Example: the % change in sales for a 1% change in advertising spending Let us start with an example:\nImage that you have a £100,000 budget to spend on advertising A 1% increase in online advertising increases sales by 0.12% A 1% increase in offline advertising increases sales by 0.08% 如何使用弹性系数法？\n总弹性系数：0.12 + 0.08 = 0.20。 弹性系数比率： 在线广告：0.12/0.20 = 60% 线下广告：0.08/0.20 = 40%\n建议：将60%（£60,000）的预算分配给在线广告，40%（£40,000）的预算分配给线下广告。 1.3.2 How to obtain elasticities from a linear regression model? 广告弹性 = 广告估算值 * (基线广告/基线销售) 基线广告 = 平均广播广告支出（=23.26） Baseline Sales = average sales (=14.02) What is the elasticity of radio advertising? (0.32) A 1% increase in radio advertising results in a 0.32% increase in sales. 1 2 3 4 5 6 7 8 \u0026gt; mean(spending.data$radio) [1] 23.264 \u0026gt; mean(spending.data$sales) [1] 14.0225 \u0026gt; 0.19 * (23.26/14.02) [1] 0.3152211 2 Marketing Mix Modelling 2.1 Modelling non-linear returns on investment 假设广告在初始预算为50万英镑的情况下在1周内播出，旨在提高英国消费者对Airbnb作为一个包容性品牌的认知，同时增加访问Airbnb网站和预订的流量。\n由于该广告活动的结果，Airbnb的预订量增加了1%。\n一年后，Airbnb决定再次进行一场相似的广告活动，同样持续1周，但将预算翻倍至100万英镑，希望预订量会增加2%。 你认为会发生这种情况吗？为什么（为什么不）？\n\u0026ldquo;电视和广告之间的关系是否线性？\u0026rdquo;\n1 plot(spending.data$tv, spending.data$sales) {% asset_image R_week8_code_2.png %}\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 \u0026gt; summary(spending.data$sales) Min. 1st Qu. Median Mean 3rd Qu. Max. 1.60 10.38 12.90 14.02 17.40 27.00 \u0026gt; summary(spending.data$tv) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.70 74.38 149.75 147.04 218.82 296.40 \u0026gt; summary(spending.data$radio) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.000 9.975 22.900 23.264 36.525 49.600 \u0026gt; summary(spending.data$newspaper) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.30 12.75 25.75 30.55 45.10 114.00 \u0026gt; regression \u0026lt;- lm(log(sales) ~ log(radio+0.01) + log(tv) + log(newspaper), data=spending.data) \u0026gt; summary(regression) Call: lm(formula = log(sales) ~ log(radio + 0.01) + log(tv) + log(newspaper), data = spending.data) Residuals: Min 1Q Median 3Q Max -0.45346 -0.08881 -0.01746 0.06781 0.78863 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 0.479773 0.052031 9.221 \u0026lt;2e-16 *** log(radio + 0.01) 0.144177 0.007865 18.333 \u0026lt;2e-16 *** log(tv) 0.349297 0.008857 39.437 \u0026lt;2e-16 *** log(newspaper) 0.017488 0.009306 1.879 0.0617 . --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 0.1254 on 196 degrees of freedom Multiple R-squared: 0.9098,\tAdjusted R-squared: 0.9084 F-statistic: 658.7 on 3 and 196 DF, p-value: \u0026lt; 2.2e-16 我们已经将0.01添加到广播的值上，因为广播的最小值是0，这样做会导致对log（广播）的无效结果。\n哪个模型更好，线性模型还是对数-对数模型？\n绘制变量之间的关系图 比较线性模型和对数-对数模型的拟合情况 如何解释系数？\n广播广告的增加1%导致销售额增加0.14%。 电视广告的增加1%导致销售额增加0.35%。 Coefficients are elasticities!\nSummary\n1 2 3 4 5 6 7 8 9 Linear • Equation: Y = β0 + β1X • Interpretation: One unit change in X leads to beta1 unit change in Y • When to use? Linear relation between X and Y Log-Log • Equation: log(Y) = β0 + β1log(X) • Interpretation: One percent change in X leads to β1 percent change in Y • When to use? A non-linear relation between X and Y Which one to use? • Plot the data to learn about the relation between X and Y. • Estimate both models and identify the best fitting model 2.2 Modelling media synergy 营销组合工具的综合使用可以产生协同效应。 当多种媒体的联合影响超过它们各自部分的总和时，就会产生协同效应。 Is TV advertising more effective in the presence of radio advertising?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \u0026gt; regression \u0026lt;- lm(log(sales) ~ log(radio+0.01) + log(tv) + log(newspaper) + log(radio+0.01)*log(tv), data=spending.data) \u0026gt; summary(regression) Call: lm(formula = log(sales) ~ log(radio + 0.01) + log(tv) + log(newspaper) + log(radio + 0.01) * log(tv), data = spending.data) Residuals: Min 1Q Median 3Q Max -0.29117 -0.06889 -0.02084 0.05787 0.74453 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 1.347498 0.101811 13.235 \u0026lt; 2e-16 *** log(radio + 0.01) -0.152661 0.032200 -4.741 4.10e-06 *** log(tv) 0.153799 0.022032 6.981 4.49e-11 *** log(newspaper) 0.019947 0.007741 2.577 0.0107 * log(radio + 0.01):log(tv) 0.066311 0.007043 9.415 \u0026lt; 2e-16 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 0.1043 on 195 degrees of freedom Multiple R-squared: 0.938,\tAdjusted R-squared: 0.9367 F-statistic: 737 on 4 and 195 DF, p-value: \u0026lt; 2.2e-16 使用中心化变量\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 \u0026gt; center \u0026lt;- function(x) { scale(x, scale = F)} # scale = F, means only center not standardize \u0026gt; spending.data \u0026lt;- spending.data %\u0026gt;% mutate(radio_log_centered = center(log(radio+0.01)), + tv_log_centered .... [TRUNCATED] \u0026gt; regression \u0026lt;- lm(log(sales) ~ radio_log_centered + tv_log_centered + newspaper_log_centered + + radio_log_centered*tv_log_centere .... [TRUNCATED] \u0026gt; summary(regression) Call: lm(formula = log(sales) ~ radio_log_centered + tv_log_centered + newspaper_log_centered + radio_log_centered * tv_log_centered, data = spending.data) Residuals: Min 1Q Median 3Q Max -0.29117 -0.06889 -0.02084 0.05787 0.74453 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 2.561748 0.007376 347.306 \u0026lt;2e-16 *** radio_log_centered 0.157146 0.006681 23.521 \u0026lt;2e-16 *** tv_log_centered 0.337076 0.007476 45.086 \u0026lt;2e-16 *** newspaper_log_centered 0.019947 0.007741 2.577 0.0107 * radio_log_centered:tv_log_centered 0.066311 0.007043 9.415 \u0026lt;2e-16 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 0.1043 on 195 degrees of freedom Multiple R-squared: 0.938,\tAdjusted R-squared: 0.9367 F-statistic: 737 on 4 and 195 DF, p-value: \u0026lt; 2.2e-16 2.3 Modelling carryover effects 到目前为止，我们假设在给定时间段内的广告只会影响该时间段内的销售。实际上，消费者对广告的反应可能会有延迟。不考虑延迟效应可能会导致广告弹性系数被低估。\n广告存量度量了广告在当前时间段之外的影响。\n广告存量背后的理论是，营销曝光在消费者心中建立了意识。这种意识不会在消费者看到广告后立即消失，而是留存在他们的记忆中。记忆会随着时间的推移而减弱，因此广告存量中存在衰减部分。\n1 Adstockt = Advertisingt + λAdstockt−1 在每个时间段中，你被认为会保留前一个广告存量的一部分（λ）。\n例如，如果λ等于0.3，则来自一个时间段之前的广告存量在当前时间段仍然具有30%的影响。\nHow to do this in R?\n“rate = 0.1” sets λ to 0.1 (i.e. 10%) You can empirically test multiple values of λ. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 \u0026gt; adstock \u0026lt;- function(x, rate){ + return(as.numeric(stats::filter(x=x, filter=rate, method=\u0026#34;recursive\u0026#34;))) + } \u0026gt; # filter() function from stats package applies linear filtering to a univariate time series \u0026gt; \u0026gt; spending.data \u0026lt;- spending.data %\u0026gt;% mutate(tv_adstoc .... [TRUNCATED] \u0026gt; regression \u0026lt;- lm(log(sales) ~ log(radio+0.01) + log(tv_adstock) + log(newspaper), data=spending.data) \u0026gt; summary(regression) Call: lm(formula = log(sales) ~ log(radio + 0.01) + log(tv_adstock) + log(newspaper), data = spending.data) Residuals: Min 1Q Median 3Q Max -0.96605 -0.08394 -0.00081 0.07966 0.81860 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) -0.09547 0.08418 -1.134 0.2581 log(radio + 0.01) 0.13177 0.01008 13.074 \u0026lt;2e-16 *** log(tv_adstock) 0.45582 0.01542 29.567 \u0026lt;2e-16 *** log(newspaper) 0.02219 0.01190 1.865 0.0637 . --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 0.1604 on 196 degrees of freedom Multiple R-squared: 0.8523,\tAdjusted R-squared: 0.8501 F-statistic: 377.1 on 3 and 196 DF, p-value: \u0026lt; 2.2e-16 2.4 Predictive accuracy * 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 \u0026gt; # Total number of rows in the data frame \u0026gt; n \u0026lt;- nrow(spending.data) \u0026gt; # Number of rows for the training set (80% of the dataset) \u0026gt; n_train \u0026lt;- round(0.80 * n) # Training data \u0026gt; spending.data.train \u0026lt;- subset(spending.data, week \u0026lt;= n_train) # Holdout data \u0026gt; spending.data.holdout \u0026lt;- subset(spending.data, week \u0026gt; n_train) \u0026gt; # Estimation on training data \u0026gt; regression \u0026lt;- lm(log(sales) ~ log(radio_adstock) + log(tv_adstock) + log(newspaper_adstock), data=spending.data.trai .... [TRUNCATED] \u0026gt; summary(regression) Call: lm(formula = log(sales) ~ log(radio_adstock) + log(tv_adstock) + log(newspaper_adstock), data = spending.data.train) Residuals: Min 1Q Median 3Q Max -0.95544 -0.06206 -0.00140 0.07359 0.60782 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) -0.37264 0.09377 -3.974 0.000108 *** log(radio_adstock) 0.19215 0.01473 13.048 \u0026lt; 2e-16 *** log(tv_adstock) 0.47022 0.01566 30.031 \u0026lt; 2e-16 *** log(newspaper_adstock) 0.02057 0.01545 1.332 0.184852 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 0.1483 on 156 degrees of freedom Multiple R-squared: 0.8812,\tAdjusted R-squared: 0.8789 F-statistic: 385.6 on 3 and 156 DF, p-value: \u0026lt; 2.2e-16 1 2 3 4 5 6 7 8 9 10 11 12 \u0026gt; # Predict sales on holdout data \u0026gt; spending.data.holdout$predicted_sales_log \u0026lt;- + predict(object =regression, newdata = spending.data.holdout) \u0026gt; # Convert predicted log sales to actual sales \u0026gt; spending.data.holdout$predicted_sales \u0026lt;- exp(spending.data.holdout$predicted_sales_log) \u0026gt; # Quantify predictive accuracy: Mean Average Percentage Error (MAPE) \u0026gt; mape \u0026lt;- mean(abs((spending.data.holdout$sales -spending.data.holdout$predicte .... [TRUNCATED] \u0026gt; mape # Reflects the average percentage error in a given week [1] 0.1010304 1 2 3 4 5 6 \u0026gt; # Plot actual versus predicted sales \u0026gt; plot(spending.data.holdout$week, spending.data.holdout$sales, type=\u0026#34;l\u0026#34;, col=\u0026#34;blue\u0026#34;) # Plot actual sales \u0026gt; lines(spending.data.holdout$week,spending.data.holdout$predicted_sales, type = \u0026#34;l\u0026#34;, col = \u0026#34;red\u0026#34;) # Add predicted sales \u0026gt; legend(\u0026#34;topleft\u0026#34;, legend=c(\u0026#34;Actual sales\u0026#34;, \u0026#34;Predicted sales\u0026#34;), col=c(\u0026#34;blue\u0026#34;, \u0026#34;red\u0026#34;), lty = 1:2, cex=0.6) {% asset_image R_week8_code_3.png %}\n","date":"2024-04-02T00:00:00Z","permalink":"https://MyLoveES.github.io/p/rweek8-managing-resource-trade-offs-code/","title":"R[week8] Managing Resource Trade-Offs Code"},{"content":" R: 4.3.2 (2023-10-31) R studio: 2023.12.1+402 (2023.12.1+402)\n0. Learning Objectives 市场营销问题 管理资源权衡 分析工具 市场实验 市场反应模型 0.1 Framework for Managing Resource Trade-Offs {% asset_image R_week8_lecture_1.png %}\n0.2 Attribution Approach 指定归因模型，使公司能够了解资源增加的确切美元影响，还可以回答以下问题：\n营销投资的相对美元价值影响是多少？ 什么是利润最大化的投资水平？ 通过使用归因模型，管理者可以将资源分配到优化他们期望的结果，避免浪费或依赖于任意启发式方法。\n这些归因模型主要分为两大类：\n市场反应模型（营销组合模型） 实验 1. Market Response Model 第一部分：营销效果和资源分配\n为什么/如何衡量营销投资回报？ 分配营销预算 第二部分：营销组合建模\n线性回归建模 营销组合工具之间的协同建模 携带效应建模 评估模型 1.1 Why Measure Returns on Marketing Investment? {% asset_image R_week8_lecture_2.png %}\n1.2 What are market response models? How it Works? 利用过去的数据揭示营销资源和业绩之间的关系，响应模型提供了两个主要的见解。 营销资源和结果之间关系的形状（凹形或凸形）。 如果营销投入增加1%，财务业绩会发生多大变化，也称为营销弹性。 1.3 How to Measure Return on Marketing Investment {% asset_image R_week8_lecture_3.png %}\n1.4 The Basics: Simple linear regression 1.4.1 Example1: Radio Ads for Sales {% asset_image R_week8_lecture_7.png %}\nSales = a+ b1 Radio advertising\n{% asset_image R_week8_lecture_4.png %}\nDoes radio advertising affects sales for an electronics brand?\n{% asset_image R_week8_lecture_5.png %}\nPredict sales\n{% asset_image R_week8_lecture_6.png %}\n1.4.2 Radio and TV ads for sales {% asset_image R_week8_lecture_8.png %}\nSales =a+ b1 Radio advertising + b2 TV advertising\n{% asset_image R_week8_lecture_9.png %}\n{% asset_image R_week8_lecture_10.png %}\n{% asset_image R_week8_lecture_11.png %}\n1.5 Allocating Marketing Budgets 弹性系数是什么？ 预测变量变化1%，响应变量变化的百分比 例如：广告支出变化1%，销售变化的百分比 {% asset_image R_week8_lecture_12.png %}\n1.6 Modelling log-log Returns on Investment {% asset_image R_week8_lecture_13.png %}\n{% asset_image R_week8_lecture_14.png %}\n{% asset_image R_week8_lecture_15.png %}\n1.7 Modelling Media Synergy 组合使用营销组合工具可以产生协同效应。 当多种媒体的联合影响超过它们各自部分的总和时，就会产生协同效应。 {% asset_image R_week8_lecture_16.png %}\n1.8 Modelling carryover effects 到目前为止，我们假设在给定时间段内的广告只会影响该时间段内的销售。\n实际上，消费者对广告的反应可能会有延迟。\n不考虑传递效应可能会导致低估广告弹性。\n广告储存指标衡量了当前时间段之后的广告效应\n广告储存t = 当期广告t + λ * 广告储存t-1\n在每个时间段中，你假设保留你之前广告储存的一部分（λ）\n例如，如果λ等于0.3，则来自一个时间段之前的广告储存在当前时间段仍然有30%的影响\nHow to calculate Adstock levels?\nAdstockt= Advertisingt+ λAdstockt-1\n{% asset_image R_week8_lecture_17.png %}\nDo it in R\n{% asset_image R_week8_lecture_18.png %}\n1.9 Building and Evaluating Market Response Model {% asset_image R_week8_lecture_19.png %}\n1.10 Takeaways 所有资源都是有限的。管理者必须在资源之间进行权衡，以制定有效的营销策略。 管理资源权衡的方法已经从纯启发式的时代发展到数据化时代，管理者依靠统计模型和详细信息。 基于响应模型的归因方法捕捉了过去营销资源和过去结果之间的关系。然后利用过去的数据可以揭示营销资源和绩效之间的关系。 2. Marketing Experiment Marketing experiment AB testing Multivariate testing Natural experiment Research examples 营销实验测试客户对营销决策的反应，同时排除其他可能存在的混杂因素，这些因素在比较处理组和对照组时可能存在。\n何时使用？ 确定特定营销投资与客户或公司结果之间是否存在直接关系。 根据它们的财务影响（例如，销售额增长）在一组投资策略和策略中进行选择。 2.1 Marketing Experiment: Components 实验归因方法涉及干预、结果、处理条件的设计以及对照条件。 决定测试哪些因素至关重要，实验很快就会变得非常复杂。 Component Definition Intervention A key marketing action whose effectiveness the firm seeks to document Outcome The key marketing gain for the firm implementing the experiment Treatment When, where, and to whom the firm administers the intervention Control A region, customer, or situation similar to the experimental intervention that remains unchanged during the experimental process 2.2 Marketing Experiment: How it works 实验旨在建立自变量（营销投资）和结果之间的关系。\n必须建立一个良好的处理组和比较（对照）组。处理组是接受该处理的受试者群体（例如，消费者、销售人员）。对照组保持因果因素不变（例如，向另一组销售人员支付的佣金保持不变）。 处理组和对照组在所有其他方面必须相似（例如，样本大小、人口构成、销售动机、经验）。在试验条件和对照条件之间，所有其他因素（至少是公司可控制的因素）都被有意地保持不变，这些因素可能会影响结果（例如，销售）。 为了达到这个标准，通常使用随机分配。通过随机分配，从概率意义上来说，受试者接受处理的机会在不同组之间是相等的。 1 2 3 4 5 6 7 𝑌 𝑖 = 𝛽0 + 𝛽1𝑇𝑖 + 𝜀𝑖 - 𝑌 𝑖 是感兴趣的因变量； - 𝑇𝑖 如果主体i被分配到处理组，则编码为1，否则为0， - 系数 𝛽1 是处理效应。 - 𝜀𝑖 捕捉随机统计误差。 - 𝛽0 是截距。 在进行实验后，如果 𝛽1 在统计上显著，则处理效应是合法的。\n根据分析的目标，实验可以采用两种不同的设计：\n“后验”设计：在客户接受营销活动后，测量营销行为对客户行为的影响。 “前后对照”设计：在客户接受营销活动之前和之后，测量营销行为的影响。 2.3 Marketing Experiment: Example 鲜花送货公司DFG正在进行季度营销预算会议。一位经理指出公司每年在广告上花费了25万美元，质疑是否合理，或者DFG是否花费过多。在随后的内部讨论中，一些经理坚持认为地方电视广告对于建立品牌资产和产生收入至关重要；而另一些则认为公司花费过多。\n2.3.1 Marketing Experiment: “after-only” DFG决定进行一项受控的营销实验。将处理组和对照组匹配在已知属性上（例如人口统计学特征）。\n{% asset_image R_week9_lecture_1.png %}\n2.3.2 Marketing Experiment: “before-and-after” DFG决定采用控制的营销实验，采用前后对照的设计。\n{% asset_image R_week9_lecture_2.png %}\n2.3.3 Case 随机分配顾客 分析 比较各组以确定均值差异是否显著 T检验/方差分析/回归 {% asset_image R_week9_lecture_3.png %}\n2.3.4 68-95-99.7 rule {% asset_image R_week9_lecture_4.png %}\n2.3.5 GOGOGO 公司估计了三个回归方程，从三个不同的模型中获得了系数 𝛽1，分别捕捉了由于地方电视广告增加而导致的品牌知名度、品牌回忆率和销售的统计变化。 在每个回归中，处理效应都是显著的： 当DFG增加其地方电视广告时，处理区域的品牌知名度、品牌回忆率和销售分别增长了1.5％、3.2％和3％（与对照组相比）。 DFG每年销售额为2500万美元，因此该实验使公司的决策者对地方电视广告带来的销售增长更有信心。因此，一项实验帮助解决了DFG内部的冲突。 2.4 A/B testing 使用实验设计比较一个设计的两个或更多变体\n新想法 运行实验 分析结果 选择获胜的想法 强大且广泛使用的指标\n转化率（例如点击率） 参与率（例如点赞、分享） 停留时间（例如在首页停留的秒数） 2.5 Multivariate testing {% asset_image R_week9_lecture_5.png %}\n2.6 Natural Experiments: when to use 我们并不总是能够进行随机化 政策上的意外变化可以被视为“自然实验”。例如，对一些市场引入了新的退货政策。 即使在自然实验中，我们也需要确定哪个是处理组，哪个是对照组 处理组：受政策变化影响的组；例如，退货政策发生变化的市场。 对照组：不受政策变化影响的组；例如，退货政策保持不变的市场。 2.6.1 Natural Experiments: Difference-in-Difference {% asset_image R_week9_lecture_6.png %}\n差异中的差异（DiD）试图通过研究自然实验中处理组与对照组之间的处理对结果变量的差异效应，来模拟实验研究设计。 DiD利用面板数据来测量随时间发生的处理组和对照组之间结果变量的变化差异。 1 2 3 4 5 6 7 8 9 10 11 𝑌 𝑖 = 𝛽0 + 𝛽1t𝑖 + 𝛽2T𝑖 + 𝛽3 𝑇𝑖 × 𝑡𝑖 + 𝜀𝑖 - 𝑌 𝑖是依赖变量; - 𝑡𝑖如果是介入后，则编码为1，如果是介入前，则编码为0； - 𝛽1是控制组和处理组共同的时间趋势; - 𝑇𝑖如果主体i分配到处理组，则编码为1，否则为0， - 𝛽2是处理效应。 - 交互项𝑇𝑖 × 𝑡𝑖是DiD项 - 𝛽3是真实效应。 - 𝜀𝑖捕获了随机统计误差 - 𝛽0是截距 3 Takeaways 实验评估因果关系 通常使用T检验、方差分析或线性回归来比较组，以测试差异是否显著 网络实验（A/B测试）成本较低且速度较快 ‒ 实验成本可以是可变的，而不是固定的 差异中的差异用于检验准实验，如自然实验。它测量了处理组和对照组在一段时间内结果变量的变化差异 ","date":"2024-04-01T00:00:00Z","permalink":"https://MyLoveES.github.io/p/rweek8-managing-resource-trade-offs/","title":"R[week8] Managing Resource Trade-Offs"},{"content":" R: 4.3.2 (2023-10-31) R studio: 2023.12.1+402 (2023.12.1+402)\n市场营销人员通常希望从交易中提取见解。关联规则分析试图从庞大、稀疏的数据集中找到一组有信息量的模式。我们使用一个包含超过 80,000 笔市场购物篮交易和 16,000 个独特项目的真实数据集来展示关联规则。然后，我们将探讨规则挖掘在交易数据中的潜在用途，并使用关联规则来探索这些交易数据中的模式。\n1 The Basics of Association Rules 关联规则挖掘的基本思想是，当事件一起发生的频率超过了它们各自发生率的预期时，这种共同发生就是一个有趣的模式。\n理解关联规则需要一些术语。\n• 关联，简单来说就是两个或更多事物的共同发生。例如，热狗可能与腌菜、热狗面包、苏打水、薯片和番茄酱呈正相关。但是关联不一定强烈。在像 Costco 这样销售从热狗到电视等各种商品的商店中，所有商品都与其他商品相关联，但其中大多数关联并不强烈。一组项目是一个包含一个或多个项目的组合，可以写成 {项目1，项目2，\u0026hellip;}。例如，一个集合可能是 {腌菜} 或 {热狗，苏打水，薯片}。\n• 一次交易包含在一个观察中共同出现的一组项目。在市场营销中，一个交易单位是市场购物篮，即一次购买或考虑购买的一组物品。\n任何共同出现的数据点都被视为一次交易，即使在此情境中使用术语“交易”似乎有些不寻常。例如，用户在会话期间访问的网页集合在这个意义上是一次交易。\n• 规则表示了一组项目在另一组项目的条件下在交易中的发生情况。在规则 {腌菜} ⇒ {热狗} 中，表示如果顾客购买腌菜，他们也很可能购买热狗。\n规则可以表达多个项目之间的关系；例如，{腌菜，番茄酱，芥末，薯片} ⇒ {热狗，汉堡肉饼，热狗面包，苏打水，啤酒}。在这种意义上，条件并不意味着因果关系，只是一种关联，无论是强还是弱。\n关联规则通常使用三种常见的度量来表示，这些度量反映了条件概率的规则。\n• 一组项目的支持度是包含该组项目的所有交易的比例。如果 {热狗，苏打水} 出现在 200 笔交易中的 10 笔中，那么支持度({热狗，苏打水}) = 0.05。这并不影响这 10 笔交易是否还包含其他项目；支持度是针对每个唯一的项目组合单独定义的。\n• 置信度是规则中所有项目共同出现的支持度，条件是左侧集合单独的支持度。因此，置信度(X ⇒ Y) = 支持度(X ∩ Y)/支持度(X)（其中“∩”表示“和”）。考虑规则 {腌菜} ⇒ {热狗}。如果{腌菜}出现在 1% 的交易中（即支持度({腌菜}) = 0.01），并且{腌菜，热狗}出现在 0.5% 的交易中，那么置信度({腌菜} ⇒ {热狗}) = 0.005/0.01 = 0.5。换句话说，热狗在腌菜出现的情况下有50%的可能性也会出现。\n• 提升度衡量了一个集合的支持度在每个元素的联合支持度的条件下的情况，即提升度(X ⇒ Y) = 支持度(X ∩ Y)/(支持度(X) * 支持度(Y))。继续热狗的例子，如果支持度({腌菜}) = 0.01，支持度({热狗}) = 0.01，支持度({腌菜，热狗}) = 0.005，那么提升度({腌菜 ⇒ 热狗}) = 0.005/(0.01 * 0.01) = 50。换句话说，组合 {腌菜，热狗} 出现的频率是两个项目独立出现时的 50 倍。\n这三种度量告诉我们不同的信息。当我们搜索规则时，我们希望在每个度量上都超过一个最小阈值：找到在交易中相对频繁发生的项目集（支持），显示强有力的条件关系（置信度），并且比随机发生更常见（提升度）。\n在实践中，分析人员会将所需支持度的水平设置为诸如 0.01、0.10、0.20 等具有意义和对业务有用的值，考虑到数据特征（如项目集的大小）。同样，所需的置信度水平可能是高的（如 0.8）或低的（如 0.2），取决于数据和业务。对于提升度，较高的值通常更好，肯定应该高于 1.0，尽管我们必须注意具有巨大提升度的异常值。\n2 Retail Transaction Data: Groceries 我们要检查的数据集包含超市交易数据。我们首先检查的是 arules 软件包中包含的一个小型数据集。尽管规模较小，但这个数据集很有用，因为项目标记有类别名称，使其更易于阅读。然后，我们转向一个来自超市连锁店的较大数据集，该数据集的数据被伪装，但更典型于大型数据集。\n2.1 Data: Groceries 我们将使用 arules 软件包中的 Groceries 数据集来说明关联规则的一般概念。这个数据集包含一起购买的商品列表（即市场购物篮），其中的单个项目被记录为类别标签，而不是产品名称。在继续之前，您应该安装 arules 和 arulesViz 软件包。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 \u0026gt; data(\u0026#34;Groceries\u0026#34;) \u0026gt; summary(Groceries) transactions as itemMatrix in sparse format with 9835 rows (elements/itemsets/transactions) and 169 columns (items) and a density of 0.02609146 most frequent items: whole milk other vegetables rolls/buns soda yogurt (Other) 2513 1903 1809 1715 1372 34055 element (itemset/transaction) length distribution: sizes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 26 27 28 29 32 2159 1643 1299 1005 855 645 545 438 350 246 182 117 78 77 55 46 29 14 14 9 11 4 6 1 1 1 1 3 1 Min. 1st Qu. Median Mean 3rd Qu. Max. 1.000 2.000 3.000 4.409 6.000 32.000 includes extended item information - examples: labels level2 level1 1 frankfurter sausage meat and sausage 2 sausage sausage meat and sausage 3 liver loaf sausage meat and sausage \u0026gt; inspect(head(Groceries,3)) items [1] {citrus fruit, semi-finished bread, margarine, ready soups} [2] {tropical fruit, yogurt, coffee} [3] {whole milk} 2.2 Finding rules 现在我们使用 apriori(data, parameters = \u0026hellip;) 函数来使用 Apriori 算法查找关联规则。在概念层面上，Apriori 算法搜索在一系列交易中频繁出现的项目集。对于每个项目集，它评估在特定支持度水平或以上表达项目之间关联的各种可能规则，然后保留显示置信度超过某个阈值的规则。\n为了控制 apriori() 的搜索范围，我们使用参数 list() 来指示算法搜索具有最小支持度 0.01（1% 的交易）的规则，并提取那些进一步表现出最小置信度 0.3 的规则。生成的规则集被赋值给 groc.rules 对象：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \u0026gt; groc.rules \u0026lt;- apriori(Groceries, parameter = list(supp=0.01, conf=0.3, target=\u0026#34;rules\u0026#34;)) Apriori Parameter specification: confidence minval smax arem aval originalSupport maxtime support minlen maxlen target ext 0.3 0.1 1 none FALSE TRUE 5 0.01 1 10 rules TRUE Algorithmic control: filter tree heap memopt load sort verbose 0.1 TRUE TRUE FALSE TRUE 2 TRUE Absolute minimum support count: 98 set item appearances ...[0 item(s)] done [0.00s]. set transactions ...[169 item(s), 9835 transaction(s)] done [0.00s]. sorting and recoding items ... [88 item(s)] done [0.00s]. creating transaction tree ... done [0.00s]. checking subsets of size 1 2 3 4 done [0.00s]. writing ... [125 rule(s)] done [0.00s]. creating S4 object ... done [0.00s]. 2.3 Inspecting rules 要解释上面 apriori() 的结果，有两个关键的事情需要检查。\n• 首先，检查进入规则的项目数量，这在输出行“sorting and recoding items \u0026hellip;”中显示，在这种情况下告诉我们找到的规则使用了总共 88 个项目中的数量。如果这个数字太小（只有你的项目的一个小集合）或太大（几乎全部），那么你可能希望调整支持度和置信度水平。 • 接下来，检查找到的规则数量，如“writing \u0026hellip;”行所示。在这种情况下，算法找到了 125 条规则。如果这个数字太低，那么说明需要降低支持度或置信度水平；如果太高（例如比项目数多很多的规则），你可能需要增加支持度或置信度水平。\n一旦我们从 apriori() 获得了一组规则集，我们使用 inspect(rules) 来检查关联规则。上面的完整列表有 125 条规则，太长了，这里无法全部检查，因此我们选择具有高提升度（lift \u0026gt; 3）的部分规则子集。我们发现我们的规则集中有五条规则的提升度大于 3.0：\n1 2 3 4 5 6 7 \u0026gt; inspect(subset(groc.rules, lift \u0026gt; 3)) lhs rhs support confidence coverage lift count [1] {beef} =\u0026gt; {root vegetables} 0.01738688 0.3313953 0.05246568 3.040367 171 [2] {citrus fruit, root vegetables} =\u0026gt; {other vegetables} 0.01037112 0.5862069 0.01769192 3.029608 102 [3] {citrus fruit, other vegetables} =\u0026gt; {root vegetables} 0.01037112 0.3591549 0.02887646 3.295045 102 [4] {tropical fruit, root vegetables} =\u0026gt; {other vegetables} 0.01230300 0.5845411 0.02104728 3.020999 121 [5] {tropical fruit, other vegetables} =\u0026gt; {root vegetables} 0.01230300 0.3427762 0.03589222 3.144780 121 第一条规则告诉我们，如果一次交易包含 {牛肉}，那么它也更有可能包含 {根菜类蔬菜}——我们假设这个类别包括土豆和洋葱等项目。支持度显示该组合出现在 1.7% 的购物篮中，提升度显示该组合共同出现的可能性是单独出现的发生率的 3 倍。\n根据这样的信息，商店可能会得出几个见解。例如，商店可以在牛肉柜台附近设置一个土豆和洋葱的陈列，以鼓励正在查看牛肉的购物者购买这些蔬菜，或者考虑使用它们的食谱。它还可以建议将牛肉的优惠券放在根菜类蔬菜区，或者在商店的某个地方展示食谱卡。\n2.4 Plot rules 为了了解规则的分布情况，我们加载 arulesViz 软件包，然后使用 plot() 函数绘制规则集，根据置信度（Y 轴）和支持度（X 轴）绘制规则，并根据提升度调整点的深浅以表示规则的重要性。\n1 \u0026gt; plot(groc.rules) {% asset_image R_week7_code_plot1.png %}\n在该图表中，我们可以看到大多数规则涉及很少发生的项目组合（即支持度低），而置信度相对平稳分布。\n简单地显示点并不是非常有用，arulesViz 的一个关键特性是交互式绘图。在上述图中，有一些规则位于左上角，具有很高的提升度。我们可以使用交互式绘图来检查这些规则。要做到这一点，将 interactive=TRUE 添加到 plot() 命令中：\n1 \u0026gt; plot(groc.rules, engine = \u0026#34;plotly\u0026#34;) {% asset_image R_week7_code_plot2.png %}\n在交互模式下，您可以检查规则的区域。为此，请在感兴趣的区域的一个角落单击一次，然后在相反的角落再次单击。您可以使用放大功能放大该区域或使用检查功能列出该区域的规则。完成后，请单击结束。\n我们选择了左上角的区域，并放大了该区域。然后，我们从放大的区域中选择了几条规则，并单击检查以将它们显示在控制台中。在该子区域中有七条规则。这显示了两条高提升度的规则。\n其中一条规则告诉我们，组合 {柑橘类水果，根菜类蔬菜} 在大约 1.0% 的购物篮中出现（支持度=0.0104），当它出现时，高度可能包含 {其他蔬菜}（置信度=0.586）。该组合的出现频率比我们从单独考虑 {柑橘类水果，根菜类蔬菜} 和 {其他蔬菜} 的个体发生率预期的要多 3 倍（提升度=3.03）。\n这样的信息可以用于多种方式。如果我们将交易与客户信息配对，我们可以用于定向邮寄或电子邮件建议。对于经常一起销售的商品，我们可以一起调整价格和利润率；例如，将一个商品打折销售，同时提高另一个商品的价格。或者收银员可能会问客户：“您是否需要配一些其他蔬菜？”\n2.5 Finding and Plotting Subsets of Rules 在市场购物篮分析中，一个常见的目标是找到提升度高的规则。我们可以通过对提升度排序来轻松找到这样的规则。我们使用 sort() 函数对规则按提升度排序，并从头部取出前 15 条规则，以提取提升度最高的 15 条规则：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \u0026gt; groc.hi \u0026lt;- head(sort(groc.rules, by=\u0026#34;lift\u0026#34;), 15) \u0026gt; inspect(groc.hi) lhs rhs support confidence coverage lift count [1] {citrus fruit, other vegetables} =\u0026gt; {root vegetables} 0.01037112 0.3591549 0.02887646 3.295045 102 [2] {tropical fruit, other vegetables} =\u0026gt; {root vegetables} 0.01230300 0.3427762 0.03589222 3.144780 121 [3] {beef} =\u0026gt; {root vegetables} 0.01738688 0.3313953 0.05246568 3.040367 171 [4] {citrus fruit, root vegetables} =\u0026gt; {other vegetables} 0.01037112 0.5862069 0.01769192 3.029608 102 [5] {tropical fruit, root vegetables} =\u0026gt; {other vegetables} 0.01230300 0.5845411 0.02104728 3.020999 121 [6] {other vegetables, whole milk} =\u0026gt; {root vegetables} 0.02318251 0.3097826 0.07483477 2.842082 228 [7] {whole milk, curd} =\u0026gt; {yogurt} 0.01006609 0.3852140 0.02613116 2.761356 99 [8] {root vegetables, rolls/buns} =\u0026gt; {other vegetables} 0.01220132 0.5020921 0.02430097 2.594890 120 [9] {root vegetables, yogurt} =\u0026gt; {other vegetables} 0.01291307 0.5000000 0.02582613 2.584078 127 [10] {tropical fruit, whole milk} =\u0026gt; {yogurt} 0.01514997 0.3581731 0.04229792 2.567516 149 [11] {yogurt, whipped/sour cream} =\u0026gt; {other vegetables} 0.01016777 0.4901961 0.02074225 2.533410 100 [12] {other vegetables, whipped/sour cream} =\u0026gt; {yogurt} 0.01016777 0.3521127 0.02887646 2.524073 100 [13] {tropical fruit, other vegetables} =\u0026gt; {yogurt} 0.01230300 0.3427762 0.03589222 2.457146 121 [14] {root vegetables, whole milk} =\u0026gt; {other vegetables} 0.02318251 0.4740125 0.04890696 2.449770 228 [15] {whole milk, whipped/sour cream} =\u0026gt; {yogurt} 0.01087951 0.3375394 0.03223183 2.419607 107 支持度和提升度对于一个项目集来说是相同的，无论在规则的左侧还是右侧（右手或左手）内的项目的顺序如何。然而，置信度反映了方向，因为它计算了右手集合在左手集合条件下的出现情况。\n规则的图形显示可能对寻找更高层次的主题和模式很有用。我们使用 plot(\u0026hellip;, method=\u0026ldquo;graph\u0026rdquo;) 来绘制按提升度排名的前 15 条规则的图形显示：\n{% asset_image R_week7_code_plot3.png %}\n结果图中项目的位置可能因您的系统而异，但项目集群应该是相似的。每个圆圈代表一条规则，其中来自规则左侧项的入站箭头，指向右侧项的出站箭头。圆圈的大小（面积）表示规则的支持度，而颜色的深浅表示提升度（颜色越深表示提升度越高）。\n3 Supermarket Data 现在我们将调查来自比利时一家超市连锁店的更大规模的零售交易数据中的关联情况。这个数据集包括一起购买的商品的购物篮，其中每条记录都包括任意编号的商品编号，没有商品描述（为了保护该连锁店的专有数据）。这个数据集是由Brijs等人公开提供的。\n3.1 Data Preparation First we use readLines() to get the data from where it is hosted:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 \u0026gt; retail.raw \u0026lt;- readLines(\u0026#34;retail.dat\u0026#34;) \u0026gt; head(retail.raw) [1] \u0026#34;0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \u0026#34; [2] \u0026#34;30 31 32 \u0026#34; [3] \u0026#34;33 34 35 \u0026#34; [4] \u0026#34;36 37 38 39 40 41 42 43 44 45 46 \u0026#34; [5] \u0026#34;38 39 47 48 \u0026#34; [6] \u0026#34;38 39 48 49 50 51 52 53 54 55 56 57 58 \u0026#34; \u0026gt; tail(retail.raw) [1] \u0026#34;48 201 255 278 407 479 767 824 986 1395 1598 2022 2283 2375 6725 13334 14006 14099 \u0026#34; [2] \u0026#34;39 875 2665 2962 12959 14070 14406 15518 16379 \u0026#34; [3] \u0026#34;39 41 101 346 393 413 479 522 586 635 695 799 1466 1786 1994 2449 2830 3035 3591 3722 6217 11493 12129 13033 \u0026#34; [4] \u0026#34;2310 4267 \u0026#34; [5] \u0026#34;39 48 2528 \u0026#34; [6] \u0026#34;32 39 205 242 1393 \u0026#34; \u0026gt; summary(retail.raw) Length Class Mode 88162 character character 该对象中的每一行表示一篮子购物中一起购买的商品。在每一行内，商品被分配了任意的数字，这些数字从0开始，随着后续交易的需要逐渐增加新的商品编号。\n数据包括 88,162 个交易，第一个购物篮有 30 个商品（编号为 0-29），第二个有 3 个商品，依此类推。在 tail() 中，我们看到最后一个购物篮有 5 个商品，其中大部分商品，如 32、39、205 和 242 号，编号较低，反映了这些特定商品最初出现在数据集的早期交易中。\n在这种文本格式中，数据还不能直接使用；我们必须首先将每个交易文本行拆分成单独的商品。为此，我们使用 strsplit(lines, \u0026quot; \u0026ldquo;)。这个命令在每个空格字符（\u0026rdquo; \u0026ldquo;）处将每一行拆分，并将结果保存到一个列表中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026gt; retail.list \u0026lt;- strsplit(retail.raw, \u0026#34; \u0026#34;) \u0026gt; names(retail.list) \u0026lt;- paste(\u0026#34;Trans\u0026#34;, 1:length(retail.list)) \u0026gt; str(retail.list) List of 88162 $ Trans 1 : chr [1:30] \u0026#34;0\u0026#34; \u0026#34;1\u0026#34; \u0026#34;2\u0026#34; \u0026#34;3\u0026#34; ... $ Trans 2 : chr [1:3] \u0026#34;30\u0026#34; \u0026#34;31\u0026#34; \u0026#34;32\u0026#34; $ Trans 3 : chr [1:3] \u0026#34;33\u0026#34; \u0026#34;34\u0026#34; \u0026#34;35\u0026#34; $ Trans 4 : chr [1:11] \u0026#34;36\u0026#34; \u0026#34;37\u0026#34; \u0026#34;38\u0026#34; \u0026#34;39\u0026#34; ... $ Trans 5 : chr [1:4] \u0026#34;38\u0026#34; \u0026#34;39\u0026#34; \u0026#34;47\u0026#34; \u0026#34;48\u0026#34; $ Trans 6 : chr [1:13] \u0026#34;38\u0026#34; \u0026#34;39\u0026#34; \u0026#34;48\u0026#34; \u0026#34;49\u0026#34; ... $ Trans 7 : chr [1:6] \u0026#34;32\u0026#34; \u0026#34;41\u0026#34; \u0026#34;59\u0026#34; \u0026#34;60\u0026#34; ... 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 \u0026gt; some(retail.list) #note: random sample; your results may vary $`Trans 1458` [1] \u0026#34;32\u0026#34; \u0026#34;38\u0026#34; \u0026#34;39\u0026#34; \u0026#34;48\u0026#34; \u0026#34;370\u0026#34; \u0026#34;371\u0026#34; \u0026#34;373\u0026#34; \u0026#34;931\u0026#34; $`Trans 5763` [1] \u0026#34;41\u0026#34; \u0026#34;79\u0026#34; \u0026#34;186\u0026#34; \u0026#34;251\u0026#34; \u0026#34;374\u0026#34; \u0026#34;389\u0026#34; \u0026#34;751\u0026#34; \u0026#34;3532\u0026#34; \u0026#34;4993\u0026#34; \u0026#34;5344\u0026#34; $`Trans 13502` [1] \u0026#34;39\u0026#34; \u0026#34;41\u0026#34; \u0026#34;279\u0026#34; \u0026#34;475\u0026#34; \u0026#34;1516\u0026#34; \u0026#34;1739\u0026#34; \u0026#34;1960\u0026#34; \u0026#34;3281\u0026#34; \u0026#34;5354\u0026#34; \u0026#34;6128\u0026#34; $`Trans 27697` [1] \u0026#34;7797\u0026#34; \u0026#34;11864\u0026#34; $`Trans 27864` [1] \u0026#34;39\u0026#34; \u0026#34;956\u0026#34; \u0026#34;1783\u0026#34; \u0026#34;1907\u0026#34; \u0026#34;3185\u0026#34; \u0026#34;4208\u0026#34; $`Trans 29944` [1] \u0026#34;32\u0026#34; \u0026#34;39\u0026#34; \u0026#34;48\u0026#34; \u0026#34;12136\u0026#34; $`Trans 50578` [1] \u0026#34;597\u0026#34; \u0026#34;727\u0026#34; \u0026#34;1907\u0026#34; \u0026#34;14031\u0026#34; $`Trans 62789` [1] \u0026#34;1163\u0026#34; \u0026#34;3497\u0026#34; \u0026#34;3941\u0026#34; \u0026#34;5685\u0026#34; \u0026#34;12968\u0026#34; \u0026#34;15148\u0026#34; $`Trans 67790` [1] \u0026#34;4165\u0026#34; \u0026#34;4398\u0026#34; \u0026#34;14098\u0026#34; $`Trans 77780` [1] \u0026#34;18\u0026#34; \u0026#34;32\u0026#34; \u0026#34;39\u0026#34; \u0026#34;41\u0026#34; \u0026#34;262\u0026#34; \u0026#34;418\u0026#34; \u0026#34;438\u0026#34; \u0026#34;864\u0026#34; \u0026#34;913\u0026#34; \u0026#34;1342\u0026#34; \u0026#34;1696\u0026#34; \u0026#34;1872\u0026#34; \u0026#34;2077\u0026#34; \u0026#34;2856\u0026#34; \u0026#34;5740\u0026#34; \u0026#34;10423\u0026#34; [17] \u0026#34;10939\u0026#34; \u0026#34;15832\u0026#34; 使用 str()，我们确认列表有 88,162 个条目，并且单个条目看起来合适。使用 some() 从整个较大的集合中对一些交易进行抽样，以进行额外确认。\n然后，我们将其转换为 transactions 对象，这样可以增强我们处理数据的方式并加速 arules 操作。要从列表转换为 transactions，我们使用 as(. . . , \u0026ldquo;transactions\u0026rdquo;) 来转换对象：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 \u0026gt; retail.trans \u0026lt;- as(retail.list, \u0026#34;transactions\u0026#34;) #takes a few seconds \u0026gt; summary(retail.trans) transactions as itemMatrix in sparse format with 88162 rows (elements/itemsets/transactions) and 16470 columns (items) and a density of 0.0006257289 most frequent items: 39 48 38 32 41 (Other) 50675 42135 15596 15167 14945 770058 element (itemset/transaction) length distribution: sizes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 3016 5516 6919 7210 6814 6163 5746 5143 4660 4086 3751 3285 2866 2620 2310 2115 1874 1645 1469 1290 1205 981 887 819 684 586 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 582 472 480 355 310 303 272 234 194 136 153 123 115 112 76 66 71 60 50 44 37 37 33 22 24 21 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 71 73 74 76 21 10 11 10 9 11 4 9 7 4 5 2 2 5 3 3 1 1 1 1 Min. 1st Qu. Median Mean 3rd Qu. Max. 1.00 4.00 8.00 10.31 14.00 76.00 includes extended item information - examples: labels 1 0 2 1 3 10 includes extended transaction information - examples: transactionID 1 Trans 1 2 Trans 2 3 Trans 3 查看结果对象的 summary()，我们可以看到交易-商品矩阵是 88,162 行乘以 16,470 列。在这 14 亿个交叉点中，只有 0.06% 有正数据（密度），因为大多数商品在大多数交易中都没有被购买。商品 39 出现最频繁，在 50,675 个篮子中出现，超过了所有交易的一半。3,016 个交易只包含一个商品（“sizes” = 1），中位篮子大小为 8 个商品。\n3.2 Exercise: Please now use the prepared supermarket data to do the market basket analysis, finding and visualizing the association rules.\n4 Key Point 关联规则是探索数据集中关系的强大方法。\n• 关联规则通常与稀疏数据集一起使用，这些数据集具有许多观察结果，但每个观察结果的信息很少。在市场营销中，这是市场篮子和类似的交易数据的典型特征。\n• arules 软件包是 R 中用于关联规则的标准软件包。arules 提供了处理稀疏数据和查找规则的支持，而 arulesViz 软件包则提供了可视化方法。\n• 评估关联规则的核心指标是支持度（频率）、置信度（共同发生）和提升度（纯粹偶然情况下的共同发生）。除了提升度应该略高于 1.0 外，它们没有绝对值的要求。解释取决于对类似数据的经验以及特定业务问题的实用性。\n• 关联规则的典型工作流程包括：\n导入原始数据，并使用 as(data, \u0026ldquo;transactions\u0026rdquo;) 将其转换为 transactions 对象，以获得更好的性能。 使用 apriori(transactions, support= , confidence= , target=\u0026ldquo;rules\u0026rdquo;) 查找一组关联规则。 使用 plot(. . . , interactive=TRUE) 绘制结果规则并检查规则。 通过选择规则的子集来寻找模式，例如具有最高提升度的规则，并使用 plot(. . . , method=\u0026ldquo;graph\u0026rdquo;) 进行可视化。 5 CODE 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 library(\u0026#34;arules\u0026#34;) # processing of regression output library(\u0026#34;arulesViz\u0026#34;) # used for report compilation and table display library(\u0026#34;grid\u0026#34;) # very popular plotting library ggplot2 library(\u0026#34;car\u0026#34;) # multinomial logit library(\u0026#34;plotly\u0026#34;) # ConfusionMatrix # 获取当前已加载文件的目录 file_dir \u0026lt;- dirname(parent.frame(2)$ofile) print(file_dir) # 将工作目录设置为当前已加载文件的目录 setwd(file_dir) data(\u0026#34;Groceries\u0026#34;) summary(Groceries) inspect(head(Groceries,3)) groc.rules \u0026lt;- apriori(Groceries, parameter = list(supp=0.01, conf=0.3, target=\u0026#34;rules\u0026#34;)) inspect(subset(groc.rules, lift \u0026gt; 3)) plot(groc.rules) plot(groc.rules, engine = \u0026#34;plotly\u0026#34;) groc.hi \u0026lt;- head(sort(groc.rules, by=\u0026#34;lift\u0026#34;), 15) inspect(groc.hi) plot(groc.hi, method=\u0026#34;graph\u0026#34;) retail.raw \u0026lt;- readLines(\u0026#34;retail.dat\u0026#34;) head(retail.raw) tail(retail.raw) summary(retail.raw) retail.list \u0026lt;- strsplit(retail.raw, \u0026#34; \u0026#34;) names(retail.list) \u0026lt;- paste(\u0026#34;Trans\u0026#34;, 1:length(retail.list)) str(retail.list) some(retail.list) #note: random sample; your results may vary rm(retail.raw) retail.trans \u0026lt;- as(retail.list, \u0026#34;transactions\u0026#34;) #takes a few seconds summary(retail.trans) rm(retail.list) #remove retail.list as it is not needed anymore. ","date":"2024-03-31T00:00:00Z","permalink":"https://MyLoveES.github.io/p/rweek7-managing-sustainable-competitive-advantage-code-ii/","title":"R[week7] Managing Sustainable Competitive Advantage code II"},{"content":" R: 4.3.2 (2023-10-31) R studio: 2023.12.1+402 (2023.12.1+402)\n一、Market Basket Analysis 市场购物篮分析\n1 2 3 4 5 在市场营销领域常用的数据分析技术，也被称为关联规则分析或者关联分析。它的主要目标是发现产品或服务之间的相关性，即顾客在购买某一项产品或服务的同时，往往也会购买另一项产品或服务。通过分析这些购买行为，市场营销人员可以更好地了解顾客的购买偏好和行为模式，从而制定更加精准的营销策略。 市场购物篮分析的基本原理是利用关联规则来描述不同产品或服务之间的关系。关联规则通常采用“如果...那么...”的形式，其中一个项目集合（或称为项集）的出现被认为是另一个项目集合的充分条件。例如，“如果顾客购买了洗发水，那么他们也有可能购买护发素”。 在R语言中，你可以使用一些包如\u0026#34;arules\u0026#34;来进行市场购物篮分析。这些包提供了一系列函数来发现频繁项集（即经常同时出现的产品组合）以及关联规则。通过分析这些规则，你可以洞察顾客的购买行为模式，并据此进行更有针对性的营销活动。 • 市场购物篮分析，又称亲和性分析，根据它们在数据集中的共同出现，揭示不同实体之间的有意义的相关性。 • 它生成满足预定义标准的关联规则，以识别频繁项目集中最重要的关系，并帮助揭示大数据中的隐藏模式。\n1.1 Benefits of Market Basket Analysis • When you understand product relationships and purchase sequences, you can identify and track customers who have bought a given product and deliver tailored messages to them. • With personalization, you’re also able to create more effective marketing campaigns.\n1 2 当您了解产品关系和购买顺序时，您可以识别和跟踪购买特定产品的客户，并向他们发送定制的消息。 通过个性化，您还能够创建更有效的营销活动。 • Market basket analysis might tell a retailer that customers often purchase shampoo and conditioner together, so putting both items on promotion at the same time would not create a significant increase in revenue, while a promotion involving just one of the items would likely drive sales of the other.\n1 市场购物篮分析可能告诉零售商，顾客经常一起购买洗发水和护发素，因此同时促销这两种商品不会显著增加收入，而仅促销其中一种商品可能会推动另一种商品的销售。 • Inventory management\n1 库存管理, 存储适量的相关产品 • Refine marketing\n1 优化营销，根据客户间的亲和关系来定位市场细分 二、Key terms 一个项目集是由形成关联规则的所有项目的列表表示。\n一个关联规则，{面包，鸡蛋} =\u0026gt; {牛奶}，或更普遍的 {X} =\u0026gt; {Y}，表明如果顾客一起购买面包和鸡蛋，他们很可能也会购买牛奶。\n• Support is a measure of absolute frequency.\n1 25% 的支持度表示在所有交易中，面包、鸡蛋和牛奶一起购买的频率为 25%。 • Confidence is a measure of correlative frequency.\n1 60% 的置信度表示购买了面包和鸡蛋的人中，有 60% 也购买了牛奶。 • Lift is a measure of the strength of association between the products on the left and right hand side of the rule.\n1 2 3 4 5 提升度是左侧和右侧规则中产品之间关联强度的度量。 规则中所有项目一起出现的概率除以左侧和右侧项目单独出现的概率的乘积，就得到了提升度。 例如，如果面包、鸡蛋和牛奶一起出现在所有交易的 2.5% 中，面包和鸡蛋一起出现在 10% 的交易中，牛奶在 8% 的交易中出现，那么提升度将为：0.025/(0.1*0.08) = 3.125。 提升度越大，两个产品之间的联系越紧密。提升度大于1表示面包和鸡蛋的存在增加了牛奶也出现在交易中的概率。 • Association Rule: {X → Y} is a representation of finding Y on the basket which has X on it\n• Itemset: {X,Y} is a representation of the list of all items which form the association rule\n• Support: Fraction of transactions containing the itemset\n• Confidence: Probability of occurrence of {Y} given {X} is present\n• Lift: Ratio of confidence to baseline probability of occurrence of {Y}\n1 2 3 4 5 • 关联规则：{X → Y} 表示在包含 X 的购物篮中找到 Y 的表示。 • 项目集：{X,Y} 是形成关联规则的所有项目的列表表示。 • 支持度：包含该项目集的交易的比例。 • 置信度：在 {X} 存在的情况下 {Y} 发生的概率。 • 提升度：置信度与 {Y} 基线发生概率的比值。 {% asset_image R_week7_schemas.png %}\n","date":"2024-03-30T00:00:00Z","permalink":"https://MyLoveES.github.io/p/rweek7-managing-sustainable-competitive-advantage-lecture-ii/","title":"R[week7] Managing Sustainable Competitive Advantage lecture II"},{"content":" R: 4.3.2 (2023-10-31) R studio: 2023.12.1+402 (2023.12.1+402)\n假设一家公司正在开发一款新系列的平板电脑，并试图确定平板电脑的尺寸应该有多大，以及应该具有何种类型的存储和内存。 为了支持这个决定，了解客户对这些不同特征的价值会很有帮助。\n客户喜欢还是不喜欢大屏幕尺寸？ 如果他们喜欢，他们愿意为更大尺寸的屏幕支付多少更多的费用？ 是否有一些客户群体比其他客户更喜欢大屏幕？ 1. Choice-Based Conjoint Analysis Data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \u0026gt; cbc.df\u0026lt;-read.csv(\u0026#34;Data_Conjoint_Choice.csv\u0026#34;, stringsAsFactors = TRUE) \u0026gt; str(cbc.df) \u0026#39;data.frame\u0026#39;:\t6165 obs. of 10 variables: $ ConsumerId : int 1 1 1 1 1 1 1 1 1 1 ... $ ChoiceSetId : int 1 1 1 2 2 2 3 3 3 4 ... $ AlternativeIdInSet: int 1 2 3 1 2 3 1 2 3 1 ... $ Choice : int 1 0 0 1 0 0 0 0 1 1 ... $ Brand : Factor w/ 5 levels \u0026#34;Galaxy\u0026#34;,\u0026#34;iPad\u0026#34;,..: 2 5 3 2 5 4 5 3 1 2 ... $ Size : Factor w/ 4 levels \u0026#34;sz10inch\u0026#34;,\u0026#34;sz7inch\u0026#34;,..: 2 1 4 3 1 2 3 4 1 4 ... $ Storage : Factor w/ 4 levels \u0026#34;st128gb\u0026#34;,\u0026#34;st16gb\u0026#34;,..: 3 4 2 3 1 4 4 2 1 2 ... $ Ram : Factor w/ 3 levels \u0026#34;r1gb\u0026#34;,\u0026#34;r2gb\u0026#34;,..: 3 2 2 1 3 1 1 3 2 3 ... $ Battery : Factor w/ 3 levels \u0026#34;b7h\u0026#34;,\u0026#34;b8h\u0026#34;,\u0026#34;b9h\u0026#34;: 1 3 2 2 1 3 1 3 3 1 ... $ Price : int 499 399 499 399 299 199 199 399 499 299 ... \u0026gt; head(cbc.df) ConsumerId ChoiceSetId AlternativeIdInSet Choice Brand Size Storage Ram Battery Price 1 1 1 1 1 iPad sz7inch st32gb r4gb b7h 499 2 1 1 2 0 Surface sz10inch st64gb r2gb b9h 399 3 1 1 3 0 Kindle sz9inch st16gb r2gb b8h 499 4 1 2 1 1 iPad sz8inch st32gb r1gb b8h 399 5 1 2 2 0 Surface sz10inch st128gb r4gb b7h 299 6 1 2 3 0 Nexus sz7inch st64gb r1gb b9h 199 cbc.df 中的前三行描述了对受访者1提出的第一个问题。选择列显示该受访者选择了第一个备选方案。 ConsumerId 表示回答这个问题的受访者。 ChoiceSetId 表示这前三行是第一个问题的配置文件。ChoiceSetId 编号 1:15 是为受访者1，然后 15:30 是为受访者2，依此类推。 AlternativeIdInSet 表示第一行是备选方案1，第二行是备选方案2，第三行是备选方案3。 Choice 表示受访者选择了哪个备选方案；对于每个选择问题中被指定为首选备选方案的配置文件，它的值为1。 现在重要的是估计一个完整的选择模型。任何建模的第一步是使用基本描述性统计了解数据。我们从摘要开始:\n1 2 3 4 5 6 7 8 \u0026gt; summary(cbc.df) ConsumerId ChoiceSetId AlternativeIdInSet Choice Brand Size Storage Ram Battery Price Min. : 1 Min. : 1 Min. :1 Min. :0.0000 Galaxy :1263 sz10inch:1371 st128gb:1376 r1gb:2192 b7h:1918 Min. :169.0 1st Qu.: 35 1st Qu.: 514 1st Qu.:1 1st Qu.:0.0000 iPad :1538 sz7inch :1767 st16gb :1370 r2gb:2055 b8h:2055 1st Qu.:199.0 Median : 69 Median :1028 Median :2 Median :0.0000 Kindle :1119 sz8inch :1520 st32gb :1774 r4gb:1918 b9h:2192 Median :299.0 Mean : 69 Mean :1028 Mean :2 Mean :0.3333 Nexus :1104 sz9inch :1507 st64gb :1645 Mean :307.5 3rd Qu.:103 3rd Qu.:1542 3rd Qu.:3 3rd Qu.:1.0000 Surface:1141 3rd Qu.:399.0 Max. :137 Max. :2055 Max. :3 Max. :1.0000 Max. :499.0 我们看到每个属性水平在问题中出现了多少次。总结选择数据的一个更具信息量的方式是计算选择计数，这是受访者在每个特征水平上选择备选方案的次数的交叉表。我们可以使用 xtabs() 轻松地完成这个任务。\n1 2 3 4 \u0026gt; xtabs(Choice~Price, data=cbc.df) Price 169 199 299 399 499 688 472 329 365 201 表格的行表示选择（Choice），在这个例子中，选择是一个二元变量（1 或 0）。 表格的列表示价格（Price），列中的数值表示该价格下被选择的次数。 具体到结果的解释如下： 在价格为 169 的情况下，被选择了 688 次。\n在价格为 199 的情况下，被选择了 472 次。\n在价格为 299 的情况下，被选择了 329 次。\n在价格为 399 的情况下，被选择了 365 次。\n在价格为 499 的情况下，被选择了 201 次。\n受访者更经常选择售价为 £169 的平板电脑，而不是售价为 £299 或 £499 的平板电脑。\n如果我们计算尺寸属性的计数，我们会发现选择在 7 英寸、8 英寸和 10 英寸之间更加平衡，而在 9 英寸上选择更多。\nxtabs()\n用于创建交叉表（Contingency Table）的函数。交叉表是一种用于展示两个或多个分类变量之间关系的表格，通常显示了每个组合的频数或频率。\n1 2 3 4 \u0026gt; xtabs(Choice~Size, data=cbc.df) Size sz10inch sz7inch sz8inch sz9inch 475 535 472 573 在估计选择模型之前，始终鼓励为每个属性计算选择计数。\n2. Prepare the data 我们现在可以估计我们的第一个选择模型了。通过拟合选择模型，我们可以精确地测量每个属性与受访者选择的关联程度。 我们使用 mlogit 包，你可能需要使用 install.packages() 安装。mlogit 估计最基本和常用的选择模型，即多项逻辑回归模型。\n2.1 Define reference levels - used when estimating model 1 2 3 4 5 cbc.df$Brand \u0026lt;- relevel(cbc.df$Brand, ref = \u0026#34;Nexus\u0026#34;) cbc.df$Size \u0026lt;- relevel(cbc.df$Size, ref = \u0026#34;sz7inch\u0026#34;) cbc.df$Storage \u0026lt;- relevel(cbc.df$Storage, ref = \u0026#34;st16gb\u0026#34;) cbc.df$Ram \u0026lt;- relevel(cbc.df$Ram, ref = \u0026#34;r1gb\u0026#34;) cbc.df$Battery \u0026lt;- relevel(cbc.df$Battery, ref = \u0026#34;b7h\u0026#34;) 2.2 Define data format mlogit 要求选择数据具有特定的数据格式。我们使用 dfidx 包中的 dfidx 函数来整理格式。\nchoice 参数指示包含响应数据的列。在我们的案例中，choice = “Choice”。 idx 参数指示备选方案的结构。列表中的第一个索引表示选择集和消费者的列，第二个索引表示每个选择集中备选方案的列。 1 cbc.mlogit \u0026lt;- dfidx(cbc.df, choice=\u0026#34;Choice\u0026#34;, idx=list(c(\u0026#34;ChoiceSetId\u0026#34;, \u0026#34;ConsumerId\u0026#34;), \u0026#34;AlternativeIdInSet\u0026#34;)) 3 Multinomial conjoint model estimation with mlogit() 当我们运行模型时，它会选择每个离散属性的参考水平。参考水平的效用被归一化为零。我们在数据加载阶段为每个离散属性指定了一个参考水平。这些参考水平是 Nexus、7 英寸屏幕、16GB 硬盘、1GB 内存、7 小时电池。我们将价格视为连续变量，因此不需要指定参考水平。\n该模型假定没有误差项的备选方案 j 的效用如下所示。\n$$ V_j = \\beta_{11}[ \\text{Brand} = \\text{Galaxy}] + \\beta_{12}[ \\text{Brand} = \\text{iPad}] + \\beta_{13}[ \\text{Brand} = \\text{Kindle}] + \\beta_{14}[ \\text{Brand} = \\text{Surface}] + \\beta_{21}[ \\text{Screen} = \\text{10inch}] + \\beta_{22}[ \\text{Screen} = \\text{9inch}] + \\beta_{23}[ \\text{Screen} = \\text{8inch}] + \\beta_{31}[ \\text{Storage} = \\text{128gb}] + \\beta_{32}[ \\text{Storage} = \\text{64gb}] + \\beta_{33}[ \\text{Storage} = \\text{32gb}] + \\beta_{41}[ \\text{RAM} = \\text{4gb}] + \\beta_{42}[ \\text{RAM} = \\text{2gb}] + \\beta_{51}[ \\text{Battery} = \\text{9h}] + \\beta_{52}[ \\text{Battery} = \\text{8h}] + \\beta_6 \\text{Price} $$其中，$ U_j = V_j + error $ 。也就是说，有 15 个参数 $ \\beta $ 需要估计。\n假设独立的极值误差分布，消费者以概率从三个备选方案中选择备选方案 j。\n$$ p_j = \\frac{\\exp(V_j)}{\\sum_{k=1}^{3} \\exp(V_k)}, \\quad \\text{for } j \\in \\{1,2,3\\} $$显然，$ p_1 + p_2 + p_3 = 1 $ 我们实际估计模型。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 \u0026gt; model\u0026lt;-mlogit(Choice ~ 0+Brand+Size+Storage+Ram+Battery+Price, data=cbc.mlogit) \u0026gt; kable(summary(model)$CoefTable) | | Estimate| Std. Error| z-value| Pr(\u0026gt;\u0026amp;#124;z\u0026amp;#124;)| |:--------------|----------:|----------:|----------:|------------------:| |BrandGalaxy | 0.3378857| 0.0925056| 3.652596| 0.0002596| |BrandiPad | 0.9780287| 0.0937336| 10.434136| 0.0000000| |BrandKindle | 0.2630105| 0.0996254| 2.639995| 0.0082907| |BrandSurface | 0.1450365| 0.0938521| 1.545373| 0.1222560| |Sizesz10inch | 0.3240632| 0.0841953| 3.848949| 0.0001186| |Sizesz8inch | 0.1890775| 0.0829232| 2.280151| 0.0225987| |Sizesz9inch | 0.4355415| 0.0808408| 5.387644| 0.0000001| |Storagest128gb | 0.5897703| 0.0870533| 6.774822| 0.0000000| |Storagest32gb | 0.2168719| 0.0829213| 2.615395| 0.0089124| |Storagest64gb | 0.5782183| 0.0808259| 7.153877| 0.0000000| |Ramr2gb | 0.3189348| 0.0672579| 4.741970| 0.0000021| |Ramr4gb | 0.6357438| 0.0645225| 9.853053| 0.0000000| |Batteryb8h | 0.1299599| 0.0651501| 1.994777| 0.0460672| |Batteryb9h | 0.1253824| 0.0650588| 1.927216| 0.0539528| |Price | -0.0050888| 0.0002752| -18.488626| 0.0000000| 3.1 Meaning of parameters 在估计后，我们得到了每个离散属性的每个水平（除了参考水平）的系数估计。这样的系数捕获了与参考相比属性水平的相对效用或部分价值。例如，在品牌属性的情况下，品牌iPad系数给出了iPad相对于Nexus（参考品牌）的品牌相对效用的估计。\n正号告诉我们，平均而言，客户更喜欢iPad而不是Nexus，因为较大的估计表示更强的偏好，所以我们可以看到客户非常喜欢64GB存储（相对于基本水平，即16GB）。这些参数估计值是在logit刻度上的。\n在连续价格的情况下，我们得到一个单一的系数，它捕捉到当价格增加一单位（$1）时备选方案的效用如何变化，同时保持备选方案的所有其他特征不变。\n3.2 Model fit 有人可能会想知道偏好是否仅受品牌效应驱动。我们可以估计一个只有品牌作为预测变量的模型。\n1 \u0026gt; model.constraint \u0026lt;-mlogit(Choice ~ 0+Brand, data = cbc.mlogit) mlogit() 用于拟合多项 Logit 模型。多项 Logit 模型是一种用于处理多分类问题的统计模型，通常用于解释和预测个体选择不同类别的概率。\n然后我们可以使用 lrtest 来比较这两个模型。\n1 2 3 4 5 6 7 8 9 10 \u0026gt; lrtest(model, model.constraint) Likelihood ratio test Model 1: Choice ~ 0 + Brand + Size + Storage + Ram + Battery + Price Model 2: Choice ~ 0 + Brand #Df LogLik Df Chisq Pr(\u0026gt;Chisq) 1 15 -1938.9 2 4 -2218.0 -11 558.29 \u0026lt; 2.2e-16 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 lrtest() 用于进行两个线性回归模型之间的 Likelihood Ratio Test（似然比检验）。Likelihood Ratio Test 用于比较两个具有不同复杂度的模型是否在解释数据方面有显著的差异。\n4. Interpreting Conjoint Analysis Findings 通常很难直接解释选择模型的部分效用估计。系数处于一个不熟悉的尺度上（即log），它们衡量了各个水平的相对偏好，这使得它们很难理解。因此，大多数选择模型偏好专注于使用模型进行选择份额预测或计算每个属性的支付意愿，而不是呈现系数。\n4.1 Predicted Market Share 我们还可以使用估计的参数来预测数据中不同备选方案的选择概率。在这里，我们打印出数据中前六个选择集的预测。\n1 2 3 4 5 6 7 8 9 10 \u0026gt; kable(head(predict(model,cbc.mlogit))) | 1| 2| 3| |---------:|---------:|---------:| | 0.3717263| 0.4405521| 0.1877216| | 0.2367797| 0.4718620| 0.2913583| | 0.4760867| 0.2974319| 0.2264814| | 0.3730366| 0.4456505| 0.1813129| | 0.3984632| 0.1618560| 0.4396807| | 0.3791075| 0.2506170| 0.3702755| 现在，我们可以测量整个数据集中预测的准确性。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 \u0026gt; predicted_alternative \u0026lt;- apply(predict(model,cbc.mlogit),1,which.max) \u0026gt; selected_alternative \u0026lt;- cbc.mlogit$AlternativeIdInSet[cbc.mlogit$Choice\u0026gt;0] \u0026gt; confusionMatrix(table(predicted_alternative,selected_alternative),positive = \u0026#34;1\u0026#34;) Confusion Matrix and Statistics selected_alternative predicted_alternative 1 2 3 1 362 158 130 2 164 449 149 3 136 160 347 Overall Statistics Accuracy : 0.5635 95% CI : (0.5417, 0.5851) No Information Rate : 0.3732 P-Value [Acc \u0026gt; NIR] : \u0026lt;2e-16 Kappa : 0.343 Mcnemar\u0026#39;s Test P-Value : 0.8875 Statistics by Class: Class: 1 Class: 2 Class: 3 Sensitivity 0.5468 0.5854 0.5543 Specificity 0.7933 0.7570 0.7929 Pos Pred Value 0.5569 0.5892 0.5397 Neg Pred Value 0.7865 0.7541 0.8024 Prevalence 0.3221 0.3732 0.3046 Detection Rate 0.1762 0.2185 0.1689 Detection Prevalence 0.3163 0.3708 0.3129 Balanced Accuracy 0.6700 0.6712 0.6736 请注意，如果预测是随机的，准确率将为33.3%（对于三个备选方案）。我们的简单模型比这要好得多，尽管它并不完美。\n4.2 Conjoint simulator 现在，让我们看看如何使用模型参数场景来预测在假设市场条件下任意一组产品的市场份额。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 \u0026gt; predict.share \u0026lt;- function(model, d) { + temp \u0026lt;- model.matrix(update(model$formula, 0 ~ .), data = d)[,-1] # generate dummy matri + u \u0026lt;- temp%*% .... [TRUNCATED] \u0026gt; # hypothetical base market structure with 4 alternatives in the market \u0026gt; d.base \u0026lt;- cbc.df[c(44,34,33,40),c(\u0026#34;Brand\u0026#34;,\u0026#34;Size\u0026#34;,\u0026#34;Storage\u0026#34;,\u0026#34;Ram\u0026#34;, \u0026#34;Battery\u0026#34; .... [TRUNCATED] \u0026gt; d.base \u0026lt;- cbind(d.base,as.vector(predict.share(model,d.base))) \u0026gt; colnames(d.base)[7] \u0026lt;- \u0026#34;Predicted.Share\u0026#34; \u0026gt; rownames(d.base) \u0026lt;- c() \u0026gt; kable(d.base) |Brand |Size |Storage |Ram |Battery | Price| Predicted.Share| |:-------|:--------|:-------|:----|:-------|-----:|---------------:| |iPad |sz7inch |st64gb |r2gb |b8h | 399| 0.3423928| |Galaxy |sz10inch |st32gb |r2gb |b7h | 299| 0.2540301| |Surface |sz10inch |st64gb |r1gb |b7h | 399| 0.1313854| |Kindle |sz7inch |st32gb |r1gb |b9h | 169| 0.2721917| 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \u0026gt; # hypothetical market structure after Galaxy gets a RAM upgrade \u0026gt; d.new \u0026lt;- d.base \u0026gt; d.new[2, \u0026#39;Ram\u0026#39;] \u0026lt;- \u0026#34;r4gb\u0026#34; \u0026gt; d.new$Predicted.Share \u0026lt;- as.vector(predict.share(model,d.new)) \u0026gt; kable(d.new) |Brand |Size |Storage |Ram |Battery | Price| Predicted.Share| |:-------|:--------|:-------|:----|:-------|-----:|---------------:| |iPad |sz7inch |st64gb |r2gb |b8h | 399| 0.3127768| |Galaxy |sz10inch |st32gb |r4gb |b7h | 299| 0.3185544| |Surface |sz10inch |st64gb |r1gb |b7h | 399| 0.1200209| |Kindle |sz7inch |st32gb |r1gb |b9h | 169| 0.2486479| 4.3 Willingness to pay 非常重要的是，使用参数估计，我们通过除以该属性的系数来为所选属性水平估计，换句话说，我们估计价格的变化将导致由于问题属性水平变化而引起的效用变化的等价物。例如，我们发现，普通消费者在选择是否购买 Galaxy 和支付额外 $125.8 之间处于中立状态，或者购买 iPad。换句话说，普通消费者愿意支付高达 $125.8 来获取 iPad 而不是 Nexus，同时保持所有其他特征不变。\n4.3.1 What is the brand value of iPad relative to Galaxy? 品牌资产 - 从 Galaxy 升级到 iPad 的美元价值\n1 2 3 \u0026gt; (coef(model)[\u0026#34;BrandiPad\u0026#34;]-coef(model)[\u0026#34;BrandGalaxy\u0026#34;]) / (-coef(model)[\u0026#34;Price\u0026#34;]) BrandiPad 125.7944 coef()\n提取模型系数的函数，可以用于提取线性回归、逻辑回归、多项式回归等模型的系数。\n4.3.2 Willingness to Pay for an Attribute Upgrade 从 1GB 升级到 4GB RAM 的美元价值（1GB 是参考水平。因此其系数为0）。\n1 2 3 \u0026gt; coef(model)[\u0026#34;Sizesz9inch\u0026#34;] / (-coef(model)[\u0026#34;Price\u0026#34;]) Sizesz9inch 85.5882 5. Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 library(\u0026#34;xtable\u0026#34;) # processing of regression output library(\u0026#34;knitr\u0026#34;) # used for report compilation and table display library(\u0026#34;ggplot2\u0026#34;) # very popular plotting library ggplot2 library(\u0026#34;mlogit\u0026#34;) # multinomial logit library(\u0026#34;caret\u0026#34;) # ConfusionMatrix # 获取当前已加载文件的目录 file_dir \u0026lt;- dirname(parent.frame(2)$ofile) print(file_dir) # 将工作目录设置为当前已加载文件的目录 setwd(file_dir) cbc.df\u0026lt;-read.csv(\u0026#34;Data_Conjoint_Choice.csv\u0026#34;, stringsAsFactors = TRUE) str(cbc.df) head(cbc.df) summary(cbc.df) xtabs(Choice~Price, data=cbc.df) xtabs(Choice~Size, data=cbc.df) cbc.df$Brand \u0026lt;- relevel(cbc.df$Brand, ref = \u0026#34;Nexus\u0026#34;) cbc.df$Size \u0026lt;- relevel(cbc.df$Size, ref = \u0026#34;sz7inch\u0026#34;) cbc.df$Storage \u0026lt;- relevel(cbc.df$Storage, ref = \u0026#34;st16gb\u0026#34;) cbc.df$Ram \u0026lt;- relevel(cbc.df$Ram, ref = \u0026#34;r1gb\u0026#34;) cbc.df$Battery \u0026lt;- relevel(cbc.df$Battery, ref = \u0026#34;b7h\u0026#34;) library(dfidx) #install if needed cbc.mlogit \u0026lt;- dfidx(cbc.df, choice=\u0026#34;Choice\u0026#34;, idx=list(c(\u0026#34;ChoiceSetId\u0026#34;, \u0026#34;ConsumerId\u0026#34;), \u0026#34;AlternativeIdInSet\u0026#34;)) model\u0026lt;-mlogit(Choice ~ 0+Brand+Size+Storage+Ram+Battery+Price, data=cbc.mlogit) kable(summary(model)$CoefTable) model.constraint \u0026lt;-mlogit(Choice ~ 0+Brand, data = cbc.mlogit) lrtest(model, model.constraint) kable(head(predict(model,cbc.mlogit))) predicted_alternative \u0026lt;- apply(predict(model,cbc.mlogit),1,which.max) selected_alternative \u0026lt;- cbc.mlogit$AlternativeIdInSet[cbc.mlogit$Choice\u0026gt;0] confusionMatrix(table(predicted_alternative,selected_alternative),positive = \u0026#34;1\u0026#34;) predict.share \u0026lt;- function(model, d) { temp \u0026lt;- model.matrix(update(model$formula, 0 ~ .), data = d)[,-1] # generate dummy matri u \u0026lt;- temp%*%model$coef[colnames(temp)] # calculate utilities; %*% is matrix multiplicati probs \u0026lt;- t(exp(u)/sum(exp(u))) # calculate probabilities colnames(probs) \u0026lt;- paste(\u0026#34;alternative\u0026#34;, colnames(probs)) return(probs) } # hypothetical base market structure with 4 alternatives in the market d.base \u0026lt;- cbc.df[c(44,34,33,40),c(\u0026#34;Brand\u0026#34;,\u0026#34;Size\u0026#34;,\u0026#34;Storage\u0026#34;,\u0026#34;Ram\u0026#34;, \u0026#34;Battery\u0026#34;,\u0026#34;Price\u0026#34;)] d.base \u0026lt;- cbind(d.base,as.vector(predict.share(model,d.base))) colnames(d.base)[7] \u0026lt;- \u0026#34;Predicted.Share\u0026#34; rownames(d.base) \u0026lt;- c() kable(d.base) # hypothetical market structure after Galaxy gets a RAM upgrade d.new \u0026lt;- d.base d.new[2, \u0026#39;Ram\u0026#39;] \u0026lt;- \u0026#34;r4gb\u0026#34; d.new$Predicted.Share \u0026lt;- as.vector(predict.share(model,d.new)) kable(d.new) (coef(model)[\u0026#34;BrandiPad\u0026#34;]-coef(model)[\u0026#34;BrandGalaxy\u0026#34;]) / (-coef(model)[\u0026#34;Price\u0026#34;]) coef(model)[\u0026#34;Sizesz9inch\u0026#34;] / (-coef(model)[\u0026#34;Price\u0026#34;]) ","date":"2024-03-29T00:00:00Z","permalink":"https://MyLoveES.github.io/p/rweek6-managing-sustainable-competitive-advantage-code/","title":"R[week6] Managing Sustainable Competitive Advantage code"},{"content":" R: 4.3.2 (2023-10-31) R studio: 2023.12.1+402 (2023.12.1+402)\n了解通过提供创新产品来管理可持续竞争优势（SCA）的重要性 能够进行共同分析以评估新产品/服务的开发 1. Innovation Offering 创新的新产品有助于企业建立和维持可持续竞争优势，并阻碍因竞争对手不断对企业的成功做出反应而产生的竞争攻击（MP#3）。 提供是一个有意广泛的术语，涵盖了企业提供的既有形产品又有无形服务。 创新是“通过创造性地改变业务的一个或多个维度，为客户创造实质性的新价值”。 创新的关键方面 比产品或技术创新更广泛 必须为客户创造新价值 涉及导致差异化和可持续竞争优势的变化 星巴克、戴尔和iPod创造了可持续竞争优势。 1.1 Benefits of Innovation and Offering’s Equity 创新和产品价值的权益\n提供的权益指的是产品或服务性能为客户提供的核心价值。 通过建立提供的权益，一个创新的公司可以使竞争对手更难侵入其业务。 新的产品往往会激励客户从竞争对手转向创新公司，以获得新产品的使用权限。 新的产品还可以帮助公司获得新客户或进入新市场，尤其是当它们提供类似性能但价格更低时。 提供新的创新产品往往会增强公司的品牌形象，即使客户没有购买新产品。 1.2 Stage-Gate Design Review Process for Effective Product Development 阶段门设计审查流程用于有效的产品开发\nConcept and Definition\n概念与定义阶段包括对所有潜在想法进行初步筛选、概念评估、项目定义和可行性评估。 Design and Development\n设计与开发阶段包括产品和流程的设计与开发。还包括财务可行性考虑，包括价格点和客户接受度的测试。 Validation and Production\n验证与生产阶段包括持续的市场推出规划、产品制造和流程验证。也可能包括测试营销和启动计划的评估。 Final Audit\n审计阶段包括最终产品和产品评估。通常还包括对一些反思 1.3 Research Approaches for Designing and Launching New Offerings 设计和推出新产品的研究方法\n在开发早期，定性技术如观察、焦点小组和客户访谈非常有效；它们能揭示一些重要的新需求，或者是公司不知道的需求。 为了避免新产品高失败率带来的风险，公司可以采用不同的技术来提升决策水平，避免不成功的推出，如共同分析 2. Choice-Based Conjoint Analysis 2.1 Product design task 产品可以被看作是属性水平或特征的捆绑。 产品特征为消费者提供价值 {% asset_image week6_lec_1.png %}\n最佳设计涉及选择产品的属性水平，以最大化目标 典型目标 市场份额 盈利能力 最佳设计是基于对消费者偏好的分析 2.2 conjoint analysis 共同分析有助于使新产品“更”成功 产品优势推动财务成功 产品设计需要做出权衡决策（价格、性能、尺寸、位置、功能等） 共同分析是一种方法，用于 理解消费者在产品属性和特征之间做出权衡 测量产品属性对消费者的重要性 使用共同分析，营销人员可以将产品视为属性捆绑包，然后确定哪种属性组合最适合满足客户的偏好，从而设计和开发新产品。\n何时使用？ 识别客户为新产品愿意做出的产品属性权衡 预测提议的新产品（即属性捆绑）的市场份额和影响 确定客户愿意为新产品支付的金额 2.3 Conceptual Underpinnings of Conjoint Analysis 共同分析的概念基础\n消费者在属性水平上的效用不同 产品的效用 = 其属性水平的效用之和 $$ U = u(\\text{ProcessorLevel}) + u(\\text{RamLevel}) + u(\\text{HardDiskLevel}) + u(\\text{PriceLevel}) + \\ldots $$ 可以通过消费者对产品配置的评估来测量效用 可以利用效用估计来预测新产品的市场份额 2.4 Conjoint Analysis Process 设计研究 选择属性和水平（范围和数量） 制定产品配置（最多不超过16个） 收集受访者数据 设计数据收集工具 获取受访者对配置的偏好或评级 分析数据 计算部分效用值 评估产品设计选项 评估市场模拟 评估不同的选择规则 2. Multinomial Logistic Regression (MNL) Model 多项逻辑回归（MNL）模型\n每个备选方案对消费者都有一个效用 效用是产品属性的函数 它是对平板电脑吸引力的衡量 在面对选择集时，消费者选择具有最大效用的平板电脑 2.1 Every Attribute Level has a Sub-Utility (Part-Worth) 每个属性水平都有一个子效用（部分效用）\n例如，每个品牌的价值为： Galaxy：$β_{Gal}$ iPad：$β_{iPad}$ Kindle：$β_{Kind}$ Surface：$β_{Surf}$ Nexus：0（参考值） β值是从数据中估计的参数 $$ V_j = β_{iPad} \\times iPad_j + β_{Gal} \\times Gal_j + β_{Kind} \\times Kind_j + β_{Surf} \\times Surf_j\nβ_{10} \\times 10inch_j + β_{9} \\times 9inch_j + β_{8} \\times 8inch_j β_{128gbhd} \\times 128gbhd_j + β_{64gbhd} \\times 64gbhd_j + β_{32gbhd} \\times 32gbhd_j β_{ram4} \\times ram4gb_j + β_{ram2} \\times ram2gb_j β_{batt9} \\times batt9hrs_j + β_{batt8} \\times batt8hrs_j β_{price} \\times Price_j $$ {% asset_image week6_lec_2.png %}\n在使用 MNL（Multinomial Logit）模型进行参数估计时，通常会获得一些统计量来评估模型参数的显著性。在统计软件中，例如 R 或 Python 中，这些统计量通常包括以下几个：\nEstimate：表示模型参数的估计值。它是在模型拟合过程中计算出来的参数值，代表了每个自变量对因变量的影响程度。\nStd.Error：表示模型参数估计值的标准误差。标准误差是对模型参数估计值的不确定性进行的一个度量，它越小表示模型参数估计值越可靠。\nt-value：表示模型参数估计值与零的偏差除以标准误差的结果，称为 $t$ 值。$t$ 值用于检验模型参数的显著性，如果 $t$ 值的绝对值越大，表明模型参数越显著。\nPr(\u0026gt;|t|)：表示 $t$ 值对应的 $p$ 值，即检验模型参数的显著性水平。$p$ 值是在原假设成立的情况下观察到的 $t$ 值或更极端情况的概率。如果 $p$ 值小于预先设定的显著性水平（例如 $0.05$），则可以拒绝原假设，认为模型参数是显著的。\n2.2 Tablet Conjoint (sub) Utilities (β Parameters Estimates) 平板电脑共同分析效用（β参数估计）\n参考水平用灰色标记 {% asset_image week6_lec_3.png %}\n在选择任务示例中，平板电脑1和2的效用\n{% asset_image week6_lec_4.png %}\nChoice Probabilities $p_1 = \\frac{exp(V_1)}{exp(V_1) + exp(V_2) + exp(V_3)}$ $p_2 = \\frac{exp(V_2)}{exp(V_1) + exp(V_2) + exp(V_3)}$ $p_3 = \\frac{exp(V_3)}{exp(V_1) + exp(V_2) + exp(V_3)}$\n$0 \\leq p_i \\leq 1, \\forall i$\n$\\sigma_i p_i = 1$ or $p_1 + p_2 + p_3 = 1$\nPredicted Market Share 假设市场上有这三款平板电脑： {% asset_image week6_lec_5.png %}\nHit Rate: Choice Prediction Accuracy {% asset_image week6_lec_5.png %}\nConjoint Simulator 2GB RAM 升级对 Galaxy 市场份额的影响: {% asset_image week6_lec_7.png %}\nWhat is the Brand Value of iPad Relative to Galaxy? 相对于 Galaxy，iPad 的品牌价值是多少？ $$ β_iPad - β_Galaxy ≈ 0.6401 $$$$ iPad 的价值 = (β_iPad - β_Galaxy) / |β_Price| = 0.6401 * $196.54 = $125.80 $$一个普通消费者在选择购买 Galaxy 平板电脑再支付额外 $125.80 和购买 iPad 之间会感到无所谓。\n类似的技术经常在诉讼中使用，例如，三星对苹果：https://www.greenbook.org/mr/market-research-methodology/how-apple-samsung-and-conjoint-came-together/\nWillingness to Pay for an Attribute Upgrade 对属性升级的支付意愿:\n$$ β_{4gbRAM} ≈ 0.6357 $$ $$ β_{1gbRAM} = 0 $$4GB Ram Value = 0.6357*$196.54= $124.94\n一个普通消费者愿意支付高达 $124.94 从 1gb 升级到 4gb RAM，保持所有其他属性不变。\n3. Takeaways 共同分析可以通过帮助管理者根据消费者对各种产品属性的价值来定义最佳产品，从而促进新产品的设计和推出。 共同分析是行业中最流行的营销分析工具 共同分析有不同类型 选择型共同分析 ‒ 在行业中使用最广泛，可以纳入无选择替代方案以更好地捕捉需求 ","date":"2024-03-28T00:00:00Z","permalink":"https://MyLoveES.github.io/p/rweek6-managing-sustainable-competitive-advantage-lecture/","title":"R[week6] Managing Sustainable Competitive Advantage lecture"},{"content":" R: 4.3.2 (2023-10-31) R studio: 2023.12.1+402 (2023.12.1+402)\n1. Customer Lifetime Value (CLV) 1.1 The Marketing Problem 我们需要了解我们的客户，或者知道某个特定的客户是否与我们的组织相关。因此：\n我们想知道客户为组织创造了多少价值。 我们想知道生成的价值将如何演变。 我们希望根据价值对客户进行细分。 这样我们就可以针对不同的客户群体执行特定的操作。 1.2 Customer Lifetime Value (CLV) 解决业务问题的技术是理解客户的价值。我们有很多方法来了解客户的价值。其中最相关的之一是客户生命周期价值（CLV）。\n创建一个仅包含固定酸度大于或等于8的葡萄酒的数据框。 创建一个仅包含品质为7的葡萄酒的数据框。 创建一个仅包含2列（pH和品质）的数据框。 创建一个仅包含2列（pH和品质）的数据框，其中葡萄酒按照pH的升序排序。 创建一个仅包含2列（pH和品质）的数据框，其中葡萄酒按照品质的降序排序。 创建一个仅包含没有柠檬酸的葡萄酒的数据框，并按照品质的降序排序。 创建一个红葡萄酒数据集的版本，其中包含一个额外的列，该列包含酒精含量乘以密度。 创建一个总结红葡萄酒数据集中每个变量的数据框。CLV 是企业与客户整个关系中获得的所有价值的预测。所有客户生成的价值称为客户权益，用于评估公司。\n主要概念。影响客户价值的因素很多，在 CLV 计算中必须考虑其中的一些：\n现金流：客户与组织的关系期间对组织产生的收入的净现值。 生命周期：与组织的关系持续的时间。 维护成本：确保每位客户的收入流量的相关成本。 风险成本：与客户相关的风险。 获取成本：获取新客户所需的成本和努力。 保留成本：保留新客户所需的成本和努力。 推荐价值：客户在其影响范围内对公司收入的推荐影响。 细分改进：客户信息在改进客户细分模型中的价值。 CLV\n$$ CLV = \\sum_{t=0}^{T} \\frac{(p_t - c_t) r_t}{(1 + i)^t} - AC $$其中，\n$p_t$ 表示时间 $t$ 客户支付的价格， $c_t$ 表示时间 $t$ 的客户服务直接成本， $i$ 表示公司的折现率或资金成本。 $r$ 表示客户在时间 $t$ 返回购买或保持活跃的概率， $AC$ 表示获取客户的成本， $T$ 表示用于估算客户生命周期价值（CLV）的时间范围。 A simplification\n让我们假设 $ p $ 和 $ c $ 是常数值，而 $ r $ 是随时间递减的函数，那么公式如下所示：\n$$ CLV = \\sum_{t=0}^{\\infty} \\frac{m \\cdot r^t}{(1 + i)^t} - AC = m \\cdot \\left( \\frac{r}{1 + i - r} \\right) - AC $$其中，$ p - c = m $ 表示利润，$ r $ 表示保留率，$ i $ 表示折现率。\n最终考虑：\n通常，我们会为公式中的每个概念拥有一个数据集。 我们需要估计其余的数值。 因此，重要的是要记住：我们对未来的预测能力受到我们过去经验的限制。 1.3 Implementation Process and Use Cases 实施过程如下所示：\n讨论 CLV 是否适合作为我们业务的指标 辨别和理解数据源和元数据 提取、转换、清理和加载数据 选择 CLV 方法 分析结果并调整参数 展示并解释结果 CLV常用于：\n基于CLV制定市场战略 基于CLV的客户细分 跨段落进行预测和客户演变分析 基于CLV创建不同的沟通、服务和忠诚计划 唤醒“非活跃”客户 估算公司的价值（在收购情况下，例如初创公司） 1.4 ARPU/ARPA as CLV approximation 平均用户收入（ARPU）或平均账户收入（ARPA）可用于计算历史CLV。 过程如下：\n计算每位客户每月的平均收入（即总收入/客户加入的月份数） 将它们相加 乘以12或24以获得一年或两年的CLV。 练习：假设Josep和Laura是您唯一的客户，他们的购买情况如下：\nCustomer Name Purchase Date Amount Josep Jan 1, 2023 $150 Josep May 1, 2023 $45 Josep May 15, 2023 $50 Laura Jun 15, 2023 $100 Laura Jun 15, 2023 $75 Laura Jun 30, 2023 $100 假设今天是2023年7月1日。\nJosep的平均每月收入是 (150 + 50 + 100) / 6 = 50，\nLaura的平均每月收入是 (45 + 75 + 100) / 2 = 110。\n将这两个数字相加，得到每位客户的平均月收入为 160 / 2 = 80。要找到12个月或24个月的CLV，将该数字乘以12或24。\nARPU方法的好处是计算简单，但它没有考虑到客户行为的变化。\n1.5 How to implement CLV in R 1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026gt; CLV.df \u0026lt;- read_excel(\u0026#34;Data_CLV.xlsx\u0026#34;, sheet = \u0026#34;Ex2\u0026#34;) \u0026gt; CLV.df # A tibble: 7 × 6 t active p c i r \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; 1 1 200 375 20 0.01 0.91 2 2 165 375 20 0.01 0.825 3 3 150 375 20 0.01 0.75 4 4 176 375 20 0.01 0.88 5 5 178 375 20 0.01 0.89 6 6 150 375 20 0.01 0.75 7 7 135 375 20 0.01 0.675 t：表示时间，通常表示观察期的长度或预测的时间段。它可以以天、月、年等为单位。\nactive：表示客户是否活跃。在 CLV 模型中，通常将客户分为活跃客户和不活跃客户，根据他们在观察期内的购买行为或者活动频率进行分类。\np：表示客户购买或者消费的总额。它是客户在观察期内购买的产品或服务的总额度，也称为客户价值。\nc：表示客户获取或者留存的成本。这包括了获取新客户的营销成本、维护现有客户的成本以及服务客户的成本等。\ni：表示折现率或贴现率。在 CLV 模型中，通常使用折现率来考虑时间价值的概念，即未来收入的现值。\nr：表示留存率。它是指客户在一段时间内继续购买产品或服务的概率，通常作为客户流失的相反概率。\n1.5.1 Exploratory Data Analysis 1 2 3 4 5 6 7 8 \u0026gt; summary(CLV.df) t active p c i r Min. :1.0 Min. :135.0 Min. :375 Min. :20 Min. :0.01 Min. :0.6750 1st Qu.:2.5 1st Qu.:150.0 1st Qu.:375 1st Qu.:20 1st Qu.:0.01 1st Qu.:0.7500 Median :4.0 Median :165.0 Median :375 Median :20 Median :0.01 Median :0.8250 Mean :4.0 Mean :164.9 Mean :375 Mean :20 Mean :0.01 Mean :0.8114 3rd Qu.:5.5 3rd Qu.:177.0 3rd Qu.:375 3rd Qu.:20 3rd Qu.:0.01 3rd Qu.:0.8850 Max. :7.0 Max. :200.0 Max. :375 Max. :20 Max. :0.01 Max. :0.9100 摘要显示价格和成本是恒定的。活跃客户的范围是[135,200]。保留率的范围是[0.83,1.11]。\n客户演变。我们可以创建一条折线图来了解客户数量的变化情况。 ggplot() 用于构建初始绘图对象，并几乎总是紧跟着 + 添加组件到图中。\nActive Customer Evolution\n1 2 3 ggplot(CLV.df, aes(x = t, y = active)) + geom_line() + ylab(\u0026#34;Customer\u0026#34;) + xlab(\u0026#34;Period\u0026#34;) + ggtitle(\u0026#34;Active Customer Evolution\u0026#34;) {% asset_image week5_code_1.png %}\nRetention Ratio Evolution\n1 2 ggplot(CLV.df, aes(x = t, y = r)) + geom_line() + ylab(\u0026#34;Customer\u0026#34;) + xlab(\u0026#34;Period\u0026#34;) + ggtitle(\u0026#34;Retention Ratio Evolution\u0026#34;) {% asset_image week5_code_2.png %}\n问题：我们观察到了什么？由于一些变量是恒定的，CLV只取决于客户数量。图表与之前的类似。\n1.5.2 Calculate CLV 现在，我们可以计算（历史）CLV。首先，我们创建一个新的列，其中包含每个时期的CLV：\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026gt; CLV.df$CLV \u0026lt;- (CLV.df$p-CLV.df$c)*CLV.df$r^CLV.df$t/(1+CLV.df$i)^CLV.df$t \u0026gt; CLV.df # A tibble: 7 × 7 t active p c i r CLV \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; 1 1 200 375 20 0.01 0.91 320. 2 2 165 375 20 0.01 0.825 237. 3 3 150 375 20 0.01 0.75 145. 4 4 176 375 20 0.01 0.88 205. 5 5 178 375 20 0.01 0.89 189. 6 6 150 375 20 0.01 0.75 59.5 7 7 135 375 20 0.01 0.675 21.1 CLV演变。现在我们创建一个图表来看看CLV的变化：\n1 ggplot(CLV.df, aes(x = t, y = CLV)) + geom_line() + ggtitle(\u0026#34;CLV evolution\u0026#34;) + ylab(\u0026#34;CLV\u0026#34;) + xlab(\u0026#34;Period\u0026#34;) CLV evolution\n{% asset_image week5_code_3.png %}\n问题：我们观察到了什么？由于一些变量是恒定的，CLV只取决于客户数量。图表与之前的类似。\n1 2 3 4 5 \u0026gt; CLV \u0026lt;- apply(CLV.df, 2, sum) \u0026gt; CLV[7] CLV 1175.932 问题：这个值代表什么意思？CLV值代表了客户的生命周期价值，即客户在其与公司的整个关系中为公司带来的预期收益总和。这个值可以帮助公司评估客户的重要性，并为营销策略和资源分配提供指导。\n1.6 Exercise: 如果保留率恒定为0.80会发生什么呢？\n解决方案 首先，我们可以添加一个新的列：\n1 CLV.df$CLV2 \u0026lt;- (CLV.df$p - CLV.df$c) * 0.8 / (1 + CLV.df$i)^(CLV.df$t - 1) 然后，我们可以创建一个新的图表：\n1 ggplot(CLV.df, aes(x = t, y = CLV2)) + geom_line() + ylab(\u0026#34;CLV2\u0026#34;) + xlab(\u0026#34;Period\u0026#34;) + labs(title = \u0026#34;CLV 2 Evolution\u0026#34;) {% asset_image week5_code_4.png %}\n最后，我们可以计算CLV值：\n1 2 3 4 5 6 7 8 9 \u0026gt; CLV \u0026lt;- apply(CLV.df, 2, sum) \u0026gt; CLV[7] CLV 1175.932 \u0026gt; CLV[8] CLV2 1929.915 CLV[7] 代表了我们在分析中所考虑的时间段内的 CLV 值，通常表示了一个时间段的结束。例如，如果我们在分析中考虑的是过去一年的数据，则 CLV[7] 就表示了过去一年的客户生命周期价值。\nCLV[8] 则代表了在 CLV[7] 时间段之后的 CLV 值，通常表示了一个新的时间段的开始。例如，如果我们想要预测未来一年的客户生命周期价值，则 CLV[8] 就表示了未来一年的预期 CLV 值。\n2 Data Manipulation 需要安装dplyr包。获取dplyr的最简单方法是安装tidyverse：install.packages(\u0026quot;tidyverse\u0026quot;)。 tidyverse是一组设计用于数据科学的R包。所有的包都共享一个基础设计理念、语法和数据结构。核心tidyverse包括以下几个：\nggplot2是一个用于声明性创建图形的系统。 dplyr提供数据操作的语法，提供一组一致的动词，解决最常见的数据操作挑战。 tidyr提供了一组函数，帮助您将数据整理成整洁的形式。整洁的数据是具有一致形式的数据：简言之，每个变量都放在一列中，每列都是一个变量。 readr提供了一种快速友好的方式来读取矩形数据（例如csv、tsv和fwf） stringr提供了一组连贯的函数，旨在使处理字符串变得尽可能简单。 forcats提供了一组有用的工具，解决了因子（分类变量）的常见问题 purr tibble 或者，只安装dplyr：install.packages(\u0026quot;dplyr\u0026quot;, dependencies=TRUE)。Dependencies = TRUE将安装此包所需的依赖项。\n让我们加载包和数据集。mtcars数据集随dplyr包一起提供。我们可以使用这些包来探索数据。\nmtcars\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \u0026gt; head(mtcars) mpg cyl disp hp drat wt qsec vs am gear Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 carb Mazda RX4 4 Mazda RX4 Wag 4 Datsun 710 1 Hornet 4 Drive 1 Hornet Sportabout 2 Valiant 1 \u0026gt; str(mtcars) \u0026#39;data.frame\u0026#39;:\t32 obs. of 11 variables: $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... $ disp: num 160 160 108 258 360 ... $ hp : num 110 110 93 110 175 105 245 62 95 123 ... $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... $ wt : num 2.62 2.88 2.32 3.21 3.44 ... $ qsec: num 16.5 17 18.6 19.4 17 ... $ vs : num 0 0 1 1 0 1 0 1 1 1 ... $ am : num 1 1 1 0 0 0 0 0 0 0 ... $ gear: num 4 4 4 3 3 3 3 4 4 4 ... $ carb: num 4 4 1 1 2 1 4 2 2 4 ... 2.1 Manipulate cases filter() 函数根据其值选择案例。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \u0026gt; filter(mtcars, gear == 4) # 过滤器可与所有标准逻辑运算符一起使用 mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 \u0026gt; filter(mtcars, gear == 3 | gear == 4) # 即 \u0026gt;、\u0026lt;、=\u0026gt;、\u0026lt;=、!=、== mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 \u0026gt; filter(mtcars, mpg \u0026gt; 21) mpg cyl disp hp drat wt qsec vs am gear carb Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 arrange() 函数按照选定列的值对数据框的行进行排序。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 \u0026gt; arrange(mtcars, gear) mpg cyl disp hp drat wt qsec vs am gear carb Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 \u0026gt; arrange(mtcars, gear, mpg) mpg cyl disp hp drat wt qsec vs am gear carb Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 \u0026gt; arrange(mtcars, desc(mpg)) # desc 以降序排序 mpg cyl disp hp drat wt qsec vs am gear carb Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 2.2 Manipulate variables select() 函数根据变量名选择变量。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 \u0026gt; select(mtcars, gear, mpg, hp) gear mpg hp Mazda RX4 4 21.0 110 Mazda RX4 Wag 4 21.0 110 Datsun 710 4 22.8 93 Hornet 4 Drive 3 21.4 110 Hornet Sportabout 3 18.7 175 Valiant 3 18.1 105 Duster 360 3 14.3 245 Merc 240D 4 24.4 62 Merc 230 4 22.8 95 Merc 280 4 19.2 123 Merc 280C 4 17.8 123 Merc 450SE 3 16.4 180 Merc 450SL 3 17.3 180 Merc 450SLC 3 15.2 180 Cadillac Fleetwood 3 10.4 205 Lincoln Continental 3 10.4 215 Chrysler Imperial 3 14.7 230 Fiat 128 4 32.4 66 Honda Civic 4 30.4 52 Toyota Corolla 4 33.9 65 Toyota Corona 3 21.5 97 Dodge Challenger 3 15.5 150 AMC Javelin 3 15.2 150 Camaro Z28 3 13.3 245 Pontiac Firebird 3 19.2 175 Fiat X1-9 4 27.3 66 Porsche 914-2 5 26.0 91 Lotus Europa 5 30.4 113 Ford Pantera L 5 15.8 264 Ferrari Dino 5 19.7 175 Maserati Bora 5 15.0 335 Volvo 142E 4 21.4 109 \u0026gt; select(mtcars, -drat) # - 符号表示删除变量/列 mpg cyl disp hp wt qsec vs am gear carb Mazda RX4 21.0 6 160.0 110 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160.0 110 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108.0 93 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258.0 110 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360.0 175 3.440 17.02 0 0 3 2 Valiant 18.1 6 225.0 105 3.460 20.22 1 0 3 1 Duster 360 14.3 8 360.0 245 3.570 15.84 0 0 3 4 Merc 240D 24.4 4 146.7 62 3.190 20.00 1 0 4 2 Merc 230 22.8 4 140.8 95 3.150 22.90 1 0 4 2 Merc 280 19.2 6 167.6 123 3.440 18.30 1 0 4 4 Merc 280C 17.8 6 167.6 123 3.440 18.90 1 0 4 4 Merc 450SE 16.4 8 275.8 180 4.070 17.40 0 0 3 3 Merc 450SL 17.3 8 275.8 180 3.730 17.60 0 0 3 3 Merc 450SLC 15.2 8 275.8 180 3.780 18.00 0 0 3 3 Cadillac Fleetwood 10.4 8 472.0 205 5.250 17.98 0 0 3 4 Lincoln Continental 10.4 8 460.0 215 5.424 17.82 0 0 3 4 Chrysler Imperial 14.7 8 440.0 230 5.345 17.42 0 0 3 4 Fiat 128 32.4 4 78.7 66 2.200 19.47 1 1 4 1 Honda Civic 30.4 4 75.7 52 1.615 18.52 1 1 4 2 Toyota Corolla 33.9 4 71.1 65 1.835 19.90 1 1 4 1 Toyota Corona 21.5 4 120.1 97 2.465 20.01 1 0 3 1 Dodge Challenger 15.5 8 318.0 150 3.520 16.87 0 0 3 2 AMC Javelin 15.2 8 304.0 150 3.435 17.30 0 0 3 2 Camaro Z28 13.3 8 350.0 245 3.840 15.41 0 0 3 4 Pontiac Firebird 19.2 8 400.0 175 3.845 17.05 0 0 3 2 Fiat X1-9 27.3 4 79.0 66 1.935 18.90 1 1 4 1 Porsche 914-2 26.0 4 120.3 91 2.140 16.70 0 1 5 2 Lotus Europa 30.4 4 95.1 113 1.513 16.90 1 1 5 2 Ford Pantera L 15.8 8 351.0 264 3.170 14.50 0 1 5 4 Ferrari Dino 19.7 6 145.0 175 2.770 15.50 0 1 5 6 Maserati Bora 15.0 8 301.0 335 3.570 14.60 0 1 5 8 Volvo 142E 21.4 4 121.0 109 2.780 18.60 1 1 4 2 slice() 函数根据行号选择特定的行。 1 2 3 4 5 \u0026gt; slice(mtcars, 1:3) mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 sample_n() 和 sample_frac() 函数选择随机行： 1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026gt; sample_n(mtcars, 5) # 选择的行数 mpg cyl disp hp drat wt qsec vs am gear carb Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 \u0026gt; sample_frac(mtcars, .1) # 选择的行数占总行数的比例 mpg cyl disp hp drat wt qsec vs am gear carb Merc 230 22.8 4 140.8 95 3.92 3.150 22.9 1 0 4 2 Merc 240D 24.4 4 146.7 62 3.69 3.190 20.0 1 0 4 2 Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.9 1 1 4 1 mutate() 函数添加新变量，这些变量是现有变量的函数。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 \u0026gt; mutate(mtcars, wt_mpg = wt * mpg) mpg cyl disp hp drat wt qsec vs am gear carb wt_mpg Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 55.0200 Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 60.3750 Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 52.8960 Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 68.8010 Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 64.3280 Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 62.6260 Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 51.0510 Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 77.8360 Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 71.8200 Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 66.0480 Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 61.2320 Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 66.7480 Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 64.5290 Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 57.4560 Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 54.6000 Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 56.4096 Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 78.5715 Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 71.2800 Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 49.0960 Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 62.2065 Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 52.9975 Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 54.5600 AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 52.2120 Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 51.0720 Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 73.8240 Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 52.8255 Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 55.6400 Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 45.9952 Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 50.0860 Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 54.5690 Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 53.5500 Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 59.4920 \u0026gt; mutate(mtcars, mpg_mean = mean(mpg), mpg_diff_mean = mpg - mpg_mean) mpg cyl disp hp drat wt qsec vs am gear carb mpg_mean mpg_diff_mean Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 20.09062 0.909375 Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 20.09062 0.909375 Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 20.09062 2.709375 Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 20.09062 1.309375 Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 20.09062 -1.390625 Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 20.09062 -1.990625 Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 20.09062 -5.790625 Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 20.09062 4.309375 Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 20.09062 2.709375 Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 20.09062 -0.890625 Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 20.09062 -2.290625 Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 20.09062 -3.690625 Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 20.09062 -2.790625 Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 20.09062 -4.890625 Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 20.09062 -9.690625 Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 20.09062 -9.690625 Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 20.09062 -5.390625 Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 20.09062 12.309375 Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 20.09062 10.309375 Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 20.09062 13.809375 Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 20.09062 1.409375 Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 20.09062 -4.590625 AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 20.09062 -4.890625 Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 20.09062 -6.790625 Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 20.09062 -0.890625 Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 20.09062 7.209375 Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 20.09062 5.909375 Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 20.09062 10.309375 Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 20.09062 -4.290625 Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 20.09062 -0.390625 Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 20.09062 -5.090625 Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 20.09062 1.309375 \u0026gt; mutate(mtcars, mpg_mean_diff = mpg - mean(mpg)) mpg cyl disp hp drat wt qsec vs am gear carb mpg_mean_diff Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 0.909375 Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 0.909375 Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 2.709375 Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 1.309375 Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 -1.390625 Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 -1.990625 Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 -5.790625 Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 4.309375 Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 2.709375 Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 -0.890625 Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 -2.290625 Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 -3.690625 Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 -2.790625 Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 -4.890625 Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 -9.690625 Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 -9.690625 Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 -5.390625 Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 12.309375 Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 10.309375 Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 13.809375 Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 1.409375 Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 -4.590625 AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 -4.890625 Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 -6.790625 Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 -0.890625 Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 7.209375 Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 5.909375 Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 10.309375 Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 -4.290625 Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 -0.390625 Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 -5.090625 Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 1.309375 2.3 Summarise cases summarise() 函数创建一个新的数据框。它将有一行总结所有观察结果，并包含每个您指定的摘要统计量的一个列。如果有分组变量，它将为每个分组变量的组合创建一行。\n1 2 3 4 5 6 7 \u0026gt; summarise(mtcars, sd(disp)) sd(disp) 1 123.9387 \u0026gt; summarise(mtcars, median(mpg), mean(mpg), max(mpg), min(mpg)) median(mpg) mean(mpg) max(mpg) min(mpg) 1 19.2 20.09062 33.9 10.4 2.4 Group cases 使用 group_by() 函数创建表的一个“分组”副本。dplyr函数将分别处理每个“组”，然后将结果组合起来。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \u0026gt; group_by(mtcars, cyl) # A tibble: 32 × 11 # Groups: cyl [3] mpg cyl disp hp drat wt qsec vs am gear carb \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; 1 21 6 160 110 3.9 2.62 16.5 0 1 4 4 2 21 6 160 110 3.9 2.88 17.0 0 1 4 4 3 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 4 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 5 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 6 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 7 14.3 8 360 245 3.21 3.57 15.8 0 0 3 4 8 24.4 4 147. 62 3.69 3.19 20 1 0 4 2 9 22.8 4 141. 95 3.92 3.15 22.9 1 0 4 2 10 19.2 6 168. 123 3.92 3.44 18.3 1 0 4 4 # ℹ 22 more rows # ℹ Use `print(n = ...)` to see more rows 2.5 管道操作符 %\u0026gt;% 允许您将一个函数的输出直接发送到下一个函数的输入。使用管道操作符时，请记住以下几点：\n第一个参数始终是一个数据框。 后续参数指定对数据框要执行的操作。 参数的顺序将返回一个新的数据框（除非您通过分配操作（如使用“\u0026lt;-”）对第一个参数中的数据框进行了更改）。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \u0026gt; mtcars %\u0026gt;% # take dataframe, then + group_by(gear) %\u0026gt;% # group it by gear, then + summarise(mean(mpg)) # summarise the mean mpg for each level o .... [TRUNCATED] # A tibble: 3 × 2 gear `mean(mpg)` \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; 1 3 16.1 2 4 24.5 3 5 21.4 \u0026gt; mtcars %\u0026gt;% + filter(mpg \u0026gt; 21) %\u0026gt;% select(gear, mpg, hp) %\u0026gt;% arrange(gear) gear mpg hp Hornet 4 Drive 3 21.4 110 Toyota Corona 3 21.5 97 Datsun 710 4 22.8 93 Merc 240D 4 24.4 62 Merc 230 4 22.8 95 Fiat 128 4 32.4 66 Honda Civic 4 30.4 52 Toyota Corolla 4 33.9 65 Fiat X1-9 4 27.3 66 Volvo 142E 4 21.4 109 Porsche 914-2 5 26.0 91 Lotus Europa 5 30.4 113 如果您想要使用这个较小版本的数据创建一个新的对象，可以通过为它指定一个新名称来实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \u0026gt; mtcars_mpg \u0026lt;- mtcars %\u0026gt;% filter(mpg \u0026gt; 21) %\u0026gt;% select(gear, mpg, hp) %\u0026gt;% arrange(gear) \u0026gt; mtcars_mpg gear mpg hp Hornet 4 Drive 3 21.4 110 Toyota Corona 3 21.5 97 Datsun 710 4 22.8 93 Merc 240D 4 24.4 62 Merc 230 4 22.8 95 Fiat 128 4 32.4 66 Honda Civic 4 30.4 52 Toyota Corolla 4 33.9 65 Fiat X1-9 4 27.3 66 Volvo 142E 4 21.4 109 Porsche 914-2 5 26.0 91 Lotus Europa 5 30.4 113 2.6 Excercise 1 2 3 4 5 6 7 8 9 10 \u0026gt; redwine \u0026lt;- read.csv(\u0026#34;Data_redwine.csv\u0026#34;, header=TRUE) \u0026gt; head(redwine) fixed.acidity volatile.acidity citric.acid residual.sugar chlorides free.sulfur.dioxide total.sulfur.dioxide density pH sulphates alcohol quality 1 7.4 0.70 0.00 1.9 0.076 11 34 0.9978 3.51 0.56 9.4 5 2 7.8 0.88 0.00 2.6 0.098 25 67 0.9968 3.20 0.68 9.8 5 3 7.8 0.76 0.04 2.3 0.092 15 54 0.9970 3.26 0.65 9.8 5 4 11.2 0.28 0.56 1.9 0.075 17 60 0.9980 3.16 0.58 9.8 6 5 7.4 0.70 0.00 1.9 0.076 11 34 0.9978 3.51 0.56 9.4 5 6 7.4 0.66 0.00 1.8 0.075 13 40 0.9978 3.51 0.56 9.4 5 创建一个仅包含固定酸度大于或等于8的葡萄酒的数据框。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026gt; wines_fixed_acidity_8_or_greater \u0026lt;- redwine %\u0026gt;% + filter(fixed.acidity \u0026gt;= 8) \u0026gt; wines_fixed_acidity_8_or_greater fixed.acidity volatile.acidity citric.acid residual.sugar chlorides free.sulfur.dioxide total.sulfur.dioxide density pH sulphates alcohol quality 1 11.2 0.280 0.56 1.9 0.075 17 60 0.99800 3.16 0.58 9.8 6 2 8.9 0.620 0.18 3.8 0.176 52 145 0.99860 3.16 0.88 9.2 5 3 8.9 0.620 0.19 3.9 0.170 51 148 0.99860 3.17 0.93 9.2 5 4 8.5 0.280 0.56 1.8 0.092 35 103 0.99690 3.30 0.75 10.5 7 5 8.1 0.560 0.28 1.7 0.368 16 56 0.99680 3.11 1.28 9.3 5 6 8.9 0.220 0.48 1.8 0.077 29 60 0.99680 3.39 0.53 9.4 6 7 8.5 0.490 0.11 2.3 0.084 9 67 0.99680 3.17 0.53 9.4 5 8 8.3 0.655 0.12 2.3 0.083 15 113 0.99660 3.17 0.66 9.8 5 3 12.0 0.380 0.56 2.1 0.093 6 24 0.99925 3.14 0.71 10.9 6 创建一个仅包含品质为7的葡萄酒的数据框。 1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026gt; # 使用filter函数筛选品质为7的葡萄酒 \u0026gt; redwine_quality_7 \u0026lt;- redwine %\u0026gt;% + filter(quality == 7) \u0026gt; # 显示新数据框的前几行 \u0026gt; head(redwine_quality_7) fixed.acidity volatile.acidity citric.acid residual.sugar chlorides free.sulfur.dioxide total.sulfur.dioxide density pH sulphates alcohol quality 1 7.3 0.65 0.00 1.2 0.065 15 21 0.9946 3.39 0.47 10.0 7 2 7.8 0.58 0.02 2.0 0.073 9 18 0.9968 3.36 0.57 9.5 7 3 8.5 0.28 0.56 1.8 0.092 35 103 0.9969 3.30 0.75 10.5 7 4 8.1 0.38 0.28 2.1 0.066 13 30 0.9968 3.23 0.73 9.7 7 5 7.5 0.52 0.16 1.9 0.085 12 35 0.9968 3.38 0.62 9.5 7 6 8.0 0.59 0.16 1.8 0.065 3 16 0.9962 3.42 0.92 10.5 7 创建一个仅包含2列（pH和品质）的数据框。 1 2 3 4 5 6 # 使用select函数选择指定的两列 redwine_selected_columns \u0026lt;- redwine %\u0026gt;% select(pH, quality) # 显示新数据框的前几行 head(redwine_selected_columns) 创建一个仅包含2列（pH和品质）的数据框，其中葡萄酒按照pH的升序排序。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026gt; # 使用select函数选择指定的两列，然后使用arrange函数按照pH的升序排序 \u0026gt; redwine_selected_and_sorted \u0026lt;- redwine %\u0026gt;% + select(pH, quality) %\u0026gt;% + arrange(pH) \u0026gt; # 显示新数据框的前几行 \u0026gt; head(redwine_selected_and_sorted) pH quality 1 2.74 4 2 2.86 6 3 2.87 6 4 2.88 8 5 2.89 5 6 2.89 5 创建一个仅包含2列（pH和品质）的数据框，其中葡萄酒按照品质的降序排序。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026gt; # 使用select函数选择指定的两列，然后使用arrange函数按照品质的降序排序 \u0026gt; redwine_selected_and_sorted \u0026lt;- redwine %\u0026gt;% + select(pH, quality) %\u0026gt;% + arrange(desc(quality)) \u0026gt; # 显示新数据框的前几行 \u0026gt; head(redwine_selected_and_sorted) pH quality 1 3.35 8 2 3.23 8 3 3.56 8 4 2.88 8 5 3.22 8 6 3.15 8 创建一个仅包含没有柠檬酸的葡萄酒的数据框，并按照品质的降序排序。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026gt; # 使用filter函数筛选没有柠檬酸的葡萄酒，然后使用arrange函数按照品质的降序排序 \u0026gt; redwine_no_citric_acid \u0026lt;- redwine %\u0026gt;% + filter(citric.acid == 0) %\u0026gt;% + arrange(desc(quality)) \u0026gt; # 显示新数据框的前几行 \u0026gt; head(redwine_no_citric_acid) fixed.acidity volatile.acidity citric.acid residual.sugar chlorides free.sulfur.dioxide total.sulfur.dioxide density pH sulphates alcohol quality 1 7.3 0.650 0 1.2 0.065 15 21 0.99460 3.39 0.47 10.0 7 2 5.1 0.585 0 1.7 0.044 14 86 0.99264 3.56 0.94 12.9 7 3 4.9 0.420 0 2.1 0.048 16 42 0.99154 3.71 0.74 14.0 7 4 5.6 0.660 0 2.2 0.087 3 11 0.99378 3.71 0.63 12.8 7 5 5.6 0.660 0 2.2 0.087 3 11 0.99378 3.71 0.63 12.8 7 6 7.8 0.645 0 2.0 0.082 8 16 0.99640 3.38 0.59 9.8 6 创建一个红葡萄酒数据集的版本，其中包含一个额外的列，该列包含酒精含量乘以密度。 1 2 3 4 5 6 # 使用mutate函数创建一个新列，该列为酒精含量乘以密度 redwine_with_extra_column \u0026lt;- redwine %\u0026gt;% mutate(alcohol_density = alcohol * density) # 显示新数据框的前几行 head(redwine_with_extra_column) 创建一个总结红葡萄酒数据集中每个变量的数据框。 1 2 3 4 5 6 7 8 9 10 11 12 \u0026gt; # 使用summary函数对红葡萄酒数据集进行总结 \u0026gt; redwine_summary \u0026lt;- summary(redwine) \u0026gt; # 显示总结结果 \u0026gt; redwine_summary fixed.acidity volatile.acidity citric.acid residual.sugar chlorides free.sulfur.dioxide total.sulfur.dioxide density pH sulphates alcohol quality Min. : 4.600 Min. :0.1200 Min. :0.0000 Min. : 1.200 Min. :0.01200 Min. : 1.00 Min. : 6.00 Min. :0.9906 Min. :2.740 Min. :0.3300 Min. : 8.40 Min. :3.000 1st Qu.: 7.400 1st Qu.:0.4000 1st Qu.:0.1200 1st Qu.: 2.000 1st Qu.:0.07200 1st Qu.: 7.00 1st Qu.: 23.00 1st Qu.:0.9964 1st Qu.:3.190 1st Qu.:0.5600 1st Qu.: 9.50 1st Qu.:5.000 Median : 8.300 Median :0.5200 Median :0.2800 Median : 2.300 Median :0.08100 Median :13.00 Median : 39.00 Median :0.9973 Median :3.300 Median :0.6200 Median : 9.90 Median :5.000 Mean : 8.729 Mean :0.5283 Mean :0.2946 Mean : 2.579 Mean :0.09037 Mean :15.17 Mean : 48.33 Mean :0.9973 Mean :3.299 Mean :0.6685 Mean :10.24 Mean :5.594 3rd Qu.: 9.800 3rd Qu.:0.6350 3rd Qu.:0.4700 3rd Qu.: 2.700 3rd Qu.:0.09300 3rd Qu.:20.25 3rd Qu.: 64.25 3rd Qu.:0.9984 3rd Qu.:3.400 3rd Qu.:0.7400 3rd Qu.:10.80 3rd Qu.:6.000 Max. :15.900 Max. :1.3300 Max. :1.0000 Max. :15.500 Max. :0.61100 Max. :68.00 Max. :165.00 Max. :1.0032 Max. :3.900 Max. :2.0000 Max. :14.90 Max. :8.000 3. Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 library(\u0026#34;readxl\u0026#34;) # used for report compilation and table display library(\u0026#34;ggplot2\u0026#34;) # confusion matrix library(\u0026#34;dplyr\u0026#34;) # confusion matrix # 获取当前已加载文件的目录 file_dir \u0026lt;- dirname(parent.frame(2)$ofile) print(file_dir) # 将工作目录设置为当前已加载文件的目录 setwd(file_dir) CLV.df \u0026lt;- read_excel(\u0026#34;Data_CLV.xlsx\u0026#34;, sheet = \u0026#34;Ex2\u0026#34;) CLV.df summary(CLV.df) ggplot(CLV.df, aes(x = t, y = active)) + geom_line() + ylab(\u0026#34;Customer\u0026#34;) + xlab(\u0026#34;Period\u0026#34;) + ggtitle(\u0026#34;Active Customer Evolution\u0026#34;) ggplot(CLV.df, aes(x = t, y = r)) + geom_line() + ylab(\u0026#34;Customer\u0026#34;) + xlab(\u0026#34;Period\u0026#34;) + ggtitle(\u0026#34;Retention Ratio Evolution\u0026#34;) CLV.df$CLV \u0026lt;- (CLV.df$p-CLV.df$c)*CLV.df$r^CLV.df$t/(1+CLV.df$i)^CLV.df$t CLV.df ggplot(CLV.df, aes(x = t, y = CLV)) + geom_line() + ggtitle(\u0026#34;CLV evolution\u0026#34;) + ylab(\u0026#34;CLV\u0026#34;) + xlab(\u0026#34;Period\u0026#34;) CLV \u0026lt;- apply(CLV.df, 2, sum) CLV[7] CLV.df$CLV2 \u0026lt;- (CLV.df$p - CLV.df$c) * 0.8 / (1 + CLV.df$i)^(CLV.df$t - 1) ggplot(CLV.df, aes(x = t, y = CLV2)) + geom_line() + ylab(\u0026#34;CLV2\u0026#34;) + xlab(\u0026#34;Period\u0026#34;) + labs(title = \u0026#34;CLV 2 Evolution\u0026#34;) CLV \u0026lt;- apply(CLV.df, 2, sum) CLV[7] CLV[8] head(mtcars) str(mtcars) filter(mtcars, gear == 4) # 过滤器可与所有标准逻辑运算符一起使用 filter(mtcars, gear == 3 | gear == 4) # 即 \u0026gt;、\u0026lt;、=\u0026gt;、\u0026lt;=、!=、== filter(mtcars, mpg \u0026gt; 21) arrange(mtcars, gear) arrange(mtcars, gear, mpg) arrange(mtcars, desc(mpg)) # desc 以降序排序 select(mtcars, gear, mpg, hp) select(mtcars, -drat) # - 符号表示删除变量/列 slice(mtcars, 1:3) sample_n(mtcars, 5) # 选择的行数 sample_frac(mtcars, .1) # 选择的行数占总行数的比例 mutate(mtcars, wt_mpg = wt * mpg) mutate(mtcars, mpg_mean = mean(mpg), mpg_diff_mean = mpg - mpg_mean) mutate(mtcars, mpg_mean_diff = mpg - mean(mpg)) summarise(mtcars, sd(disp)) summarise(mtcars, median(mpg), mean(mpg), max(mpg), min(mpg)) group_by(mtcars, cyl) mtcars %\u0026gt;% # take dataframe, then group_by(gear) %\u0026gt;% # group it by gear, then summarise(mean(mpg)) # summarise the mean mpg for each level of gear mtcars %\u0026gt;% filter(mpg \u0026gt; 21) %\u0026gt;% select(gear, mpg, hp) %\u0026gt;% arrange(gear) mtcars_mpg \u0026lt;- mtcars %\u0026gt;% filter(mpg \u0026gt; 21) %\u0026gt;% select(gear, mpg, hp) %\u0026gt;% arrange(gear) mtcars_mpg # Excercise redwine \u0026lt;- read.csv(\u0026#34;Data_redwine.csv\u0026#34;, header=TRUE) head(redwine) wines_fixed_acidity_8_or_greater \u0026lt;- redwine %\u0026gt;% filter(fixed.acidity \u0026gt;= 8) wines_fixed_acidity_8_or_greater # 使用filter函数筛选品质为7的葡萄酒 redwine_quality_7 \u0026lt;- redwine %\u0026gt;% filter(quality == 7) # 显示新数据框的前几行 head(redwine_quality_7) # 使用select函数选择指定的两列，然后使用arrange函数按照pH的升序排序 redwine_selected_and_sorted \u0026lt;- redwine %\u0026gt;% select(pH, quality) %\u0026gt;% arrange(pH) # 显示新数据框的前几行 head(redwine_selected_and_sorted) # 使用select函数选择指定的两列，然后使用arrange函数按照品质的降序排序 redwine_selected_and_sorted \u0026lt;- redwine %\u0026gt;% select(pH, quality) %\u0026gt;% arrange(desc(quality)) # 显示新数据框的前几行 head(redwine_selected_and_sorted) # 使用filter函数筛选没有柠檬酸的葡萄酒，然后使用arrange函数按照品质的降序排序 redwine_no_citric_acid \u0026lt;- redwine %\u0026gt;% filter(citric.acid == 0) %\u0026gt;% arrange(desc(quality)) # 显示新数据框的前几行 head(redwine_no_citric_acid) # 使用summary函数对红葡萄酒数据集进行总结 redwine_summary \u0026lt;- summary(redwine) # 显示总结结果 redwine_summary ","date":"2024-03-27T00:00:00Z","permalink":"https://MyLoveES.github.io/p/rweek5-customer-lifetime-value-code/","title":"R[week5] Customer Lifetime Value code"},{"content":" R: 4.3.2 (2023-10-31) R studio: 2023.12.1+402 (2023.12.1+402)\n1. CLV CLV（Customer Lifetime Value）指的是客户生命周期价值，是指客户与公司建立关系期间所获得的所有未来利润的现值。这个指标可以帮助公司评估他们对客户的投资价值，从而确定在客户获取、留存和发展方面应该投入多少资源。CLV的计算通常涉及预测客户未来的购买行为、利润和关系持续时间，并将这些数据折现到当前值，以便进行比较和决策制定。\n对于企业制定收购（Acquisition）、扩张（Expansion）、保留（Retention）决策而言的关键分析工具。以下是CLV对每个AER决策组成部分的贡献：\n收购（Acquisition）：CLV帮助企业确定他们可以投入多少成本来获得新客户并实现盈利。通过了解客户的潜在长期价值，企业可以更有效地分配收购预算，针对那些可能产生最高CLV的客户群体。\n扩张（Expansion）：CLV分析可以识别出对现有客户进行升级或交叉销售的机会。通过根据客户的CLV进行分组，企业可以优先将资源投入到那些具有更高潜力进行额外购买或升级的客户群体，从而最大化现有客户群体的收入。\n保留（Retention）：CLV可以帮助企业识别出具有高价值的客户，并采取措施来增强与这些客户的关系，以提高客户的忠诚度和保留率。通过投资于提升服务质量、个性化营销、客户服务等方面，企业可以更好地保持和发展高价值客户，从而提高整体CLV。\n2. How CLV works CLV是客户关系的美元价值，根据其现值和预测未来现金流量来确定。\n计算过程包括三个步骤： 估计剩余客户寿命，或客户可能与公司保持关系的年数，通常根据保留率进行估计。 预测客户在预期寿命内的净利润。 计算未来金额的净现值。 根据预期的长期利润或CLV（Customer Lifetime Value，客户生命周期价值）来对当前和潜在客户进行分段，利用CLV分布图来识别和针对最具利润性的客户进行营销保留活动。\nCLV分布： 不活跃客户（CLV低至负值） 活跃客户（CLV为正） 高度活跃客户（CLV非常高） 公司可以利用这样的图表来识别并针对最具利润性的客户进行营销保留活动。 考虑不同客户之间利润差异，捕捉现有客户之间的差异，以便获取、扩展和保留“最佳客户”。\nCLV考虑了客户之间利润的差异，超越了80/20法则：企业从30%的客户中赚取了150%的利润。 CLV捕捉了现有客户之间的这些差异，因此您可以获取、扩展和保留“最佳客户”。 CLV考虑了客户利润的时间变化。\n平均而言，客户的年度收益通常会随着时间增长，这是由于交叉销售/升级。 但是，一些客户的获取或保留成本较高（降低价格，提供高服务水平）。 3. CLV 公式 $$ CLV = \\sum_{t=0}^{T} \\frac{(p_t - c_t) r_t}{(1 + i)^t} - AC $$其中：\n$ p_t $ 代表第 $ t $ 年的利润（Profit） $ c_t $ 代表第 $ t $ 年的成本（Cost） $ p_t - c_t $ ：代表时间 $ t $ 时消费者的利润率（Profit margin） $ r $：代表客户的保留率，即消费者再次购买的概率（Retention rate） $ i $ 代表折现率（Interest rate） $ (1+i)^t $：代表将时间 $ t $ 的利润折现(Discounting the profit in period $ t $) $ AC $ 代表获客成本（Acquisition cost） 4. CLV Analysis 假设：\n当 $ T $ 趋近于无穷时（$ T \\rightarrow \\infty $） 贡献边际（Contribution margin）$ m = p - c $ 在时间上不变 在这种情况下，CLV公式可以简化为：\n$$ CLV = \\sum_{t=0}^{\\infty} \\frac{m \\cdot r^t}{(1 + i)^t} - AC = m \\cdot \\left( \\frac{r}{1 + i - r} \\right) - AC $$ 每年贡献边际 $m$ 保持不变 每年保留率 $r$ 保持不变 折现率 $i$ 保持不变 获客成本 $AC$ 保持不变 ","date":"2024-03-26T00:00:00Z","permalink":"https://MyLoveES.github.io/p/rweek5-customer-lifetime-value-lecture/","title":"R[week5] Customer Lifetime Value Lecture"},{"content":" R: 4.3.2 (2023-10-31) R studio: 2023.12.1+402 (2023.12.1+402)\nChoice modeling 是一种市场研究方法，用于理解消费者在购买决策中做出选择的过程。这种方法使用数学模型来分析消费者如何根据不同的产品或服务特征做出选择，并量化这些选择的概率。通常，研究者会设计实验或调查来收集关于消费者对不同产品或服务的偏好和选择的数据，然后使用选择模型来解释这些数据。\n选择模型可以是基于参数的，比如概率模型（如逻辑回归）、偏好模型（如偏好函数）或效用模型（如福利函数）等；也可以是非参数的，比如决策树、随机森林等机器学习方法。这些模型能够帮助企业了解消费者对产品或服务的偏好，从而指导产品定价、市场定位、广告策略等决策。\nMarketers often observe yes/no outcomes:\n• Did a customer purchase a product?\n• Did a customer take a test drive?\n• Did a customer sign up for a credit card, renew her subscription, or respond to a promotion?\nAll of these kinds of outcomes are binary because they have only two possible overserved states: yes or no. A logistic model is used to fit such outcomes.\n这些类型的结果都是二元的，它们只有两种可能的状态：是或否。 logistic模型被用来拟合这样的结果。\n1. Basics of logistic regression The core feature of a logistic model is that it relates the probability of an outcome to an exponential function of a predictor variable.\nBy modelling the probability of an outcome, a logistic model accomplishes two things:\n• First, it more directly models what we are interested in, which is a probability or proportion, such as the likelihood of a given customer to purchase a product or the expected proportion of a segment who will respond to a promotion.\n• Second, it limits the model to the appropriate range for a proportion, which is [0, 1]. A basic linear model, as generated with lm(), does not have such a limit. The equation for the logistic function is:\n$$ p(y) = \\frac{e^{v_x}}{e^{v_x} + 1} $$Logistic模型的核心特征是它将结果的概率与预测变量的指数函数相关联。\n通过对结果的概率建模，logistic模型实现了两个目标。\n• 首先，它更直接地对我们感兴趣的内容进行建模，即概率或比例，例如给定客户购买产品的可能性或将对促销活动做出回应的细分预期比例。\n• 其次，它将模型限制在比例的适当范围内，即[0,1]。基本的线性模型，如lm()生成的模型，没有这样的限制。\nIn this equation, the outcome of interest is y, and we compute its likelihood p(y) as a function of vx. We typically estimate vx as a function of the features (x) of a product, such as price. vx can take any real value, so we are able to treat it as a continuous function in a linear model. In that case, vx is composed of one or more coefficients of the model and indicates the importance of the corresponding features of the product.\n在这个方程中，我们感兴趣的结果是y，我们计算其概率p(y)作为vx的函数。我们通常将vx估计为产品特征（x）的函数，例如价格。vx可以取任何实数值，因此我们可以将其视为线性模型中的连续函数。在这种情况下，vx由模型的一个或多个系数组成，并指示产品相应特征的重要性。\nThe formula gives a value between [0, 1]. The likelihood of y is less than 50% when vx is negative, is 50% when vx = 0 and is above 50% when vx is positive. We compute this first by hand and then switch to the equivalent plogis() function:\n这个公式给出了一个在[0, 1]之间的值。当vx为负时，y的概率小于50％，当vx = 0时，概率为50％，当vx为正时，概率大于50％。我们首先手工计算这个值，然后切换到等效的plogis()函数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026gt; exp(0) / exp(0)+1 # computing logistic by hand, or using plogis() •[1] 2 # plogis参数其实就是p(y) \u0026gt; plogis(-Inf) #infinitely low = likelihood 0 [1] 0 \u0026gt; plogis(2) #moderate probability = 88% chance of outcome [1] 0.8807971 \u0026gt; plogis(-0.2) # weak likelihood [1] 0.450166 plogis():\nplogis() 函数是 R 语言中用于计算逻辑函数（logistic function）的函数。\n逻辑函数的定义如下所示：\n$$ \\text{logistic}(x) = \\frac{1}{1 + e^{-x}} $$其中，\\(x\\) 是逻辑函数的输入值。plogis() 函数接受一个参数 \\(x\\)，表示逻辑函数的输入值，然后返回逻辑函数的值。这个函数通常用于逻辑回归模型中，将线性预测值转换为0到1之间的概率值。\n在 R 中，你可以使用 plogis() 函数来计算逻辑函数的值。例如：\n1 2 3 4 # 计算逻辑函数值 x \u0026lt;- 2 probability \u0026lt;- plogis(x) print(probability) Such a model is known as a logit model, which determines the value of vx from the logarithm of the relative probability of occurence of y:\n$$ v_x = \\log \\left( \\frac{p(y)}{1 - p(y)} \\right) $$ 1 2 3 4 5 \u0026gt; log(0.88 / (1-0.88)) # moderate high likelihood [1] 1.99243 \u0026gt; qlogis(0.88) # equivalent to hand computation [1] 1.99243 qlogis()\nqlogis() 函数是 R 语言中用于计算逻辑函数的反函数的函数。\n逻辑函数的反函数通常称为逆逻辑函数，其定义如下所示：\n$$ \\text{logit}(p) = \\log\\left(\\frac{p}{1 - p}\\right) $$其中，\\( p \\) 是逻辑函数的输出值，即概率值。\nqlogis() 函数接受一个参数 \\( p \\)，表示逻辑函数的输出值（即概率值），然后返回逆逻辑函数的值。这个函数通常用于从逻辑函数的概率值中反推出线性预测值。\n在 R 中，你可以使用 qlogis() 函数来计算逆逻辑函数的值。例如：\n1 2 3 4 # 计算逆逻辑函数值 p \u0026lt;- 0.7 linear_pred \u0026lt;- qlogis(p) print(linear_pred) 2. Generalised linear model (GLM) A logistic regression model in R is fitted as a generalised linear model (GLM) using a process similar to linear regression with lm(), but with the difference that a GLM can handle dependent variables that are not normally distributed. Thus, GLM can be used to model data counts (such as the number of purchases), time intervals (such as time spent on a website), or binary variables (e.g., did/didn’t purchase). The common feature of all GLM models is that they relate normally distributed predictors to a non-normal outcome using a function known as a link. This means that they are able to fit models for many different distributions using a single, consistent framework.\n在R中，逻辑回归模型是作为广义线性模型（GLM）进行拟合的，使用的过程类似于使用lm()进行线性回归，但不同之处在于GLM可以处理不符合正态分布的因变量。因此，GLM可用于对数据计数（例如购买次数）、时间间隔（例如在网站上的停留时间）或二元变量（例如是否购买）建模。所有GLM模型的共同特点是它们将正态分布的预测变量与一个非正态的结果相关联，使用的函数称为链接函数。这意味着它们能够使用单一、一致的框架拟合许多不同分布的模型。\n广义线性模型（Generalized Linear Model，GLM）是一种广泛应用于统计分析中的模型，它将线性模型扩展到了更广泛的数据类型和分布。GLM可以处理不同类型的响应变量，包括二项分布、泊松分布、正态分布等，并且可以处理不同的链接函数，如恒等函数、对数函数、逻辑斯蒂函数等。\nGLM的基本形式如下：\n线性部分： $$ \\eta = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p $$这部分与多元线性回归模型相似，其中 $ \\eta $ 是线性预测值，$ \\beta_0 $ , $ \\beta_1 $ , $ \\ldots $, $ \\beta_p $ 是系数，$ x_1 $ , $ x_2 $ , $ \\ldots $, $ x_p $ 是预测变量。\n链接函数： $$ g(\\mu) = \\eta $$这里的 $ g(\\cdot) $ 是链接函数，它定义了预测变量 $ \\eta $ 与响应变量 $ \\mu $ 之间的关系。链接函数通常根据响应变量的类型选择，如对数链接函数用于处理泊松分布的响应变量，逻辑斯蒂链接函数用于处理二项分布的响应变量等。\n分布族： $$ Y \\sim \\text{Dist}(\\mu) $$这里的 $ \\text{Dist}(\\mu) $ 表示响应变量 Y 的分布族，$ \\mu $ 是响应变量的均值。\nGLM的优势在于它的灵活性和适用性，可以适应不同类型和分布的数据，同时保持了对参数的解释性。它在许多领域都得到了广泛应用，包括生物统计学、医学、社会科学等。\n3. RFM (recency, frequency, monetary) RFM is a method used for analyzing customer value. RFM stands for the three dimensions: Recency: How recently did the customer purchase? Frequency: How often do they purchase? Monetary Value: How much do they spend?\nRFM是用于分析客户价值的一种方法。RFM代表三个维度：Recency（最近购买时间）：客户最近一次购买是在多久之前？Frequency（购买频率）：他们购买的频率如何？Monetary Value（购买金额）：他们的消费金额是多少？\n3.1 The Logit Model The logit model restricts the output values to lie in [0, 1] intervals.\nSpecifically, it expresses the probability of purchase by customer i as a function of coefficients β0:3 and variables in the following manner:\n逻辑斯蒂模型将输出值限制在[0, 1]的区间内。 具体而言，它将客户i的购买概率表达为系数β0:3和以下变量的函数：\n$$ P(Purchase_i) = \\frac{exp(\\beta_0 + \\beta_1 \\text{Recency}_i + \\beta_2 \\text{Frequency}_i + \\beta_3 \\text{Monetary}_i)}{exp(\\beta_0 + \\beta_1 \\text{Recency}_i + \\beta_2 \\text{Frequency}_i + \\beta_3 \\text{Monetary}_i) + 1} $$ 这个公式是一个逻辑回归模型中用于计算购买概率的方程。在这个方程中：\n$ P(Purchase_i) $ 表示第 i 个个体购买的概率。 $ \\beta_0 $, $ \\beta_1 $, $ \\beta_2 $, $ \\beta_3 $ 是模型的参数，分别表示截距和与每个预测变量（Recency、Frequency、Monetary）相关的系数。 $ \\text{Recency}_i $, $ \\text{Frequency}_i $, $ \\text{Monetary}_i $ 是第 i 个个体的预测变量值，分别表示最近一次购买距离、购买频率和购买金额。 公式的分子部分表示了一个线性组合$ (\\beta_0 + \\beta_1 \\text{Recency}_i + \\beta_2 \\text{Frequency}_i + \\beta_3 \\text{Monetary}_i) $ 的指数形式，即指数函数 $ \\text{exp}(\\ldots) $ ，代表了购买的可能性。\n分母部分是分子部分加上1，这是由于逻辑回归模型的形式，保证了概率值在0和1之间。整个方程实际上是逻辑回归模型的逻辑函数（logistic function），它将线性预测值转换为0到1之间的概率值，这表示个体购买的概率。\nIntuitively, the utility of choosing to buy is:\n$$ V_{bi} = \\beta_0 + \\beta_1 \\text{Recency}_i + \\beta_2 \\text{Frequency}_i + \\beta_3 \\text{Monetary}_i $$ 这个公式表示了一个线性模型，用于预测个体 i 的 $ V $ 值。在这个公式中：\n$ V_{bi} $ 表示个体 $ i $ 的 $ V $ 值。 $ \\beta_0, \\beta_1, \\beta_2, \\beta_3 $ 是模型的参数，分别表示截距和与每个预测变量（Recency、Frequency、Monetary）相关的系数。 $ \\text{Recency}_i, \\text{Frequency}_i, \\text{Monetary}_i $ 第 $ i $ 个个体的预测变量值，分别表示最近一次购买距离、购买频率和购买金额。 这个模型的目的是通过个体的购买行为的相关特征（Recency、Frequency、Monetary）来预测他们的 $ V $ 值。这个 $ V $ 值可能表示个体的潜在价值或其他相关的指标。\nwhereas utility of choosing not to buy is normalized to zero $ V_ni = 0 $, so $ exp(V_n) = exp(0) = 1 $ in the fraction above.\nWith the given formulation, we can estimate values $ \\beta_0:3 $ that fit the data best. We use glm() of family=“binomial”.\n选择不购买的效用被归一化为零，即 Vni = 0，因此在上述分数中 exp(Vn) = exp(0) = 1。\n通过给定的公式，我们可以估计最适合数据的 β0:3 值。我们使用 glm() 中的 family=\u0026ldquo;binomial\u0026rdquo;。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 \u0026gt; RFMdata \u0026lt;- read.csv(file = \u0026#34;RFMData.csv\u0026#34;,row.names=1) \u0026gt; head(RFMdata,5) Recency Frequency Monetary Purchase 1 120 7 41.66 0 2 90 9 46.71 0 3 120 6 103.99 1 4 270 17 37.13 1 5 60 5 88.92 0 \u0026gt; model \u0026lt;- glm(Purchase~Recency+Frequency+Monetary, data=RFMdata, family = \u0026#34;binomial\u0026#34;) \u0026gt; output \u0026lt;- cbind(coef(summary(model))[, 1:4],exp(coef(model))) \u0026gt; colnames(output) \u0026lt;- c(\u0026#34;beta\u0026#34;,\u0026#34;SE\u0026#34;,\u0026#34;z val.\u0026#34;,\u0026#34;Pr(\u0026gt;|z|)\u0026#34;,\u0026#39;exp(beta)\u0026#39;) \u0026gt; kable(output,caption = \u0026#34;Logistic regression estimates\u0026#34;) Table: Logistic regression estimates | | beta| SE| z val.| Pr(\u0026gt;\u0026amp;#124;z\u0026amp;#124;)| exp(beta)| |:-----------|-----------:|---------:|---------:|------------------:|---------:| |(Intercept) | -30.2976692| 8.5522913| -3.542638| 0.0003961| 0.000000| |Recency | 0.1114175| 0.0309797| 3.596464| 0.0003226| 1.117862| |Frequency | 0.5941268| 0.2429393| 2.445577| 0.0144620| 1.811448| |Monetary | 0.1677054| 0.0465645| 3.601572| 0.0003163| 1.182588| glm()\nglm() 函数是 R 语言中的一个重要函数，用于拟合广义线性模型（Generalized Linear Models）。广义线性模型是线性模型的扩展，允许因变量服从不同的分布，而不仅仅是正态分布。这使得广义线性模型适用于更广泛的数据类型，包括二项分布（二元逻辑回归）、泊松分布（计数数据）、多项分布（多类别分类）等。 cbind()\ncbind() 函数是 R 语言中的一个基础函数，用于按列合并多个对象（通常是向量、矩阵或数据框）。cbind 是 \"column bind\" 的缩写，表示按列合并。 kable()\nkable() 函数是 R 语言中 knitr 和 rmarkdown 包中的一个函数，用于生成美观的表格输出。它能够将 R 中的数据框、矩阵或表格转换为 Markdown 或 LaTeX 格式的表格，从而方便地将其插入到 R Markdown 文档或 HTML 页面中。 We also run the likelihood ratio test with H0 : β1 = β2 = β3 = 0 – to make sure our full logit model offers a significantly better fit than the model with just an intercept. We find that χ2 = 107.14 and P(\u0026gt; |Chi|) ≈ 0, so we reject H0.\n我们还进行了似然比检验，假设 H0：β1 = β2 = β3 = 0，以确保我们的完整 logit 模型提供了显着更好的拟合效果，而不仅仅是一个截距模型。我们发现 χ2 = 107.14，P(\u0026gt; |Chi|) ≈ 0，因此我们拒绝 H0。\n这句话表明了对全模型（含有Recency、Frequency、Monetary预测变量）和只有截距项的模型之间进行了似然比检验。在似然比检验中，原假设 $ H_0 $ 是模型中所有预测变量的系数都为零，即 $ \\beta_1 = \\beta_2 = \\beta_3 = 0 $，即只有截距项。备择假设 $ H_1 $ 是至少有一个预测变量的系数不为零，即全模型。\n通过似然比检验，可以确定是否全模型相对于只有截距项的模型提供了更好的拟合。在这种情况下，通过计算得到的卡方统计量 $ \\chi^2 $ 为107.14，对应的P值非常接近于0，通常小于显著性水平（例如0.05）。由于P值小于显著性水平，我们拒绝原假设 $ H_0 $，即认为全模型的拟合效果显著地优于只有截距项的模型。\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026gt; # likelihood ratio test \u0026gt; reduced.model \u0026lt;- glm(Purchase ~ 1, data=RFMdata, family = \u0026#34;binomial\u0026#34;) \u0026gt; kable(xtable(anova(reduced.model, model, test = \u0026#34;Chisq\u0026#34;)),caption = \u0026#34;Likelihood ratio test\u0026#34;) Table: Likelihood ratio test | Resid. Df| Resid. Dev| Df| Deviance| Pr(\u0026gt;Chi)| |---------:|----------:|--:|--------:|--------:| | 99| 137.62776| NA| NA| NA| | 96| 30.48715| 3| 107.1406| 0| 3.2 Predicting probabilities Now we calculate $ P(Purchase_i) $ for each individual in the data set.\n现在我们计算数据集中每个个体的购买概率$ P(Purchase_i) $。\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026gt; # calculate logit probabilities \u0026gt; RFMdata$Base.Probability \u0026lt;- predict(model, RFMdata, type=\u0026#34;response\u0026#34;) \u0026gt; kable(head(RFMdata,5),row.names = TRUE) | | Recency| Frequency| Monetary| Purchase| Base.Probability| |:--|-------:|---------:|--------:|--------:|----------------:| |1 | 120| 7| 41.66| 0| 0.0030728| |2 | 90| 9| 46.71| 0| 0.0008332| |3 | 120| 6| 103.99| 1| 0.9833225| |4 | 270| 17| 37.13| 1| 0.9999999| |5 | 60| 5| 88.92| 0| 0.0032378| predict()\npredict() 函数是 R 语言中的一个常用函数，用于对已拟合的模型进行预测。它可以对新的观测数据应用已经拟合好的模型，从而生成预测值。 3.3 Predicting behaviour We also calculate an indicator variable for whether individuals will purchase or not based on their predicted probabilities\n我们还根据他们的预测概率计算出个体是否会购买的指示变量。\n符号 \u0026ldquo;⊮\u0026rdquo; 代表逻辑非（negation）或者“非”。所以整个公式的含义是：购买概率不大于或等于0.5。\n$$ \\neg [P(Purchase_i) \\geq 0.5] $$If individual’s predicted probability is greater or equal to 0.5, we predict he will make a purchase.\n如果个体的预测概率大于或等于0.5，则我们预测他会购买。\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026gt; # purchase vs. no purchase \u0026lt;-\u0026gt; p\u0026gt;0.5 or p\u0026lt;0.5 \u0026gt; RFMdata$Predicted.Purchase \u0026lt;- 1*(RFMdata$Base.Probability\u0026gt;=0.5) \u0026gt; kable(head(RFMdata,5),row.names = TRUE) | | Recency| Frequency| Monetary| Purchase| Base.Probability| Predicted.Purchase| |:--|-------:|---------:|--------:|--------:|----------------:|------------------:| |1 | 120| 7| 41.66| 0| 0.0030728| 0| |2 | 90| 9| 46.71| 0| 0.0008332| 0| |3 | 120| 6| 103.99| 1| 0.9833225| 1| |4 | 270| 17| 37.13| 1| 0.9999999| 1| |5 | 60| 5| 88.92| 0| 0.0032378| 0| 3.4 Evaluating the model Now, we compute a confusion matrix between predicted purchases and actual purchase behaviour.\n现在，我们计算预测购买和实际购买行为之间的混淆矩阵。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 \u0026gt; confusionMatrix(table(RFMdata$Predicted.Purchase,RFMdata$Purchase),positive = \u0026#34;1\u0026#34;) Confusion Matrix and Statistics 0 1 0 51 2 1 4 43 Accuracy : 0.94 95% CI : (0.874, 0.9777) No Information Rate : 0.55 P-Value [Acc \u0026gt; NIR] : \u0026lt;2e-16 Kappa : 0.8793 Now we calculate Mcnemar\u0026#39;s Test P-Value : 0.6831 Sensitivity : 0.9556 Specificity : 0.9273 Pos Pred Value : 0.9149 Neg Pred Value : 0.9623 Prevalence : 0.4500 Detection Rate : 0.4300 Detection Prevalence : 0.4700 Balanced Accuracy : 0.9414 \u0026#39;Positive\u0026#39; Class : 1 confusionMatrix()\nconfusionMatrix() 函数是 caret 包中的一个函数，用于计算分类模型的混淆矩阵以及各种分类指标，如准确率、灵敏度、特异性等。混淆矩阵是一种用于评估分类模型性能的表格，其中行表示真实类别，列表示预测类别。 在市场营销场景中，confusionMatrix() 函数可以用于评估分类模型在预测客户行为方面的性能。以下是一些实际应用场景：\n• 客户流失预测：假设你正在开展客户流失预测项目。你可以使用历史数据训练一个流失预测模型，并使用 confusionMatrix() 函数来评估该模型的性能。通过混淆矩阵，你可以了解模型正确预测流失客户和未流失客户的情况，并计算出准确率、灵敏度等指标。\n• 营销活动反馈：如果你进行了一项市场营销活动，例如发送电子邮件或短信营销活动，你可以使用 confusionMatrix() 函数来评估活动的效果。你可以将实际活动的结果与预期结果进行比较，以了解活动的成功率以及是否有必要调整你的营销策略。\n• 客户分群：在客户分群项目中，你可能使用聚类算法将客户分成不同的群体。你可以使用 confusionMatrix() 函数来评估模型的性能，例如比较聚类结果与真实的客户特征之间的匹配程度。这可以帮助你确定是否需要重新调整分群方法或者改进数据质量。\n• 产品推荐系统：如果你正在开发一个产品推荐系统，你可以使用 confusionMatrix() 函数来评估系统的准确性。你可以比较系统推荐的产品与用户实际购买的产品之间的匹配情况，并计算出准确率、召回率等指标，以了解系统的性能和用户的满意度。\nWe can also plot the receiver operating characteristic (ROC) curve, which illustrates the diagnostic ability of a binary logit model. It is created by plotting the true positive rate (TPR) against the false positive rate (FPR) – at various decision threshold values for prediction.\nROC curve can be quickly evaluated using the area under the curve (AUC) metric, which captures the overall quality of the classifier. The greater the AUC, the better. AUC of 1.0 represents a perfect classifier, AUC of 0.5 (diagonal line) represents a worthless classifier. As we see, the binary logit classifier does a good job of predicting purchases on the training data.\n我们还可以绘制接收者操作特征曲线（ROC曲线），它展示了二元Logit模型的诊断能力。ROC曲线通过在不同的预测决策阈值下绘制真正率（TPR）与假正率（FPR）之间的关系来创建。\nROC曲线可以通过曲线下面积（AUC）指标进行快速评估，该指标捕捉了分类器的整体质量。AUC越大，分类器的性能越好。AUC为1.0表示完美的分类器，AUC为0.5（对角线）表示一个毫无价值的分类器。正如我们所看到的，二元Logit分类器在训练数据上预测购买行为的效果良好。\n1 2 3 4 5 6 \u0026gt; rocobj \u0026lt;- roc(RFMdata$Purchase, RFMdata$Base.Probability) Setting levels: control = 0, case = 1 Setting direction: controls \u0026lt; cases \u0026gt; {plot(rocobj,legacy.axes=TRUE) text(0.5, 0.8, labels = sprintf(\u0026#34;AUC = %.5f\u0026#34;,rocobj$auc))} roc()\nroc() 函数是 pROC 包中的一个函数，用于计算接收者操作特征（Receiver Operating Characteristic，ROC）曲线以及计算曲线下面积（Area Under the Curve，AUC）。ROC 曲线是一种用于评估二元分类器性能的图形工具，它显示了在不同分类阈值下真正例率（True Positive Rate，TPR，又称为灵敏度）与假正例率（False Positive Rate，FPR）之间的关系。 {% asset_image week4_plot.png %}\nFinally, we predict new probabilities under a hypothetical scenario that everyone’s Monetary variable went up by one unit.\n$$ V_{\\text{new}} = \\beta_0 + \\beta_1 \\text{Recency} + \\beta_2 \\text{Frequency} + \\beta_3 (\\text{Monetary} + 1) $$ 1 2 3 4 5 6 \u0026gt; # calculate new logit probabilities (Monetary+1) \u0026gt; RFMdata_new \u0026lt;- RFMdata \u0026gt; RFMdata_new$Monetary \u0026lt;- RFMdata_new$Monetary + 1 \u0026gt; RFMdata$New.Probability \u0026lt;- predict(model, RFMdata_new, type=\u0026#34;response\u0026#34;) We compare mean new probability across individuals to the mean of old probabilities, and also calculate the lift metric.\n我们比较各个个体的新概率均值与旧概率均值，并计算提升度量。\n$$ P(Purchase_i) = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\exp(V_{bi})}{\\exp(V_{bi}) + 1} $$$$ P(Purchase_{\\text{new}}) = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\exp(V_{\\text{new}})}{\\exp(V_{\\text{new}}) + 1} $$$$ \\text{Lift} = \\frac{p_{\\text{new}} - p_{\\text{old}}}{p_{\\text{old}}} $$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \u0026gt; # mean predicted base probability \u0026gt; mean(RFMdata$Base.Probability) [1] 0.45 \u0026gt; # mean new predicted probability \u0026gt; mean(RFMdata$New.Probability) [1] 0.4578851 \u0026gt; # lift \u0026gt; (mean(RFMdata$New.Probability) - mean(RFMdata$Base.Probability))/mean(RFMdata$Base.Probability) [1] 0.01752255 \u0026gt; # remove predicted purchase variable \u0026gt; RFMdata$Predicted.Purchase \u0026lt;- NULL \u0026gt; # data \u0026gt; kable(head(RFMdata,5),row.names = TRUE) | | Recency| Frequency| Monetary| Purchase| Base.Probability| New.Probability| |:--|-------:|---------:|--------:|--------:|----------------:|---------------:| |1 | 120| 7| 41.66| 0| 0.0030728| 0.0036319| |2 | 90| 9| 46.71| 0| 0.0008332| 0.0009852| |3 | 120| 6| 103.99| 1| 0.9833225| 0.9858611| |4 | 270| 17| 37.13| 1| 0.9999999| 0.9999999| |5 | 60| 5| 88.92| 0| 0.0032378| 0.0038267| 4. Recap • Logistic regression is a powerful method and a particularly good fit for many marketing problems with binary outcomes. We will cover the choice model later for modelling product choice among sets of alternatives.\n• Logistic regression relates a binary outcome such as purchase to predictors that may include continuous and factor variable by modelling the variable’s association with the probability of the outcome.\nAlthough we performed logistic regression here with categorical predictors (factor variables) due to the structure of the amusement park sales data, we could also use continuous predictors in glm(). Just add those to the right-hand side of the model formula as we did with lm()\n• A logistic regression model, also known as a logit model, is a member of the generalized linear model family and is fit using glm( , family = binomial).\n• Coefficient in a logit model can be interpreted in terms of odds ratios, the degree to which they are associated with the increased or decreased likelihood of an outcome. This is done simply by exponentiating the coefficients with exp().\n• A statistically significant result does not always mean that the model is appropriate. It is important to explore data thoroughly and construct models on the basis of careful consideration.\nWe saw that the estimated effect of promotion was positive when we estimated one model yet negative when we estimated another. This shows that it is crucial to explore data thoroughly before modelling or interpreting a model. For most marketing data, no model is ever definitive. However, through careful data exploration and consideration of multiple models, we may increase our confidence in our models and the inferences drawn from them.\n• 逻辑回归是一种强大的方法，特别适用于许多具有二元结果的营销问题。我们稍后将介绍选择模型，用于对一组替代品中的产品选择进行建模。\n• 逻辑回归将二元结果（如购买）与可能包括连续和因子变量的预测变量关联起来，方法是通过模拟变量与结果的概率之间的关联。\n尽管我们在这里使用了因子变量（分类变量）执行逻辑回归，因为娱乐园销售数据的结构如此，但我们也可以在glm()中使用连续预测变量。只需像我们在lm()中那样将它们添加到模型公式的右侧即可。\n• 逻辑回归模型，也称为logit模型，是广义线性模型家族的一员，使用glm()拟合，家族设置为binomial。\n• 在logit模型中，系数可以通过将其指数化为exp()来解释为几率比，即它们与结果的增加或减少可能性的相关程度。\n• 统计显著结果并不总意味着模型是适当的。在建模或解释模型之前，彻底探索数据并基于深思熟虑构建模型是非常重要的。\n我们看到，当我们估计一个模型时，促销的估计效果是正向的，但当我们估计另一个模型时，效果是负向的。这表明，在建模或解释模型之前，彻底探索数据是至关重要的。对于大多数营销数据，没有一个模型是绝对的。然而，通过对数据进行深入探索并考虑多个模型，我们可能会增加对模型及其推断的信心。\n","date":"2024-03-25T00:00:00Z","permalink":"https://MyLoveES.github.io/p/rweek4-choice-model-code/","title":"R[week4] Choice model code"},{"content":" R: 4.3.2 (2023-10-31) R studio: 2023.12.1+402 (2023.12.1+402)\n了解为什么有效的营销策略必须管理客户动态：这部分强调了管理客户动态对于实现有效营销策略的重要性。 批判性地解释管理客户动态的主要方法：这部分详细解释了两种主要方法，即动态客户细分方法和客户生命周期价值方法。动态客户细分方法指的是将客户按照不同的动态特征进行分类，以便更好地满足其需求。客户生命周期价值方法则是通过分析客户在整个生命周期内的价值来指导营销策略。 使用分析工具：这部分提到了使用选择模型和客户生命周期价值分析等分析工具来支持上述方法的实施。 1. Customer Dynamics 客户对大多数产品和服务的需求随时间变化或由于特定事件而变化。\n客户偏好随时间变化：因为个体消费者的需求随着年龄、经验或特定事件而改变，也可能是因为客户所处的行业或市场随着时间的推移发生了变化。 客户的需求变化不仅是由于人们之间的固有差异：还由于人们和市场的变化而引起。因此，我们需要根据客户的时间依赖性需求，而不是仅仅基于“通用”需求对所有客户进行“静态”的细分。 因此，客户动态是所有企业在制定有效营销策略时必须解决的基本“问题”。客户会发生变化；如果不能理解和解决这些动态，就会导致业务绩效不佳。\n1.1 生命周期方法 使用通用的客户成长阶段及其在生命周期中的位置来确定客户偏好和相关策略。\nCustomer lifecycle Product lifecycle Industry lifecycle 优点：\n简单易懂：将所有客户都视为遵循同一曲线。 易于使用：对所有客户进行平均处理。 使用通用的客户生命周期阶段及其在生命周期中的位置来确定客户偏好和相关策略。 缺点：\n忽略客户动态的原因。 仅适用于以离散阶段进行划分的情况，不适用于连续变化的情况。 1.2 动态客户细分 根据客户类似的获取、拓展和保留阶段对公司现有客户进行分段。\n选择模型可以在所有获取、拓展和保留阶段进行分析，因为它预测了观察到的客户选择的可能性。 优点：\n结合了生命周期和细分方法。 符合战略营销思维。 将持续变化转化为离散阶段。 缺点：\n细分可能不是完全同质的。 无法精确匹配所有客户的特点。 1.3 客户生命周期价值 捕捉客户根据其预期迁移路径在整个与公司的生命周期中的贡献。\n客户生命周期价值（CLV）分析使用折现现金流，并提供所有信息以做出最佳的获取、拓展和保留决策。 优点：\n为AER（Acquisition-Expansion-Retention）决策提供洞察。 支持客户为中心的文化。 捕捉了客户根据其预期迁移路径在与企业的整个生命周期中的贡献。 缺点：\n需要对未来迁移有深入的了解。 需要详细的财务数据。 忽略了动态和异质性。 2. Choice Model Choice Model 试图确定不同因素（价格、促销）对消费者个体选择（加入、交叉购买、离开）的影响。它是最流行的个体级响应模型。\n输入： 使用过去营销行动和与实际客户响应（选择）相关联的人口统计数据的数据库。 使用过去的行为数据；无需调查或获取客户输入（从过去客户的行为推断权重）。 输出 每个输入变量对结果的系数估计（例如，年龄、子女、信用和直邮对选择的影响） 客户选择的概率（升级销售、保留的概率，并可针对获取目标运行列表）\n• 因变量： 购买（是/否） 2.1 Generalised linear models (GLM) 广义线性模型 是一种统计模型，可以用于分析具有非正态误差结构的数据，例如二项分布、泊松分布等。GLM 是线性模型的一种扩展，它允许因变量和自变量之间的关系不是线性的，同时也不要求因变量服从正态分布。\nLinear regression：线性回归，是一种用于建立因变量和一个或多个自变量之间的线性关系的统计方法。它假设因变量和自变量之间的关系是线性的，并尝试拟合一条直线来描述这种关系。\nLogistic regression：逻辑回归，是一种用于建立因变量与一个或多个自变量之间的非线性关系的统计方法。逻辑回归用于分析因变量是二元变量的情况，它通过将线性回归模型的结果转换成概率来进行分类。\nMultinomial logistic regression：多项式逻辑回归，是逻辑回归的一种扩展，适用于因变量有多个类别的情况。与二元逻辑回归不同，多项式逻辑回归可以处理多类别的因变量，并提供各类别的概率。\n2.2 Binary choice: logistic regression Response to marketing efforts：营销活动的反应。指客户在接收到营销活动后的反应，比如使用优惠券或电子邮件广告后是否购买了产品。\nDid the customer buy after being sent a coupon or an email ad?：客户在收到优惠券或电子邮件广告后是否购买了产品。 Online/Catalogue purchase (Buy/No-Buy)：在线或目录购买。指客户在网上或通过目录购买产品的行为，以及是否购买了产品。\nRecency, Frequency, Monetary value (RFM) measures as predictors of purchase：最近购买时间、购买频率和购买金额（RFM）作为购买预测的指标。这些是用来预测客户是否会购买产品的指标。 Spend = ᵝ1Frequency + β0：这是线性回归模型的表达式，其中Spend是因变量（要预测的变量），Frequency是自变量（用来预测的变量），ᵝ1是Frequency的系数（也称为斜率），β0是截距。\nThe line slope: the effect of frequency on money spend：线的斜率表示Frequency对花费的影响，即每单位Frequency的变化导致花费的变化量。\nThe intercept: the money spend when frequency is 0：截距表示当Frequency为0时的花费，即在Frequency为0时的基础花费。\nLinear regression uses the data to estimate ᵝ1 and ᵝ0：线性回归使用数据来估计ᵝ1和ᵝ0。\nSpend = 0.5 Frequency + 0.6：这是一个线性回归模型的具体示例，其中斜率为0.5，截距为0.6。\nThen using this regression model, we can predict the money spend given a frequency：然后，使用这个回归模型，我们可以根据给定的频率来预测花费。\n2.2.1 公式 odd：购买概率 / 不购买的概率 $$ \\text{odds} = \\frac{Probability of purchase}{Probability of no-purchase} = \\frac{p}{1-p} $$ qlogis() $$ \\log (\\frac{p}{1-p}) = \\log(\\text{odds}) $$ p(plogis)：购买概率 $$ p = \\frac{e^{\\log(\\text{odds})}}{1 + e^{\\log(\\text{odds})}} $$ odd / p 转换 $$ p = \\beta_1 \\text{frequency} + \\beta_0 $$ $$ \\log(\\text{odds}) = \\beta_1 \\text{frequency} + \\beta_0 $$2.2.2 Odds and Choice Probability Utility\n$$ V_b = \\log(\\frac{p}{1-p}) $$ Odds\n$$ \\text{odds} = \\exp(V_b) = \\frac{p}{1-p} $$ Probability\n$$ p = \\frac{\\exp(V_b)}{\\exp(V_b) + 1} $$$$ Utility from not buying: V_n = 0 $$$$ Odds of buying: \\exp(2) = e^2 = 7.39 $$$$ Odds of not buying: \\exp(0) = 1 $$$$ Probability of buying: p = \\frac{\\exp(2)}{\\exp(2) + 1} = \\frac{7.39}{7.39 + 1} = \\frac{7.39}{8.39} = 0.88 $$ 模型表明，消费者对购买和不购买（保留钱）都有一定的效用：\n购买的效用：$V_b$ 不购买的效用：$V_n=0$ 如果$V_b \u0026gt; V_n=0$，消费者就会购买。 对于RFM数据，购买的效用因RFM变量而异：\n$V_b = \\beta_0 + \\beta_1 \\text{Recency} + \\beta_2 \\text{Frequency} + \\beta_3 \\text{Monetary}$ Logistic回归使用数据来估计模型参数（β系数）。\n3. Example 3.1 数据 因变量：\n购买（是/否） 解释变量：\n最近购买距今多少天 消费者购买次数 总购买金额 {% asset_image week4_lec_1.png %}\n3.2 logistic output {% asset_image week4_lec_2.png %}\nRedis.Df：重新参数化的自由度，表示模型中参数的数量。 Redis.Dev：重新参数化的偏差，表示模型与观察数据之间的偏差度量。 Df Deviance：偏差的自由度，表示模型对观察数据的拟合程度。 Pr(\u0026gt;Chi)：卡方检验的p值，用于检验模型的拟合优度。它表示给定假设条件下，观察到的偏差和期望偏差之间的差异程度，从而判断- 模型是否对观察数据拟合良好。通常，p值越小，表示模型拟合得越好。 {% asset_image week4_lec_3.png %}\nintercept：在回归分析中，拟合的模型通常包括截距项(intercept)，它代表了在所有自变量都为零时，因变量的预期值。这是模型中的一个重要部分，它确保了模型的灵活性，并使模型能够更好地拟合数据。在回归结果的输出中，\u0026quot;(intercept)\u0026ldquo;表示截距项的估计值和相关的统计信息。 beta：模型参数的估计值，表示自变量对因变量的影响程度。 SE：标准误差，表示估计的参数值与真实参数值之间的差异的标准偏差。它衡量了估计参数的不确定性。 z val.：z值，表示模型参数估计值与零假设之间的偏差量（以标准误差为单位）。z值越大，表示参数估计值与零假设之间的偏差越大。 Pr(\u0026gt;|z|)：z检验的p值，用于检验模型参数的显著性。它表示模型参数是否显著不同于零假设。通常，p值小于0.05被认为是统计上显著的。 exp(beta)：参数的指数，表示自变量单位变化对因变量的影响的倍数。通常用于解释模型中的自变量对因变量的影响程度。 更一般地，每增加1美元的购买金额，购买的几率会增加1.183倍。\n考虑两个消费者（1和2），他们的最近购买距今多少天和购买次数相同，但消费者1比消费者2的购买金额多1美元。\n那么，消费者1购买的几率将比消费者2购买的几率高出1.183倍。 在 RFM 数据中估计的效用函数为： $$ V = -30.29 + 0.111 \\times \\text{Recency} + 0.594 \\times \\text{Frequency} + 0.168 \\times \\text{Monetary} $$ 预测购买概率为： $$ p = \\frac{\\exp(V)}{\\exp(V) + 1} $$ 3.3 Lift Calculation 增加1美元购买金额对购买概率的影响： 计算购买新的效用值： $$ V_{\\text{new}} = -30.29 + 0.111 \\times \\text{Recency} + 0.594 \\times \\text{Frequency} + 0.168 \\times (\\text{Monetary} + 1) $$ 计算新的购买概率： $$ p_{\\text{new}} = \\frac{\\exp(V_{\\text{new}})}{\\exp(V_{\\text{new}}) + 1} $$ 提升： $$ \\text{Lift}_{\\text{new}} = \\frac{\\exp(V_{\\text{new}})}{\\exp(V_{\\text{base}}) + 1} - p_{\\text{base}} $$ 其中，$ p_{\\text{base}} $ 是基准购买概率。 {% asset_image week4_lec_4.png %}\n3.4 Classification {% asset_image week4_lec_5.png %}\n3.5 Classification (Hit Rate) {% asset_image week4_lec_6.png %}\n准确率：命中率=(51+43)/100=94% 灵敏度：真正率=43/(43+2)=96% 特异度：真负率=51/(51+4)=93% 假阳率=1-93%=7% 准确率（Accuracy）：分类正确的样本数与总样本数之比。它表示模型正确预测的样本比例。\n灵敏度（Sensitivity，也称为真正率或召回率）：真正例（正类样本）中被正确预测为正类的比例。它衡量了模型在正类样本中的识别能力。\n特异度（Specificity，也称为真负率）：真负例（负类样本）中被正确预测为负类的比例。它衡量了模型在负类样本中的识别能力。\n假阳率（False Positive Rate）：负类样本中被错误预测为正类的比例。它是特异度的补数，表示模型在负类样本中的错误识别率。\n这些指标通常用于评估二分类模型（正类和负类）。在实际应用中，选择哪个指标取决于具体的业务需求和分类任务的重要性。\n4. ROC (Receiver Operating Characteristic) Curve 这些是用于解释ROC曲线的AUC（曲线下面积）值的常见阈值：\n大于0.9：优秀 0.8-0.9：良好 0.7-0.8：尚可 0.6-0.7：较差 ROC曲线是衡量二分类模型性能的重要工具，AUC值是ROC曲线下的面积，用于量化模型对正例和负例的区分能力。通常情况下，AUC值越高，模型性能越好。\n{% asset_image week4_lec_7.png %}\n5. Dynamic Segmentation 根据概率尺度从高到低对客户进行排名。针对以下客户：\n属于前 X% 的顶尖客户（客户管理/资源分配） 概率高于某个阈值的客户（良好前景） 概率低于某个阈值的客户（即将“失去”的客户，营销仪表板） 这些方法可以帮助公司根据客户的概率分数进行优先级排序，并根据业务需求采取相应的行动。 6. Takeaway 选择模型是一种数学模型，用于预测观察到的客户选择的可能性，其受到公司营销的影响。选择建模在市场研究中非常普遍，用于理解各种消费者决策。\n选择分析的两种常见方法有：\n二项式逻辑回归用于二元选择 多项式逻辑模型用于选择集中超过两个备选项的情况。 ","date":"2024-03-24T00:00:00Z","permalink":"https://MyLoveES.github.io/p/rweek4-choice-model-lecture/","title":"R[week4] Choice model lecture"},{"content":" R: 4.3.2 (2023-10-31) R studio: 2023.12.1+402 (2023.12.1+402)\n1. Brand rating data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 \u0026gt; brand.ratings \u0026lt;- read.csv(\u0026#34;Data_Factor_Analysis.csv\u0026#34;, stringsAsFactors = TRUE) \u0026gt; head(brand.ratings) perform leader latest fun serious bargain value trendy rebuy brand 1 2 4 8 8 2 9 7 4 6 a 2 1 1 4 7 1 1 1 2 2 a 3 2 3 5 9 2 9 5 1 6 a 4 1 6 10 8 3 4 5 2 1 a 5 1 1 5 8 1 9 9 1 1 a 6 2 8 9 5 3 8 7 1 2 a \u0026gt; summary(brand.ratings) perform leader latest fun serious bargain value trendy rebuy brand Min. : 1.000 Min. : 1.000 Min. : 1.000 Min. : 1.000 Min. : 1.000 Min. : 1.000 Min. : 1.000 Min. : 1.00 Min. : 1.000 a :100 1st Qu.: 1.000 1st Qu.: 2.000 1st Qu.: 4.000 1st Qu.: 4.000 1st Qu.: 2.000 1st Qu.: 2.000 1st Qu.: 2.000 1st Qu.: 3.00 1st Qu.: 1.000 b :100 Median : 4.000 Median : 4.000 Median : 7.000 Median : 6.000 Median : 4.000 Median : 4.000 Median : 4.000 Median : 5.00 Median : 3.000 c :100 Mean : 4.488 Mean : 4.417 Mean : 6.195 Mean : 6.068 Mean : 4.323 Mean : 4.259 Mean : 4.337 Mean : 5.22 Mean : 3.727 d :100 3rd Qu.: 7.000 3rd Qu.: 6.000 3rd Qu.: 9.000 3rd Qu.: 8.000 3rd Qu.: 6.000 3rd Qu.: 6.000 3rd Qu.: 6.000 3rd Qu.: 7.00 3rd Qu.: 5.000 e :100 Max. :10.000 Max. :10.000 Max. :10.000 Max. :10.000 Max. :10.000 Max. :10.000 Max. :10.000 Max. :10.00 Max. :10.000 f :100 (Other):400 \u0026gt; str(brand.ratings) \u0026#39;data.frame\u0026#39;:\t1000 obs. of 10 variables: $ perform: int 2 1 2 1 1 2 1 2 2 3 ... $ leader : int 4 1 3 6 1 8 1 1 1 1 ... $ latest : int 8 4 5 10 5 9 5 7 8 9 ... $ fun : int 8 7 9 8 8 5 7 5 10 8 ... $ serious: int 2 1 2 3 1 3 1 2 1 1 ... $ bargain: int 9 1 9 4 9 8 5 8 7 3 ... $ value : int 7 1 5 5 9 7 1 7 7 3 ... $ trendy : int 4 2 1 2 1 1 1 7 5 4 ... $ rebuy : int 6 2 6 1 1 2 1 1 1 1 ... $ brand : Factor w/ 10 levels \u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;c\u0026#34;,\u0026#34;d\u0026#34;,..: 1 1 1 1 1 1 1 1 1 1 ... Perceptual adjective (column name) Example survey text perform Brand has a strong performance leader Brand is a leader in the field latest Brand has the latest products fun Brand is fun serious Brand is serious bargain Brand products are a bargain value Brand has a good value trendy Brand is trendy rebuy I would buy from Brand again 1.1 Rescaling the data 标准化 1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026gt; brand.sc \u0026lt;- brand.ratings \u0026gt; brand.sc[,1:9] \u0026lt;- scale (brand.ratings[,1:9]) \u0026gt; #we select all rows and the first 9 columns as the 10th column is a factor variable. \u0026gt; summary(brand.sc) perform leader latest fun serious bargain value trendy rebuy brand Min. :-1.0888 Min. :-1.3100 Min. :-1.6878 Min. :-1.84677 Min. :-1.1961 Min. :-1.22196 Min. :-1.3912 Min. :-1.53897 Min. :-1.0717 a :100 1st Qu.:-1.0888 1st Qu.:-0.9266 1st Qu.:-0.7131 1st Qu.:-0.75358 1st Qu.:-0.8362 1st Qu.:-0.84701 1st Qu.:-0.9743 1st Qu.:-0.80960 1st Qu.:-1.0717 b :100 Median :-0.1523 Median :-0.1599 Median : 0.2615 Median :-0.02478 Median :-0.1163 Median :-0.09711 Median :-0.1405 Median :-0.08023 Median :-0.2857 c :100 Mean : 0.0000 Mean : 0.0000 Mean : 0.0000 Mean : 0.00000 Mean : 0.0000 Mean : 0.00000 Mean : 0.0000 Mean : 0.00000 Mean : 0.0000 d :100 3rd Qu.: 0.7842 3rd Qu.: 0.6069 3rd Qu.: 0.9113 3rd Qu.: 0.70402 3rd Qu.: 0.6036 3rd Qu.: 0.65279 3rd Qu.: 0.6933 3rd Qu.: 0.64914 3rd Qu.: 0.5003 e :100 Max. : 1.7206 Max. : 2.1404 Max. : 1.2362 Max. : 1.43281 Max. : 2.0434 Max. : 2.15258 Max. : 2.3610 Max. : 1.74319 Max. : 2.4652 f :100 (Other):400 We can see the standardized variable has a mean of 0.00.\n1.2 Correlation Correlations among variables show some variables are highly related to each other and potentially could be combined.\n1 2 3 4 5 6 7 8 9 10 11 \u0026gt; cor(brand.sc[,1:9]) perform leader latest fun serious bargain value trendy rebuy perform 1.000000000 0.50020206 -0.122445813 -0.2563323 0.359172206 0.05712937 0.10194610 0.008733494 0.3066588 leader 0.500202058 1.00000000 0.026890447 -0.2903576 0.571215126 0.03309405 0.11831017 0.066512436 0.2087004 latest -0.122445813 0.02689045 1.000000000 0.2451545 0.009951527 -0.25441902 -0.34271372 0.627627667 -0.3971802 fun -0.256332316 -0.29035764 0.245154457 1.0000000 -0.281097443 -0.06655280 -0.14521849 0.127973639 -0.2371607 serious 0.359172206 0.57121513 0.009951527 -0.2810974 1.000000000 -0.00265559 0.02375656 0.121009377 0.1807027 bargain 0.057129372 0.03309405 -0.254419016 -0.0665528 -0.002655590 1.00000000 0.73962672 -0.350533746 0.4673811 value 0.101946104 0.11831017 -0.342713717 -0.1452185 0.023756556 0.73962672 1.00000000 -0.434534536 0.5059617 trendy 0.008733494 0.06651244 0.627627667 0.1279736 0.121009377 -0.35053375 -0.43453454 1.000000000 -0.2982462 rebuy 0.306658801 0.20870036 -0.397180225 -0.2371607 0.180702720 0.46738109 0.50596166 -0.298246195 1.0000000 1 corrplot(cor(brand.sc[,1:9])) {% asset_image R_week3_code_corrplot.png %}\ncorrplot()\ncorrplot 是 R 语言中用于可视化相关系数矩阵的函数。它提供了丰富的功能，可以创建各种类型的相关系数图，以帮助理解和分析变量之间的相关性。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 corrplot( M, # 相关系数矩阵 method = \u0026#34;circle\u0026#34;, # 相关系数图的类型，可以是 \u0026#34;circle\u0026#34;, \u0026#34;square\u0026#34;, \u0026#34;ellipse\u0026#34;, \u0026#34;number\u0026#34;, \u0026#34;shade\u0026#34;, \u0026#34;color\u0026#34;, \u0026#34;pie\u0026#34; 等 type = \u0026#34;full\u0026#34;, # 相关系数类型，可以是 \u0026#34;full\u0026#34;, \u0026#34;upper\u0026#34;, \u0026#34;lower\u0026#34;，默认是 \u0026#34;full\u0026#34; tl.col = \u0026#34;black\u0026#34;, # 文本标签的颜色 tl.srt = 45, # 文本标签的旋转角度 diag = FALSE, # 是否显示对角线元素 addgrid.col = \u0026#34;gray\u0026#34;, # 是否添加网格线，网格线的颜色 order = \u0026#34;original\u0026#34;, # 相关系数的排序方式，可以是 \u0026#34;original\u0026#34;, \u0026#34;alphabet\u0026#34;, \u0026#34;hclust\u0026#34;, \u0026#34;FPC\u0026#34;, \u0026#34;color\u0026#34; 等 addrect = 2, # 添加矩形边框的类型和颜色，0表示不添加，1表示默认颜色，2表示自定义颜色 rect.col = \u0026#34;black\u0026#34;, # 矩形边框的颜色 bg = \u0026#34;white\u0026#34;, # 相关系数图的背景颜色 mar = c(0,0,2,0), # 图的边缘空白 cl.lim = c(-1, 1), # 相关系数的颜色范围 cl.pos = \u0026#34;n\u0026#34;, # 相关系数的颜色条位置 addCoef.col = \u0026#34;black\u0026#34;, # 系数的颜色 总结一下，当您希望跨多个维度比较几个品牌时，将注意力集中在解释数据变异的前两个或三个主成分上可能会有所帮助。您可以使用屏幕图选择要关注的主成分数量，该图显示了每个主成分解释数据中的多少变化。感知地图将品牌绘制在前两个主成分上，揭示了观察结果与潜在维度（即成分）的关系。 PCA可以使用品牌调查评分（如我们在此处所做的）进行，也可以使用价格和物理测量等客观数据，或者二者的组合进行。无论在哪种情况下，当您面对品牌或产品的多维数据时，PCA可视化都是了解市场差异的有用工具。 addCoefasPercent = TRUE, # 是否以百分比形式显示系数 outline = TRUE, # 是否显示相关系数的外框 number.cex = 0.8, # 数字标签的大小 ...) corrplot 函数的主要参数包括：\nM：一个数值型的矩阵，其中存储了相关系数。 method：指定要绘制的相关系数图的类型，可以是 \u0026ldquo;circle\u0026rdquo;、\u0026ldquo;square\u0026rdquo;、\u0026ldquo;ellipse\u0026rdquo;、\u0026ldquo;number\u0026rdquo;、\u0026ldquo;shade\u0026rdquo;、\u0026ldquo;color\u0026rdquo;、\u0026ldquo;pie\u0026rdquo; 等。 其中，x是包含原始数据的数据框或矩阵，\u0026hellip;是其他参数，用于控制主成分分析的过程，例如是否进行变量标准化等。 princomp() 函数返回一个主成分分析结果的对象，该对象包含了主成分分析的各种信息，包括主成分分数、主成分载荷、方差解释比例等。可以使用summary()函数对主成分分析结果进行摘要统计，或者直接访问返回的对象的成分来获取相关信息。type：相关系数类型，可以是 \u0026ldquo;full\u0026rdquo;、\u0026ldquo;upper\u0026rdquo;、\u0026ldquo;lower\u0026rdquo;，默认是 \u0026ldquo;full\u0026rdquo;，表示绘制全部相关系数。\ntl.col：文本标签的颜色。 tl.srt：文本标签的旋转角度。 diag：是否显示对角线元素。 order：相关系数的排序方式，可以是 \u0026ldquo;original\u0026rdquo;、\u0026ldquo;alphabet\u0026rdquo;、\u0026ldquo;hclust\u0026rdquo;、\u0026ldquo;FPC\u0026rdquo;、\u0026ldquo;color\u0026rdquo; 等。 cl.lim：相关系数的颜色范围。 addCoefasPercent：是否以百分比形式显示系数。 1 2 # use the optional argument (order = “hclust”) to order variables for better visualization corrplot(cor(brand.sc[,1:9]), order = \u0026#34;hclust\u0026#34;) {% asset_image R_week3_code_corrplot_with_order.png %}\nThe result shows that some of the ratings are highly related to each other so that they could be represented by common factors.\n1.3 Mean rating by brand 1.3.1 aggregate() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 \u0026gt; brand.mean \u0026lt;- aggregate(. ~ brand, data=brand.sc, mean) \u0026gt; brand.mean brand perform leader latest fun serious bargain value trendy rebuy 1 a -0.88591874 -0.5279035 0.4109732 0.6566458 -0.91894067 0.21409609 0.18469264 -0.52514473 -0.59616642 2 b 0.93087022 1.0707584 0.7261069 -0.9722147 1.18314061 0.04161938 0.15133957 0.74030819 0.23697320 3 c 0.64992347 1.1627677 -0.1023372 -0.8446753 1.22273461 -0.60704302 -0.44067747 0.02552787 -0.13243776 4 d -0.67989112 -0.5930767 0.3524948 0.1865719 -0.69217505 -0.88075605 -0.93263529 0.73666135 -0.49398892 5 e -0.56439079 0.1928362 0.4564564 0.2958914 0.04211361 0.55155051 0.41816415 0.13857986 0.03654811 6 f -0.05868665 0.2695106 -1.2621589 -0.2179102 0.58923066 0.87400696 1.02268859 -0.81324496 1.35699580 7 g 0.91838369 -0.1675336 -1.2849005 -0.5167168 -0.53379906 0.89650392 1.25616009 -1.27639344 1.36092571 8 h -0.01498383 -0.2978802 0.5019396 0.7149495 -0.14145855 -0.73827529 -0.78254646 0.86430070 -0.60402622 9 i 0.33463879 -0.3208825 0.3557436 0.4124989 -0.14865746 -0.25459062 -0.80339213 0.59078782 -0.20317603 10 j -0.62994504 -0.7885965 -0.1543180 0.2849595 -0.60218870 -0.09711188 -0.07379367 -0.48138267 -0.96164748 \u0026gt; rownames(brand.mean) \u0026lt;- brand.mean[, 1] \u0026gt; # Use brand for the row name \u0026gt; brand.mean \u0026lt;- brand.mean [, -1] \u0026gt; # Remove the brand name column by not selecting the first column # Negative index is used to exclude the variable \u0026gt; brand.mean perform leader latest fun serious bargain value trendy rebuy a -0.88591874 -0.5279035 0.4109732 0.6566458 -0.91894067 0.21409609 0.18469264 -0.52514473 -0.59616642 b 0.93087022 1.0707584 0.7261069 -0.9722147 1.18314061 0.04161938 0.15133957 0.74030819 0.23697320 c 0.64992347 1.1627677 -0.1023372 -0.8446753 1.22273461 -0.60704302 -0.44067747 0.02552787 -0.13243776 d -0.67989112 -0.5930767 0.3524948 0.1865719 -0.69217505 -0.88075605 -0.93263529 0.73666135 -0.49398892 e -0.56439079 0.1928362 0.4564564 0.2958914 0.04211361 0.55155051 0.41816415 0.13857986 0.03654811 f -0.05868665 0.2695106 -1.2621589 -0.2179102 0.58923066 0.87400696 1.02268859 -0.81324496 1.35699580 g 0.91838369 -0.1675336 -1.2849005 -0.5167168 -0.53379906 0.89650392 1.25616009 -1.27639344 1.36092571 h -0.01498383 -0.2978802 0.5019396 0.7149495 -0.14145855 -0.73827529 -0.78254646 0.86430070 -0.60402622 i 0.33463879 -0.3208825 0.3557436 0.4124989 -0.14865746 -0.25459062 -0.80339213 0.59078782 -0.20317603 j -0.62994504 -0.7885965 -0.1543180 0.2849595 -0.60218870 -0.09711188 -0.07379367 -0.48138267 -0.96164748 Then we can visualize this matrix by using a heat map :\n1 heatmap.2(as.matrix(brand.mean),main = \u0026#34;Brand attributes\u0026#34;, trace = \u0026#34;none\u0026#34;, key = FALSE, dend = \u0026#34;none\u0026#34;) #turn off some options 颜色越浅表示值越高，颜色越深表示值越低。我们可以看到，品牌的感知明显不同，一些品牌在performance、leader方面得分很高(品牌b和c)，而另一些品牌在value和rebuy方面得分很高(品牌f和g)。 从相关图和热图图中，我们可以猜测形容词和品牌的分组和关系。例如，bargin/value/rebuy的各栏的颜色模式有相似之处。接下来我们将正式阐述这种见解。\n2 Principal component analysis (PCA) using princomp() princomp()函数产生未旋转的主成分分析。\n您可以使用原始的个人评分数据brand.sc或聚合的平均评分数据brand.mean。个体受访者评分的图表可能会过于密集，可能无法清楚地告诉我们有关品牌位置的信息！更好的解决方案是使用按品牌聚合的评分执行PCA。\n2.1 PCA 1 2 3 4 5 6 7 8 9 \u0026gt; brand.pc\u0026lt;- princomp(brand.mean, cor = TRUE) \u0026gt; #We added \u0026#34;cor =TRUE\u0026#34; to use correlation-based one. \u0026gt; summary(brand.pc) Importance of components: Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9 Standard deviation 2.134521 1.7349473 0.76898915 0.61498280 0.5098261 0.36661576 0.215062433 0.145882355 0.0486674686 Proportion of Variance 0.506242 0.3344491 0.06570492 0.04202265 0.0288803 0.01493412 0.005139094 0.002364629 0.0002631692 Cumulative Proportion 0.506242 0.8406911 0.90639603 0.94841868 0.9772990 0.99223311 0.997372202 0.999736831 1.0000000000 princomp()\nprincomp() 函数是R语言中用于进行主成分分析（PCA）的函数之一。它可以用于对数据进行降维，找出数据中的主要方向或主成分，从而简化数据并提取关键特征。\n其中，x是包含原始数据的数据框或矩阵，\u0026hellip;是其他参数，用于控制主成分分析的过程，例如是否进行变量标准化等。\nprincomp() 函数返回一个主成分分析结果的对象，该对象包含了主成分分析的各种信息，包括主成分分数、主成分载荷、方差解释比例等。可以使用summary()函数对主成分分析结果进行摘要统计，或者直接访问返回的对象的成分来获取相关信息。\n1 plot(brand.pc,type=\u0026#34;l\u0026#34;) # scree plot {% asset_image R_week3_PCA %}\n发生在图线出现拐点的地方，在上图中，拐点出现在第三个组件处。这表明前两个组件解释了观察到的品牌评分中大部分的变异。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 \u0026gt; loadings(brand.pc) # pc loadings Loadings: Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9 perform 0.285 0.337 0.481 0.470 0.396 0.435 leader 0.247 0.457 -0.317 -0.191 0.119 -0.610 0.451 latest -0.356 0.251 -0.496 0.275 0.461 -0.196 -0.119 -0.466 fun -0.336 -0.335 -0.152 0.324 -0.388 0.636 -0.246 0.179 serious 0.212 0.475 -0.244 -0.212 -0.394 0.334 0.439 -0.407 bargain 0.361 -0.278 -0.459 0.291 0.112 0.127 0.319 -0.513 0.321 value 0.401 -0.241 -0.336 0.206 0.778 trendy -0.311 0.375 0.484 -0.273 -0.339 0.322 0.243 0.410 rebuy 0.430 0.442 -0.438 -0.368 -0.352 -0.142 -0.372 Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9 SS loadings 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 Proportion Var 0.111 0.111 0.111 0.111 0.111 0.111 0.111 0.111 0.111 Cumulative Var 0.111 0.222 0.333 0.444 0.556 0.667 0.778 0.889 1.000 loadings()\n通常用于提取主成分分析（PCA）结果中的主成分载荷。主成分载荷是描述主成分与原始变量之间相关性的系数，它告诉我们每个主成分与每个原始变量之间的关系强度和方向。\n1 loadings(object) 其中，object是进行主成分分析后返回的结果对象，通常是使用princomp()或prcomp()函数进行主成分分析后得到的对象。\nloadings()函数会返回一个矩阵，其中的行表示原始变量，列表示主成分。矩阵中的元素是主成分载荷，表示每个主成分与对应原始变量之间的相关系数。\n2.2 Visualising PCA (perceptual map) biplot()是关于前两个PCA成分的数据点的二维图。它展示了评分形容词之间的关联。对平均评分的PCA解的biplot提供了可解释的感知图，显示了品牌相对于前两个主成分的位置。\n1 biplot(brand.pc, main = \u0026#34;Brand positioning\u0026#34;) {% asset_image R_week3_biplot.png %}\nThe result shows the adjective map in four regions:\n• category leadership: “serious”, “leader”, and “perform”\n• category value: “rebuy”, “value”, and “bargain”\n• category trendiness: “trendy” and “latest”\n• category fun: “fun”\nWhat does the map tell us? First, we interpret the adjective clusters and relationships and see four areas with well-differentiated sets of adjectives and brands that are positioned in proximity. Brands f and g are high on “value”, while brands a and j are relatively high on fun, which is opposite in direction from leadership adjectives (“leader” and “serious”).\n假设您是品牌e的品牌经理，地图告诉您什么？\n您的品牌位于中心，因此在任何维度上似乎都没有很好的差异化。这可能是好事，也可能是坏事，这取决于您的战略目标。\n关于您的品牌e的位置，您应该怎么做？\n再次强调，这取决于战略目标。如果您希望增加差异化，一种可能性是采取行动将您的品牌朝地图上的某个方向移动。\n假设您希望朝品牌c的方向移动 您可以查看数据中与c的具体差异：\n1 2 3 \u0026gt; brand.mean[\u0026#34;c\u0026#34;,] - brand.mean[\u0026#34;e\u0026#34;,] perform leader latest fun serious bargain value trendy rebuy c 1.214314 0.9699315 -0.5587936 -1.140567 1.180621 -1.158594 -0.8588416 -0.113052 -0.1689859 这表明在“value”和“fun”方面，e相对于c更强， 而在“perform”和“serious”方面则比c弱。\n这些可能是e需要努力改进的产品或信息方面。\n另一个选择是瞄准一个没有品牌定位的差异化空间 在图中，右下角和右上角之间有一个很大的间隙。这个区域可以被描述为“value leader”区域或类似的区域。我们如何找出如何定位在那里呢？\n让我们假设这个间隙大致反映了那四个品牌（g、f、c和b）的平均值。我们可以使用colMeans()在品牌的行上找到平均值，然后将e与该平均值相比较：\n1 2 3 \u0026gt; colMeans(brand.mean[c(\u0026#34;b\u0026#34;,\u0026#34;c\u0026#34;,\u0026#34;f\u0026#34;,\u0026#34;g\u0026#34;),]) - brand.mean[\u0026#34;e\u0026#34;,] perform leader latest fun serious bargain value trendy rebuy e 1.174513 0.3910396 -0.9372789 -0.9337707 0.5732131 -0.2502787 0.07921355 -0.4695304 0.6690661 这表明品牌e可以通过增加对性能的重视，同时减少对“最新”和“乐趣”的重视来瞄准这个间隙。\n3 Factor analysis using factanal() * factanal()函数执行最大似然因子分析。\nfactanal()\nfactanal()函数是R语言中用于执行因子分析（Factor Analysis）的函数之一。因子分析是一种多变量统计方法，用于探索多个变量之间的内在结构和关联关系，它试图通过发现潜在的不可观测的因子来解释变量之间的相关性。\n基本语法如下：\n1 factanal(x, factors, data, ...) 其中：\nx 是包含原始数据的数据框或矩阵。 factors 是要提取的因子数量，可以是一个整数或一个因子提取方法，如\u0026quot;ML\u0026quot;表示最大似然估计法。 data 是包含原始数据的数据框，如果在x中指定了变量名，可以不使用该参数。 \u0026hellip; 是其他参数，用于指定因子分析的一些选项，例如旋转方法、提取方法等。 factanal()函数返回一个包含因子分析结果的对象，其中包括提取的因子、因子载荷、特征值等信息。可以使用print()或summary()函数来查看因子分析结果的摘要信息。\n最大似然因子分析（Maximum Likelihood Factor Analysis）\n是因子分析（Factor Analysis）的一种参数估计方法。在最大似然因子分析中，通过最大化观察数据的似然函数来估计因子载荷、因子方差和误差方差等参数，以获得对数据结构的最佳解释。\n最大似然因子分析假设观察数据服从多变量正态分布，并试图找出能够最大程度地解释数据变异的潜在因子。具体来说，它假设每个观察数据是由一个线性组合的因子和误差项组成，并尝试找出最合适的因子载荷，使得模型对观察数据的拟合最优。\n在最大似然因子分析中，通常会使用EM算法（期望最大化算法）来估计模型参数，其中EM算法的目标是在每次迭代中最大化观察数据的似然函数，通过交替地进行E步骤（Expectation Step）和M步骤（Maximization Step）来达到这个目标。\n最大似然因子分析是一种常用的因子分析方法，它通常用于探索多个变量之间的潜在结构，并识别出能够解释观察数据变异的重要因子。\n3.1 Determine the factor number 首先要确定要估计的因子数量。有各种方法可以做到这一点，其中两种传统方法是使用screen plot，以及保留特征值（用于解释方差比例的指标）大于1.0的因子。特征值为1.0对应于可能归因于单个自变量的方差量；捕获比此类项更少方差的因子可能被认为相对不重要。\nscreen plot和eigenvalues可以通过使用nFactors中的nScree()函数从分析中正式获得。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 \u0026gt; nScree(brand.mean) noc naf nparallel nkaiser 1 2 2 2 2 \u0026gt; eigen(cor(brand.mean)) eigen() decomposition $values [1] 4.556177847 3.010042148 0.591344314 0.378203844 0.259922697 0.134407115 0.046251850 0.021281662 0.002368523 $vectors [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [1,] -0.2852486 -0.33729698 0.48121446 0.46995620 0.39623804 0.43471514 -0.02784431 0.074243080 0.012984626 [2,] -0.2473668 -0.45654557 -0.31711577 -0.19084693 -0.06130157 0.11868317 0.60997229 0.021119910 0.450594077 [3,] 0.3562989 -0.25056983 -0.49589600 0.27477470 0.46061874 -0.08173299 0.19587019 -0.119316063 -0.466262266 [4,] 0.3355152 0.33455495 -0.15167546 0.32394053 -0.38757837 0.63609709 0.24602385 0.179248006 -0.008094488 [5,] -0.2121240 -0.47463096 -0.24371327 -0.21229430 -0.39428137 0.33437227 -0.43881277 0.005157446 -0.406716076 [6,] -0.3613409 0.27776101 -0.45940272 0.29120398 0.11248446 0.12716342 -0.31905166 -0.512721569 0.320827507 [7,] -0.4010778 0.24062869 -0.33576144 0.05052374 0.20581208 -0.08329187 -0.08325891 0.778125659 -0.065102236 [8,] 0.3114405 -0.37521575 -0.08724910 0.48392969 -0.27261916 -0.33925412 -0.32150758 0.243224760 0.410460300 [9,] -0.4295359 0.04438337 0.09031492 0.44234693 -0.43824713 -0.36828116 0.35159046 -0.141872872 -0.371841553 nScree()应用了几种方法来从屏幕测试中估计因子的数量。 nScree()\n用于绘制Scree Plot（屏幕图），以帮助确定因子分析中的因子数量。\n基本语法是：\n1 nScree(fit) 其中，fit是一个包含因子分析结果的对象，通常是使用nFactors包中的fa()函数进行因子分析后得到的结果。\n此函数将会绘制一个Scree Plot，图中横轴表示因子数量，纵轴表示特征值（eigenvalue），并且可以根据特征值的变化趋势来帮助确定合适的因子数量。通常，我们会观察Scree Plot中特征值开始急剧下降的位置，这个位置之前的因子数量通常被认为是适当的因子数量。\n在您的情况下，您使用nScree(brand.mean)，其中brand.mean是您的数据，将会绘制一个Scree Plot来帮助您确定适当的因子数量。您可以根据Scree Plot的结果来选择合适的因子数量，以便在因子分析中获得最佳的解释力和简化度。\neigen()\neigen() 函数是R语言中用于计算矩阵特征值（eigenvalues）和特征向量（eigenvectors）的函数。在因子分析中，它通常用于计算相关矩阵的特征值和特征向量，以便进行主成分分析或因子分析。\n1 eigen(x) 其中，x是一个方阵，可以是一个相关矩阵或协方差矩阵。\neigen()函数返回一个列表，其中包含了两个元素：\n$values：一个向量，包含了矩阵的特征值。 $vectors：一个矩阵，每一列是矩阵的一个特征向量。 这些特征值和特征向量可以用于进行主成分分析（PCA）或因子分析（FA），以及其他需要计算矩阵特征值和特征向量的分析方法。\n最终模型的选择取决于它是否有用。最佳实践是检查几个因子解决方案，包括scree和eigenvalues结果建议的解决方案。\n1 2 3 \u0026gt; brand.fa \u0026lt;- factanal(brand.mean, factors = 2, rotation = \u0026#34;varimax\u0026#34;, scores = \u0026#34;regression\u0026#34;) 因子分析可以进行旋转，以具有解释相同方差比例的新载荷。factanal()中的默认设置是找到各因子之间的零相关性（使用varimax旋转）。 3.2 Factor loadings 因子载荷是从载荷值中获得的。\n1 \u0026gt; brand.fl\u0026lt;- brand.fa$loadings[, 1:2] 可以简单地使用因子载荷将变量绘制在两个因子上。例如：\n1 2 3 \u0026gt; plot(brand.fl,type=\u0026#34;n\u0026#34;) # set up plot \u0026gt; text(brand.fl,labels=names(brand.mean),cex=.7) {% asset_image R_week3_factor_loading.png %}\n3.3 Factor scores 从factanal()请求因子得分是通过添加\u0026quot;scores = \u0026hellip;\u0026ldquo;参数实现的。然后，我们可以从生成的对象中提取它们以形成一个新的数据框。\n1 brand.fs \u0026lt;- brand.fa$scores 我们可以简单地使用因子得分将品牌绘制在两个因子上。例如：\n1 2 3 \u0026gt; plot(brand.fl,type=\u0026#34;n\u0026#34;) # set up plot \u0026gt; text(brand.fl,labels=rownames(brand.mean),cex=.7) {% asset_image R_week3_factor_scores.png %}\nRecap 总结一下，当希望跨多个维度比较几个品牌时，将注意力集中在解释数据变异的前两个或三个主成分上可能会有所帮助。您可以使用屏幕图选择要关注的主成分数量，该图显示了每个主成分解释数据中的多少变化。感知地图将品牌绘制在前两个主成分上，揭示了观察结果与潜在维度（即成分）的关系。\nPCA可以使用品牌调查评分（如我们在此处所做的）进行，也可以使用价格和物理测量等客观数据，或者二者的组合进行。无论在哪种情况下，当您面对品牌或产品的多维数据时，PCA可视化都是了解市场差异的有用工具。\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 library(\u0026#34;corrplot\u0026#34;) library(\u0026#34;gplots\u0026#34;) library(\u0026#34;nFactors\u0026#34;) brand.ratings \u0026lt;- read.csv(\u0026#34;Data_Factor_Analysis.csv\u0026#34;, stringsAsFactors = TRUE) head(brand.ratings) summary(brand.ratings) str(brand.ratings) brand.sc \u0026lt;- brand.ratings brand.sc[,1:9] \u0026lt;- scale (brand.ratings[,1:9]) #we select all rows and the first 9 columns as the 10th column is a factor variable. summary(brand.sc) cor(brand.sc[,1:9]) corrplot(cor(brand.sc[,1:9])) corrplot(cor(brand.sc[,1:9]), order = \u0026#34;hclust\u0026#34;) brand.mean \u0026lt;- aggregate(. ~ brand, data=brand.sc, mean) brand.mean rownames(brand.mean) \u0026lt;- brand.mean[, 1] # Use brand for the row name brand.mean \u0026lt;- brand.mean [, -1] # Remove the brand name column by not selecting the first column # Negative index is used to exclude the variable brand.mean heatmap.2(as.matrix(brand.mean),main = \u0026#34;Brand attributes\u0026#34;, trace = \u0026#34;none\u0026#34;, key = FALSE, dend = \u0026#34;none\u0026#34;) #turn off some options brand.pc\u0026lt;- princomp(brand.mean, cor = TRUE) #We added \u0026#34;cor =TRUE\u0026#34; to use correlation-based one. summary(brand.pc) plot(brand.pc,type=\u0026#34;l\u0026#34;) # scree plot loadings(brand.pc) # pc loadings biplot(brand.pc, main = \u0026#34;Brand positioning\u0026#34;) brand.mean[\u0026#34;c\u0026#34;,] - brand.mean[\u0026#34;e\u0026#34;,] colMeans(brand.mean[c(\u0026#34;b\u0026#34;,\u0026#34;c\u0026#34;,\u0026#34;f\u0026#34;,\u0026#34;g\u0026#34;),]) - brand.mean[\u0026#34;e\u0026#34;,] nScree(brand.mean) eigen(cor(brand.mean)) brand.fa \u0026lt;- factanal(brand.mean, factors = 2, rotation = \u0026#34;varimax\u0026#34;, scores = \u0026#34;regression\u0026#34;) brand.fl\u0026lt;- brand.fa$loadings[, 1:2] plot(brand.fl,type=\u0026#34;n\u0026#34;) # set up plot text(brand.fl,labels=names(brand.mean),cex=.7) plot(brand.fl,type=\u0026#34;n\u0026#34;) # set up plot text(brand.fl,labels=rownames(brand.mean),cex=.7) ","date":"2024-03-23T00:00:00Z","permalink":"https://MyLoveES.github.io/p/rweek3-factor-analysis-for-perceptual-maps-code/","title":"R[week3] Factor Analysis For Perceptual Maps Code"},{"content":"Prepare 收集数据：通过调查问卷收集目标消费者群体对选定品牌在关键属性上的评价。 选择属性：确定哪些属性对目标消费者群体来说最重要。 Perceptual mapping 感知映射，可视化现实消费者观点\n{% asset_image R_week3_perceptual_mapping.png %}\nFactor Analysis 数据简化，降维到少量factor，使用因子分析来减少属性的数量，只保留最能解释数据变异的因子。 压缩成简短的相似特征，构建感知图。 解释因子：根据属性的因子载荷（即属性与因子之间的相关性），解释每个因子代表的潜在概念。 Create perceptual map 选择因子：根据因子分析的结果，选择两个或三个最主要的因子来创建感知图。 计算因子得分：为每个品牌计算每个因子的得分，这些得分将用于在感知图上定位品牌。 绘制感知图：在一个二维平面上，以选定的因子为坐标轴，绘制每个品牌的点。 Pearson相关系数（Pearson Correlation Coefficient）\n是用来衡量两个变量之间线性相关程度的统计指标。其计算公式如下：\n$$ r = \\frac{\\sum((x_i - \\bar{x}) \\cdot (y_i - \\bar{y}))}{\\sqrt{\\sum(x_i - \\bar{x})^2} \\cdot \\sqrt{\\sum(y_i - \\bar{y})^2}} $$相关系数的值范围从 -1 到 1，其中 -1 表示完全负相关，1 表示完全正相关，0 表示没有线性相关性。\nExplain 分析品牌位置：观察品牌在感知图上的相对位置，了解它们在消费者心目中的优势和劣势。 制定策略：基于感知图的结果，制定品牌定位策略，以改善品牌形象或区分竞争对手。 {% asset_image R_week3_jobs.png %}\n","date":"2024-03-22T00:00:00Z","permalink":"https://MyLoveES.github.io/p/rweek3-factor-analysis-for-perceptual-maps-lecture/","title":"R[week3] Factor Analysis For Perceptual Maps Lecture"},{"content":" R: 4.3.2 (2023-10-31)\nR studio: 2023.12.1+402 (2023.12.1+402)\n1. Segmentation data • 年龄（age）\n• 性别（gender）\n• 收入（income）\n• 孩子数量（kids）\n• 是否拥有或租赁住房（ownHome）\n• 当前是否订阅所提供的会员服务（subscribe）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026gt; seg.df \u0026lt;- read.csv(\u0026#34;Data_Segmentation.csv\u0026#34;, stringsAsFactors = TRUE) \u0026gt; head(seg.df, n=8) \u0026gt; head(seg.df, n=8) age gender income kids ownHome subscribe 1 47.31613 Male 49482.81 2 ownNo subNo 2 31.38684 Male 35546.29 1 ownYes subNo 3 43.20034 Male 44169.19 0 ownYes subNo 4 37.31700 Female 81041.99 1 ownNo subNo 5 40.95439 Female 79353.01 3 ownYes subNo 6 43.03387 Male 58143.36 4 ownYes subNo 7 37.55696 Male 19282.23 3 ownNo subNo 8 28.45129 Male 47245.24 0 ownNo subNo 1.1 Recode factor into numeric data 1 2 3 4 5 6 7 8 9 10 11 12 \u0026gt; seg.df$gender \u0026lt;- ifelse(seg.df$gender==\u0026#34;Male\u0026#34;,0,1) \u0026gt; seg.df$ownHome \u0026lt;- ifelse(seg.df$ownHome == \u0026#34;ownNo\u0026#34;, 0,1) \u0026gt; seg.df$subscribe \u0026lt;- ifelse(seg.df$subscribe == \u0026#34;subNo\u0026#34;, 0,1) \u0026gt; head(seg.df) age gender income kids ownHome subscribe 1 47.31613 0 49482.81 2 0 0 2 31.38684 0 35546.29 1 1 0 3 43.20034 0 44169.19 0 1 0 4 37.31700 1 81041.99 1 0 0 5 40.95439 1 79353.01 3 1 0 6 43.03387 0 58143.36 4 1 0 1.2 Rescaling the data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 \u0026gt; seg.df.sc \u0026lt;- seg.df \u0026gt; seg.df.sc[, c(1,3,4)] \u0026lt;- scale(seg.df[, c(1,3,4)]) # We only need to standardize continuous variables. \u0026gt; head(seg.df.sc) age gender income kids ownHome subscribe 1 0.48133138 0 -0.0721898 0.5183027 0 0 2 -0.77221071 0 -0.7642562 -0.1917010 1 0 3 0.15744276 0 -0.3360563 -0.9017048 1 0 4 -0.30554218 1 1.4949908 -0.1917010 0 0 5 -0.01930052 1 1.4111190 1.2283065 1 0 6 0.14434200 0 0.3578800 1.9383103 1 0 \u0026gt; summary(seg.df.sc, digits = 2) age gender income kids ownHome subscribe Min. :-1.73 Min. :0.00 Min. :-2.787 Min. :-0.90 Min. :0.00 Min. :0.00 1st Qu.:-0.64 1st Qu.:0.00 1st Qu.:-0.560 1st Qu.:-0.90 1st Qu.:0.00 1st Qu.:0.00 Median :-0.13 Median :1.00 Median : 0.054 Median :-0.19 Median :0.00 Median :0.00 Mean : 0.00 Mean :0.52 Mean : 0.000 Mean : 0.00 Mean :0.47 Mean :0.13 3rd Qu.: 0.53 3rd Qu.:1.00 3rd Qu.: 0.520 3rd Qu.: 0.52 3rd Qu.:1.00 3rd Qu.:0.00 Max. : 3.09 Max. :1.00 Max. : 3.145 Max. : 4.07 Max. :1.00 Max. :1.00 2. Hierarchical clustering: hclust() 2.1 Distance 1 2 3 4 5 6 7 8 9 \u0026gt; seg.dist \u0026lt;- dist(seg.df.sc) \u0026gt; as.matrix(seg.dist)[1:5, 1:5] 1 2 3 4 5 1 0.000000 1.885319 1.786323 2.139937 2.225970 2 1.885319 0.000000 1.245679 2.705915 2.883670 3 1.786323 1.245679 0.000000 2.463979 2.936121 4 2.139937 2.705915 2.463979 0.000000 1.762212 5 2.225970 2.883670 2.936121 1.762212 0.000000 2.2 Clustering 1 2 \u0026gt; seg.hc \u0026lt;- hclust(seg.dist, method=\u0026#34;complete\u0026#34;) \u0026gt; plot(seg.hc) {% asset_image R_week2_cluster_plot.png %}\nhclust(d, method = \u0026ldquo;complete\u0026rdquo;)\nd：数据的距离矩阵或相似度矩阵。距离矩阵是一个对称矩阵，其中每个元素表示两个观测点之间的距离。相似度矩阵也是一个对称矩阵，其中每个元素表示两个观测点之间的相似度。通常可以使用 dist() 函数计算距离矩阵或相似度矩阵。\nmethod：指定用于计算簇间距离的方法。常用的方法包括：\n\u0026ldquo;single\u0026rdquo;：最短距离法（single linkage）。 \u0026ldquo;complete\u0026rdquo;：最远距离法（complete linkage）。 \u0026ldquo;average\u0026rdquo;：平均距离法（average linkage）。 \u0026ldquo;ward.D\u0026rdquo;：Ward\u0026rsquo;s 方法，通过最小化簇内方差来合并簇。 hclust() 函数返回一个树形结构对象（dendrogram），可以使用 plot() 函数对其进行可视化。通过调整 method 参数，可以控制聚类的方式以及簇间的相似性度量。\n1 \u0026gt; plot(cut(as.dendrogram(seg.hc), h = 4)$lower[[1]]) {% asset_image R_week2_cluster_plot_2.png %}\n1 2 3 4 5 6 7 8 9 \u0026gt; seg.df[c(156, 152),] #similar age gender income kids ownHome subscribe 156 66.89691 0 54060.59 0 1 0 152 64.45890 0 61052.08 0 1 0 \u0026gt; seg.df[c(156, 183),] #less similar age gender income kids ownHome subscribe 156 66.89691 0 54060.59 0 1 0 183 57.79006 1 105537.82 0 1 0 as.dendrogram()\n将不同的对象转换为树形结构对象。它可以接受多种不同的输入对象，包括：\nhclust 对象：从 hclust() 函数返回的聚类结果对象。 agnes 对象：从 agnes() 函数返回的聚类结果对象。 phylo 对象：从 ape 包中的函数返回的系统发育树对象。 dist 对象：距离矩阵对象，表示观测点之间的距离或相似度。 使用 as.dendrogram() 函数可以将以上这些对象转换为 dendrogram 对象，然后可以使用 dendrogram 对象进行可视化、分析或其他操作。\n2.3 Getting groups 1 2 3 \u0026gt; plot(seg.hc) \u0026gt; rect.hclust(seg.hc, k=4, border = \u0026#34;red\u0026#34;) {% asset_image R_week2_getting_group_plot.png %}\n1 2 3 4 5 6 \u0026gt; seg.hc.segment \u0026lt;- cutree(seg.hc, k=4) #membership vector for 4 groups \u0026gt; table(seg.hc.segment) #counts seg.hc.segment 1 2 3 4 164 73 30 33 2.4 Describing clusters 1 2 3 4 5 6 7 8 library(cluster) clusplot(seg.df, seg.hc.segment, color = TRUE, #color the lines = 0, #omit distance lines between groups main = \u0026#34;Hierarchical cluster plot\u0026#34;, # figure title groupsshade = TRUE, #shade the ellipses for group membership labels = 4, #label only the groups, not the individual points ) {% asset_image R_week2_describing_clusters_plot.png %}\n1 2 3 4 5 6 \u0026gt; aggregate(seg.df, list(seg.hc.segment), mean) Group.1 age gender income kids ownHome subscribe 1 1 35.70250 0.5121951 45474.72 0.847561 0.3292683 0.15243902 2 2 37.34969 0.5479452 54571.75 3.273973 0.4520548 0.08219178 3 3 57.61221 0.6333333 40589.64 0.100000 0.9333333 0.16666667 4 4 62.11487 0.4242424 79444.84 0.000000 0.7878788 0.12121212 1 boxplot(seg.df$income ~ seg.hc.segment, ylab = \u0026#34;Income\u0026#34;, xlab = \u0026#34;Cluster\u0026#34;) {% asset_image R_week2_describing_clusters_plot_2.png %}\nboxplot()\n绘制箱线图，是一种常用的统计图形，用于显示一组数据的分布情况，包括中位数、四分位数、极值等。\n箱线图\n箱线图（Boxplot）是一种常用的统计图形，用于显示一组数据的分布情况。它提供了关于数据分布、中心位置、离散程度和异常值等方面的直观信息。箱线图通常包括以下几个部分：\n箱子（Box）：箱子由两条水平线段组成，分别表示数据的上四分位数（Q3）和下四分位数（Q1）。箱子的中间线表示数据的中位数（Q2）。\n须（Whiskers）：须延伸出箱子的两端，通常由直线或线段表示。须的长度通常是箱子高度的1.5倍的四分位距（IQR = Q3 - Q1）。须的端点通常是最大非异常值和最小非异常值，但具体的定义可能因数据分布而异。\n异常值（Outliers）：在箱子的须之外的数据点被视为异常值，并单独绘制出来，通常以圆圈或星号表示。\n箱线图的绘制过程通常涉及以下步骤：\n计算数据的描述性统计量，包括最小值、最大值、中位数、四分位数等。 根据计算的描述性统计量绘制箱子和须。 根据数据中的异常值绘制异常值。 添加额外的注释和标签，使得图形更加清晰易懂。 箱线图可以用于比较不同组之间的数据分布情况，检测异常值，以及观察数据的离散程度。它是一种简洁而有效的可视化工具，适用于各种类型的数据分析和探索。\n3. Mean-based clustering: kmeans() 3.1 Clustering 1 2 \u0026gt; set.seed(96743) \u0026gt; seg.k \u0026lt;- kmeans(seg.df.sc, centers = 4) #use standardized variables 3.2 Describing clusters 1 2 3 4 5 6 \u0026gt; aggregate(seg.df, list(seg.k$cluster), mean) Group.1 age gender income kids ownHome subscribe 1 1 26.12680 0.3728814 21850.99 1.16949153 0.2542373 0.16949153 2 2 59.86977 0.5522388 66319.07 0.02985075 0.7611940 0.08955224 3 3 38.10768 0.6136364 56864.13 2.96590909 0.4772727 0.11363636 4 4 40.15887 0.5116279 52841.06 0.56976744 0.3837209 0.16279070 1 boxplot(seg.df$income ~ seg.k$cluster, ylab = \u0026#34;Income\u0026#34;, xlab = \u0026#34;Cluster\u0026#34;) {% asset_image R_week2_means_describing_clusters_plot.png %}\n1 2 3 4 5 6 7 clusplot(seg.df, seg.k$cluster, color = TRUE, shade = TRUE, labels = 4, lines = 0, main = \u0026#34;K_means cluster plot\u0026#34;, ) {% asset_image R_week2_means_describing_clusters_plot_2.png %}\n这可能暗示了一种商业策略。在当前情况下，例如，我们看到第二组在一定程度上有所区别，并且拥有最高的平均收入。这可能使其成为潜在宣传活动的良好目标。还有许多其他策略可供选择；关键点在于分析提供了值得考虑的有趣选项。\nk-means分析的一个局限性是需要指定聚类的数量，并且很难确定一个解决方案是否比另一个更好。如果我们要对当前的问题使用k-means，我们将重复分析k=3、4、5等，并确定哪个解决方案对我们的业务目标最有用。\n4. Model-based clustering: Mclust() 关键思想是观察结果来自具有不同统计分布（例如不同的均值和方差）的群体。算法试图找到最佳的一组这样的潜在分布，以解释观察到的数据。\n4.1 Clustering 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \u0026gt; library(mclust) #install if needed \u0026gt; seg.mc \u0026lt;- Mclust(seg.df.sc) \u0026gt; summary(seg.mc) ---------------------------------------------------- Gaussian finite mixture model fitted by EM algorithm ---------------------------------------------------- Mclust VEV (ellipsoidal, equal shape) model with 3 components: log-likelihood n df BIC ICL -1291.139 300 73 -2998.655 -2998.655 Clustering table: 1 2 3 163 71 66 结果显示，数据被估计为具有三个大小不同的聚类，如表所示。\n我们还看到对数似然信息，我们可以用它来比较模型。例如，我们尝试一个四个聚类的解决方案。\n对数似然（log-likelihood）是统计学中用于评估模型拟合优良度的一种常用指标。它通常用于描述一个模型对观测数据的拟合程度有多好。对数似然值越高，表示模型对观测数据的拟合越好。\n具体来说，对数似然值是在给定模型参数的条件下，观测数据出现的概率的对数。对于给定的观测数据集，我们可以将观测数据视为从一个概率分布中独立地抽取得到的样本。然后，对数似然值表示了在给定模型参数的条件下，观测数据出现的可能性大小。对数似然值越高，表示观测数据出现的可能性越大，即模型拟合效果越好。\n在实际应用中，对数似然值通常用于比较不同模型的拟合效果。我们可以对同一组观测数据使用不同的模型进行拟合，并计算每个模型的对数似然值。然后，通过比较对数似然值的大小，我们可以确定哪个模型对观测数据的拟合效果更好。\n需要注意的是，对数似然值通常是负数。这是因为对数似然值是一个概率的对数，而概率的取值范围是0到1之间。因此，对数似然值越大，表示概率越接近于1，而对数似然值越小（即绝对值越大），表示概率越接近于0。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026gt; seg.mc4 \u0026lt;- Mclust(seg.df.sc, G =4) #specifying the number of clusters \u0026gt; summary(seg.mc4) ---------------------------------------------------- Gaussian finite mixture model fitted by EM algorithm ---------------------------------------------------- Mclust VII (spherical, varying volume) model with 4 components: log-likelihood n df BIC ICL -1690.304 300 31 -3557.425 -3597.143 Clustering table: 1 2 3 4 51 74 119 56 看起来分为3组时，对数似然估计更高。\n4.2 Comparing models 1 2 3 4 \u0026gt; BIC(seg.mc, seg.mc4) df BIC seg.mc 73 2998.655 seg.mc4 31 3557.425 3-cluster better.\n4.3 Describing clusters 1 2 3 4 5 6 7 8 9 10 \u0026gt; aggregate(seg.df, list(seg.mc$classification), mean) Group.1 age gender income kids ownHome subscribe 1 1 44.68018 0.5276074 52980.52 1.171779 0.8650307 0.2453988 2 2 38.02229 1.0000000 51550.98 1.422535 0.0000000 0.0000000 3 3 36.02187 0.0000000 45227.51 1.348485 0.0000000 0.0000000 \u0026gt; library(cluster) \u0026gt; clusplot(seg.df, seg.mc$classification, color = TRUE, shade = TRUE, + labels = 4, lines = 0, main = \u0026#34;Model-based cluster plot\u0026#34;) {% asset_image R_week2_model_based_describing_clusters_plot.png %}\nWeek2 Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 # install.packages (\u0026#34;readxl\u0026#34;) # install.packages (\u0026#34;psych\u0026#34;) # install.packages (\u0026#34;car\u0026#34;) # install.packages (\u0026#34;gpairs\u0026#34;) # install.packages (\u0026#34;grid\u0026#34;) # install.packages (\u0026#34;lattice\u0026#34;) # install.packages (\u0026#34;corrplot\u0026#34;) # install.packages (\u0026#34;gplots\u0026#34;) # install.packages (\u0026#34;mclust\u0026#34;) # install.packages (\u0026#34;cluster\u0026#34;) library(\u0026#34;readxl\u0026#34;) library(\u0026#34;psych\u0026#34;) library(\u0026#34;car\u0026#34;) library(\u0026#34;gpairs\u0026#34;) library(\u0026#34;grid\u0026#34;) library(\u0026#34;lattice\u0026#34;) library(\u0026#34;corrplot\u0026#34;) library(\u0026#34;gplots\u0026#34;) library(\u0026#34;mclust\u0026#34;) library(\u0026#34;cluster\u0026#34;) # 获取当前已加载文件的目录 file_dir \u0026lt;- dirname(parent.frame(2)$ofile) print(file_dir) # 将工作目录设置为当前已加载文件的目录 setwd(file_dir) seg.df \u0026lt;- read.csv(\u0026#34;Data_Segmentation.csv\u0026#34;, stringsAsFactors = TRUE) head(seg.df, n=8) seg.df$gender \u0026lt;- ifelse(seg.df$gender==\u0026#34;Male\u0026#34;,0,1) seg.df$ownHome \u0026lt;- ifelse(seg.df$ownHome == \u0026#34;ownNo\u0026#34;, 0,1) seg.df$subscribe \u0026lt;- ifelse(seg.df$subscribe == \u0026#34;subNo\u0026#34;, 0,1) head(seg.df) seg.df.sc \u0026lt;- seg.df seg.df.sc[, c(1,3,4)] \u0026lt;- scale(seg.df[, c(1,3,4)]) # We only need to standardize continuous variables. head(seg.df.sc) summary(seg.df.sc, digits = 2) seg.dist \u0026lt;- dist(seg.df.sc) as.matrix(seg.dist)[1:5, 1:5] seg.hc \u0026lt;- hclust(seg.dist, method=\u0026#34;complete\u0026#34;) seg.hc plot(seg.hc) plot(cut(as.dendrogram(seg.hc), h = 4)$lower[[1]]) seg.df[c(156, 152),] #similar seg.df[c(156, 183),] #less similar plot(seg.hc) rect.hclust(seg.hc, k=4, border = \u0026#34;red\u0026#34;) seg.hc.segment \u0026lt;- cutree(seg.hc, k=4) #membership vector for 4 groups table(seg.hc.segment) #counts library(cluster) clusplot(seg.df, seg.hc.segment, color = TRUE, #color the groups shade = TRUE, #shade the ellipses for group membership labels = 4, #label only the groups, not the individual points lines = 0, #omit distance lines between groups main = \u0026#34;Hierarchical cluster plot\u0026#34; # figure title ) aggregate(seg.df, list(seg.hc.segment), mean) boxplot(seg.df$income ~ seg.hc.segment, ylab = \u0026#34;Income\u0026#34;, xlab = \u0026#34;Cluster\u0026#34;) set.seed(96743) seg.k \u0026lt;- kmeans(seg.df.sc, centers = 4) #use standardized variables aggregate(seg.df, list(seg.k$cluster), mean) boxplot(seg.df$income ~ seg.k$cluster, ylab = \u0026#34;Income\u0026#34;, xlab = \u0026#34;Cluster\u0026#34;) clusplot(seg.df, seg.k$cluster, color = TRUE, shade = TRUE, labels = 4, lines = 0, main = \u0026#34;K_means cluster plot\u0026#34;, ) library(mclust) #install if needed seg.mc \u0026lt;- Mclust(seg.df.sc) summary(seg.mc) seg.mc4 \u0026lt;- Mclust(seg.df.sc, G =4) #specifying the number of clusters summary(seg.mc4) BIC(seg.mc, seg.mc4) aggregate(seg.df, list(seg.mc$classification), mean) library(cluster) clusplot(seg.df, seg.mc$classification, color = TRUE, shade = TRUE, labels = 4, lines = 0, main = \u0026#34;Model-based cluster plot\u0026#34;) ","date":"2024-03-15T00:00:00Z","permalink":"https://MyLoveES.github.io/p/rweek2-segmentation-code/","title":"R[week2] Segmentation Code"},{"content":" R: 4.3.2 (2023-10-31)\nR studio: 2023.12.1+402 (2023.12.1+402)\n识别、管理客户的差异性\nAgenda\nFramework Segmentation, Targeting, and Positioning (STP) Approach Analysis tools Cluster analysis for segmentation Factor analysis for developing perceptual maps Perceptual Maps\n1 2 3 在市场营销中，因子分析用于开发感知地图（Perceptual Maps）。感知地图是一种可视化工具，用于显示产品或品牌在消费者心中的位置，并显示与之相关的竞争对手。感知地图通常是二维图表，其中轴表示与产品相关的关键属性或特征，而各个产品或品牌则在图表上相应位置上显示。 因子分析在开发感知地图中的作用是帮助确定应该考虑哪些因素来区分产品或品牌之间的不同。它通过分析数据集中的变量之间的关系，确定潜在的因素或维度。这些因素或维度可以代表消费者在评估产品或品牌时所关注的重要特征。因子分析可以帮助识别潜在的共同因素，从而为感知地图的构建提供指导。 SWOT\nSWOT分析（SWOT analysis）是一种用于评估组织、项目或个人的内部优势（Strengths）、内部劣势（Weaknesses）、外部机会（Opportunities）和外部威胁（Threats）的方法。这种分析通常用于制定战略计划、决策制定和问题解决过程中。\n优势（Strengths）：组织或项目内部的积极因素和优势。这些可以包括资源、技能、声誉、市场地位等方面的优势。 劣势（Weaknesses）：组织或项目内部的负面因素和劣势。这些可能是缺乏某些资源、技能不足、管理问题等内部挑战。 机会（Opportunities）：外部环境中有利于组织或项目发展的因素。这些可能是市场趋势、技术进步、竞争对手的弱点等。 威胁（Threats）：外部环境中可能对组织或项目构成风险和威胁的因素。这些可能是竞争加剧、法规变化、市场需求下降等。 SWOT分析的目的是帮助组织或个人识别其所面临的内部和外部因素，并基于这些因素制定战略，利用优势应对机会，克服劣势避免威胁。这种分析通常以四个象限的形式呈现，将优势和劣势放置在水平轴上，将机会和威胁放置在垂直轴上，以便更直观地了解它们之间的关系。\n因子分析（Factor analysis）：\n因子分析是一种统计技术，用于理解观察到的变量之间的潜在结构或模式。它试图找到一组潜在的因素（或维度），这些因素可以解释变量之间的共同变异性。在因子分析中，变量被假设是由一组潜在因素所影响，而这些潜在因素无法直接观察到。因子分析可用于简化数据集、识别变量之间的模式以及探索变量之间的关系。\n聚类分析（Cluster analysis）：\n聚类分析是一种无监督学习方法，用于将数据集中的对象分成相似的组或簇。在聚类分析中，目标是通过测量对象之间的相似性，将它们划分为具有高内部一致性和低组间一致性的组。聚类分析的结果是一组簇，每个簇内的对象彼此相似，而不同簇之间的对象则具有较大的差异。聚类分析可用于发现数据集中的隐藏模式、识别潜在的群组或者简化复杂的数据集。\nGE矩阵（GE Matrix）：\nGE矩阵，也称为General Electric矩阵或GE多重业务矩阵，是一种战略规划工具，用于评估公司的业务组合并决定资源分配。该矩阵通常由两个维度组成：市场吸引力和竞争力。市场吸引力通常用于评估行业的吸引力程度，而竞争力则用于评估公司在特定市场上的竞争力。通过将不同的业务单元或产品定位在GE矩阵的不同象限中，管理者可以更好地了解哪些业务单元需要进一步发展、保持、撤退或收缩，并制定相应的战略计划。\n{% asset_image week2_ge_matrix.png %}\n欧几里得距离 Euclidean Distance\n欧几里得距离（Euclidean distance）是空间中两点之间的直线距离，通常用于衡量多维空间中的点之间的相似性或差异性。其含义是基于欧几里得几何学中的距离定义而来，即两点之间的直线最短路径的长度。\n$$ d_e = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2} $${% asset_image week2_euclidean_distance.png %}\n标准化 standardization\n标准化的过程是将原始数据进行平移和缩放，使得数据的均值变为0，标准差变为1。这有助于消除不同变量之间的尺度差异，使得它们可以更好地进行比较和分析。\n$$ z = \\frac{x - \\mu}{\\sigma} $$其中：\nz：标准化后的值。 x：原始数据点的值。 μ：数据的均值，表示整个数据集的中心位置。 σ：数据的标准差，表示数据分布的离散程度或波动程度。 $$ \\sigma = \\sqrt{\\frac{\\sum_{i=1}^{n}(x_i - \\mu)^2}{n}} $${% asset_image week2_standardization.png %}\n层次聚类 Hierarchical clustering\n单链接聚类（Single Linkage Clustering）：也称为最小距离法，它将两个簇之间的最近点之间的距离作为簇间距离的度量。\n完全链接聚类（Complete Linkage Clustering）：也称为最大距离法，它将两个簇中最远点之间的距离作为簇间距离的度量。\n平均链接聚类（Average Linkage Clustering）（Centroid Method）：它将两个簇中所有点对之间的平均距离作为簇间距离的度量。\n平均-平均链接聚类（Average-Average Linkage Clustering）：它结合了平均链接和平均-平均聚类的思想，利用两个簇中所有点对之间的平均距离作为簇间距离的度量。\n最大-最小平均链接聚类（Maximum-Minimum Average Linkage Clustering）：它结合了最大距离法和最小距离法的思想，将两个簇中最远点之间的距离和最近点之间的距离的平均值作为簇间距离的度量。\nWard Method：最小化簇内变异性或簇内点到质心的平方偏差和，它试图通过考虑每个点与其所属簇的质心之间的差异的平方和来最小化合并簇时的方差。\nK-means clustering\nK-means clustering 是一种基于距离的聚类方法，其目标是将数据点分成预先指定数量的簇，使得簇内的数据点相互之间的距离尽可能小，而不同簇之间的距离尽可能大。K-means 算法首先随机选择 k 个初始质心，然后迭代地将数据点分配到最近的质心，并更新质心的位置，直到达到收敛条件。K-means 的主要缺点是对于不同大小、密度和形状的簇表现不佳，并且对于初始质心的选择敏感。然而，K-means 是一种高效的算法，通常在大规模数据集上表现良好。\nModel-based clustering\nModel-based clustering 是一种基于概率模型的聚类方法，其主要思想是假设数据是由多个潜在的概率分布生成的混合物。这些分布通常是高斯分布或其变体，每个分布代表一个簇。模型基于这些概率分布的参数进行参数化，然后使用EM算法或其他优化方法对这些参数进行估计。在聚类时，通过最大化数据的似然性或最小化一个适当的损失函数来确定数据点的分配和模型参数。Model-based clustering 的优点包括能够处理不规则形状的簇以及能够估计每个数据点属于每个簇的概率。\n","date":"2024-03-15T00:00:00Z","permalink":"https://MyLoveES.github.io/p/rweek2-segmentation-lecture/","title":"R[week2] Segmentation Lecture"},{"content":"本篇主要讲了利用R语言进行数据简单处理分析，和图表的绘制。\nR: 4.3.2 (2023-10-31)\nR studio: 2023.12.1+402 (2023.12.1+402)\ninstall and import libraries\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # install.packages (\u0026#34;readxl\u0026#34;) # install.packages (\u0026#34;psych\u0026#34;) # install.packages (\u0026#34;car\u0026#34;) # install.packages (\u0026#34;gpairs\u0026#34;) # install.packages (\u0026#34;grid\u0026#34;) # install.packages (\u0026#34;lattice\u0026#34;) # install.packages (\u0026#34;corrplot\u0026#34;) # install.packages (\u0026#34;gplots\u0026#34;) library(\u0026#34;readxl\u0026#34;) library(\u0026#34;psych\u0026#34;) library(\u0026#34;car\u0026#34;) library(\u0026#34;gpairs\u0026#34;) library(\u0026#34;grid\u0026#34;) library(\u0026#34;lattice\u0026#34;) library(\u0026#34;corrplot\u0026#34;) library(\u0026#34;gplots\u0026#34;) 一、Data 1. vectors 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 导入数据 \u0026gt; store.df \u0026lt;- read.csv(\u0026#34;Data_Descriptive.csv\u0026#34;, stringsAsFactors=TRUE) \u0026gt; str(store.df) \u0026#39;data.frame\u0026#39;:\t2080 obs. of 10 variables: $ storeNum: int 101 101 101 101 101 101 101 101 101 101 ... $ Year : int 1 1 1 1 1 1 1 1 1 1 ... $ Week : int 1 2 3 4 5 6 7 8 9 10 ... $ p1sales : int 127 137 156 117 138 115 116 106 116 145 ... $ p2sales : int 106 105 97 106 100 127 90 126 94 91 ... $ p1price : num 2.29 2.49 2.99 2.99 2.49 2.79 2.99 2.99 2.29 2.49 ... $ p2price : num 2.29 2.49 2.99 3.19 2.59 2.49 3.19 2.29 2.29 2.99 ... $ p1prom : int 0 0 1 0 0 0 0 0 0 0 ... $ p2prom : int 0 0 0 0 1 0 0 0 0 0 ... $ country : Factor w/ 7 levels \u0026#34;AU\u0026#34;,\u0026#34;BR\u0026#34;,\u0026#34;CN\u0026#34;,..: 7 7 7 7 7 7 7 7 7 7 ... # 从上面结果可以看出，csv中一共有2080个观察值（行），10个变量（列）。 # 其中country是factor类型的，一共有7个level。 2. Summarize a single variable 2.1 Discrete variables 1 2 3 \u0026gt; table(store.df$p1price) 2.19 2.29 2.49 2.79 2.99 395 444 423 443 375 注⚠️：table() 函数 用来创建频数表（Frequency Table）的函数。它可以用于统计一组数据中各个值出现的次数，并将结果以表格的形式呈现。 它也可以提供多维的结果，比如：\n1 2 3 4 5 6 7 8 9 10 11 \u0026gt; df \u0026lt;- data.frame( + gender = c(\u0026#34;Male\u0026#34;, \u0026#34;Female\u0026#34;, \u0026#34;Female\u0026#34;, \u0026#34;Male\u0026#34;, \u0026#34;Male\u0026#34;, \u0026#34;Female\u0026#34;, \u0026#34;Male\u0026#34;, \u0026#34;Female\u0026#34;, \u0026#34;Female\u0026#34;, \u0026#34;Male\u0026#34;), + age_group = c(\u0026#34;Young\u0026#34;, \u0026#34;Young\u0026#34;, \u0026#34;Old\u0026#34;, \u0026#34;Young\u0026#34;, \u0026#34;Old\u0026#34;, \u0026#34;Young\u0026#34;, \u0026#34;Old\u0026#34;, \u0026#34;Old\u0026#34;, \u0026#34;Young\u0026#34;, \u0026#34;Young\u0026#34;) + ) # 创建多维频数表 \u0026gt; table(df$gender, df$age_group) Young Old Female 3 2 Male 4 1 除table()外，还有其他类似的函数以及功能：\n函数 描述 prop.table() 计算表格中每个单元格的比例。 xtabs() 创建频数表，支持多维数据和公式语法。 addmargins() 向表格中添加边际总计。 ftable() 创建扁平的表格，更容易查看和理解。 summary() 通常用于摘要统计，显示每个因子变量的数量，以及数值型变量的摘要统计量。 dplyr::count() 计算数据框中每个类别的频数，并返回包含频数的数据框。 psych::describe() 生成包含多个统计指标的摘要统计信息，如均值、标准差、最小值、最大值等。 题外话结束，继续前面的数据分析\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026gt; p1.table \u0026lt;- table(store.df$p1price) \u0026gt; p1.table 2.19 2.29 2.49 2.79 2.99 395 444 423 443 375 # talbe()返回的是一个table类型的结果 \u0026gt; str(p1.table) \u0026#39;table\u0026#39; int [1:5(1d)] 395 444 423 443 375 - attr(*, \u0026#34;dimnames\u0026#34;)=List of 1 ..$ : chr [1:5] \u0026#34;2.19\u0026#34; \u0026#34;2.29\u0026#34; \u0026#34;2.49\u0026#34; \u0026#34;2.79\u0026#34; ... # 转为图 \u0026gt; plot(p1.table) {% asset_image plot1.png %}\n注⚠️：plot()函数\n用于创建各种类型的图形的基本函数之一。它可以用于绘制散点图、线图、柱状图、箱线图等等。后面会有更多的用法，就不在此处列举了。 plot()会自适应地采用一种图表来展示数据，但我们同样可以根据需要调整。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 二维table，获取每个价位上的促销频数 \u0026gt; table(store.df$p1price, store.df$p1prom) 0 1 2.19 354 41 2.29 398 46 2.49 381 42 2.79 396 47 2.99 343 32 # 计算product 1在每个price上的促销比例[prod1/(prod1+prod2)] \u0026gt; p1.table2 \u0026lt;- table(store.df$p1price, store.df$p1prom) \u0026gt; p1.table2[ ,2] / (p1.table2[ ,1] + p1.table2[ ,2]) 2.19 2.29 2.49 2.79 2.99 0.10379747 0.10360360 0.09929078 0.10609481 0.08533333 2.2 Continuous variables 注⚠️：Continuous variables VS Discrete variable\n连续变量，指的是可以在一个范围内取任意值的变量，而不是一组离散的值。这种变量可以在某个范围内连续地取值，可以是任何数值。\n比如一个人的身高、体重、年龄等就是连续变量。因为身高、体重、年龄都可以在一个范围内取任何实数值，而不仅仅是某些特定的离散数值。另外，温度、时间等也是连续变量，因为它们可以在一个连续的范围内取任何值。\n离散变量，是指只能取有限个数值的变量。 比如一个班级中学生的人数、一个箱子中的球的个数等都是离散变量，因为它们只能取整数值，而不能取连续的任意数值。\n对于连续变量，根据其分布来总结数据更有帮助。最常见的方法是使用数学函数来描述数据的范围、中心、集中或分散的程度，以及可能感兴趣的具体点(如第90百分位):\nDescribe Function Value Extremes[极值] min(x) Min max(x) Max Central trendency[中心趋势] mean(x) Arithmetic mean median(x) Mid Dispersion[离散度] var(x) Variance around the mean sd(x) Standard deviaion(sqrt(var(x))) IQR(x) Interquartile range, 75th-25th percentile mad(x) Median absolute deviation Points[观察值/数据点] quantile(x, probs=c(\u0026hellip;)) Percentiles 方差 var(x) 描述数据分布离散程度的一种统计量。它衡量了数据集中每个数据点与数据集平均值之间的差异程度\n标准差 sd(x) 方差的平方根\nIQR(x)\n是统计学中用于度量数据分布的差异性的一种方法。它是第三四分位数（Q3）与第一四分位数（Q1）之间的距离，通常用来描述数据的离散程度。与方差和标准差不同，IQR不受极端值（离群值）的影响，因此在某些情况下，IQR更适合用来度量数据的离散程度。 IQR的优势在于它提供了一个对数据集的中间50%数据的度量，而不受极端值的影响。这使得它在处理偏斜或包含极端值的数据集时更为稳健。在描述偏态分布或存在离群值的数据时，IQR通常比标准差更具有代表性。 在统计学和数据分析中，通常会将IQR与箱线图结合使用。箱线图显示了数据的四分位数范围以及任何离群值的存在，这使得可以直观地了解数据的分布情况，并且可以通过比较不同组的箱线图来识别任何差异。\nmad(x) 同IQR，简单比较一下这两者：\nIQR（Interquartile Range）： 定义：IQR是数据集中第三四分位数（Q3）和第一四分位数（Q1）之间的距离。 计算方法：IQR = Q3 - Q1，其中Q3是数据集中75th百分位数，Q1是数据集中25th百分位数。\n用途：IQR通常用于识别数据中的离群值，因为它提供了数据中间50%的范围，并且对于偏斜分布的数据比标准差更鲁棒。\nMAD（Median Absolute Deviation）： 定义：MAD是数据点与数据集中位数的绝对偏差的中位数。 计算方法：首先计算每个数据点与数据集中位数的绝对偏差，然后取这些绝对偏差的中位数。\n用途：MAD也用于度量数据的离散程度，并且在处理离群值时更具有鲁棒性，因为它不受极端值的影响，类似于IQR。\nIQR 和 MAD 相比： 计算方法：IQR是基于四分位数计算的，而MAD是基于中位数计算的。 鲁棒性：IQR和MAD都是鲁棒的统计量，对异常值的影响较小，但在某些情况下，MAD可能更为鲁棒，特别是当数据集包含大量离群值时。 解释：IQR更容易解释，因为它代表了数据集中间50%的范围，而MAD则表示数据点与中位数的典型偏差。 常见用途：IQR和MAD在异常值检测和数据分析中都很常见，但在不同的背景下可能有不同的应用场景。\nquantile(x, probs=c(\u0026hellip;)) 通常用于计算数据集 x 的分位数。其中 probs 参数是一个包含所需分位数的百分比的向量。\nx：包含数据的向量或数据框。\nprobs：一个包含所需分位数的百分比的向量，例如，probs = c(0.25, 0.5, 0.75) 表示计算第一四分位数（25th百分位数）、中位数（50th百分位数）、第三四分位数（75th百分位数）。\n返回值：返回一个包含计算得到的分位数的向量，其顺序与 probs 中指定的顺序相对应。\n1 2 3 4 5 6 7 8 9 10 # 创建一个示例数据集 \u0026gt; data \u0026lt;- c(10, 15, 20, 25, 30, 35, 40, 45, 50) # 计算第一四分位数、中位数、第三四分位数 \u0026gt; quantiles \u0026lt;- quantile(data, probs = c(0.25, 0.5, 0.75)) # 打印结果 \u0026gt; print(quantiles) 25% 50% 75% 20 30 40 题外话结束。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 \u0026gt; min(store.df$p1sales) [1] 73 \u0026gt; max(store.df$p1sales) [1] 263 \u0026gt; mean(store.df$p1prom) [1] 0.1 \u0026gt; median(store.df$p2sales) [1] 96 \u0026gt; var(store.df$p1sales) [1] 805.0044 \u0026gt; sd(store.df$p1sales) [1] 28.3726 \u0026gt; IQR(store.df$p1sales) [1] 37 \u0026gt; mad(store.df$p1sales) [1] 26.6868 \u0026gt; quantile(store.df$p1sales, probs = c(0.25, 0.5, 0.75)) 25% 50% 75% 113 129 150 For skewed and asymmetric distributions that are common in marketing, such as unit sales or household income, the arithmetic mean() and standard deviation sd() may be misleading; in those cases, the median() and the interquartile range IQR() (the range of the middle 50% of data) are often used to summarize a distribution.\n对于市场营销中常见的倾斜和不对称分布，例如单位销售或家庭收入，算术平均值()和标准差sd()可能会产生误导;在这些情况下，中位数()和四分位数间距IQR()(中间50%数据的范围)通常用于总结分布。\n因为在偏斜和不对称分布的情况下，数据中可能存在极端值（离群值），这些值对于算术平均值和标准差的影响较大，导致这两个统计量可能不太准确地反映整体数据的特征。\n算术平均值的问题： 偏斜分布中的极端值会对平均值产生较大的影响，使其偏离数据集的中心趋势。这导致平均值可能不再是数据的典型中心度量。 标准差的问题： 标准差对极端值非常敏感，因为它是基于平方差的，这会放大离群值的影响。这样，标准差可能高估了数据的真实分散程度。 相比之下，中位数是数据集中的中间值，不受极端值的影响，因此更能反映数据的中心趋势。四分位数间距（IQR）是数据中间50%的范围，它也对离群值具有一定的鲁棒性，因此更适合在偏斜分布中度量数据的分散程度。\n综合起来，为了更准确地摘要和理解偏斜分布的数据特征，使用中位数和四分位数间距通常是更合适的选择。这样的度量方法更具有鲁棒性，能够提供更稳健、不受极端值干扰的数据摘要。\n1 2 3 4 5 6 7 8 9 10 11 \u0026gt; quantile(store.df$p1sales, probs = c(0.05, 0.95)) # central 90% data 5% 95% 93 184 \u0026gt; quantile(store.df$p1sales, probs = 0:10/10) 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% 73.0 100.0 109.0 117.0 122.6 129.0 136.0 145.0 156.0 171.0 263.0o \u0026gt; quantile(store.df$p1sales, probs = seq(from=0, to=1, by=0.1)) 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% 73.0 100.0 109.0 117.0 122.6 129.0 136.0 145.0 156.0 171.0 263.0 Suppose we wanted a summary of the sales for product 1 and product 2 based on their median and interquartile range. We might assemble these summary statistics into a data frame that is easier to read than the one-line- at-a-time output above. We create a data frame to hold our summary statistics. We name the columns and rows and fill in the cells with function values:\n假设我们想根据产品1和产品2的中位数和四分位数范围汇总它们的销售额。我们可以将这些汇总统计信息组合成一个数据帧，这样比上面的一行一行的输出更容易阅读。我们创建一个数据框架来保存汇总统计数据。我们为列和行命名，并用函数值填充单元格:\n1 2 3 4 5 6 7 8 mysummary.df \u0026lt;- data.frame(matrix(NA, nrow=2, ncol=2)) # 2 by 2 empty matrix names(mysummary.df) \u0026lt;- c(\u0026#34;Median Sales\u0026#34;, \u0026#34;IQR\u0026#34;) # name columns rownames(mysummary.df) \u0026lt;- c(\u0026#34;Product 1\u0026#34;, \u0026#34;Product 2\u0026#34;) # name rows mysummary.df[\u0026#34;Product 1\u0026#34;, \u0026#34;Median Sales\u0026#34;] \u0026lt;- median(store.df$p1sales) mysummary.df[\u0026#34;Product 2\u0026#34;, \u0026#34;Median Sales\u0026#34;] \u0026lt;- median(store.df$p2sales) mysummary.df[\u0026#34;Product 1\u0026#34;, \u0026#34;IQR\u0026#34;] \u0026lt;- IQR(store.df$p1sales) mysummary.df[\u0026#34;Product 2\u0026#34;, \u0026#34;IQR\u0026#34;] \u0026lt;- IQR(store.df$p2sales) mysummary.df 3 Summarize data frames 3.1 summary() 有事没事，summary一下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \u0026gt; summary(store.df) storeNum Year Week p1sales p2sales p1price p2price p1prom p2prom country Min. :101.0 Min. :1.0 Min. : 1.00 Min. : 73 Min. : 51.0 Min. :2.190 Min. :2.29 Min. :0.0 Min. :0.0000 AU:104 1st Qu.:105.8 1st Qu.:1.0 1st Qu.:13.75 1st Qu.:113 1st Qu.: 84.0 1st Qu.:2.290 1st Qu.:2.49 1st Qu.:0.0 1st Qu.:0.0000 BR:208 Median :110.5 Median :1.5 Median :26.50 Median :129 Median : 96.0 Median :2.490 Median :2.59 Median :0.0 Median :0.0000 CN:208 Mean :110.5 Mean :1.5 Mean :26.50 Mean :133 Mean :100.2 Mean :2.544 Mean :2.70 Mean :0.1 Mean :0.1385 DE:520 3rd Qu.:115.2 3rd Qu.:2.0 3rd Qu.:39.25 3rd Qu.:150 3rd Qu.:113.0 3rd Qu.:2.790 3rd Qu.:2.99 3rd Qu.:0.0 3rd Qu.:0.0000 GB:312 Max. :120.0 Max. :2.0 Max. :52.00 Max. :263 Max. :225.0 Max. :2.990 Max. :3.19 Max. :1.0 Max. :1.0000 JP:416 US:312 \u0026gt; summary(store.df$Year) Min. 1st Qu. Median Mean 3rd Qu. Max. 1.0 1.0 1.5 1.5 2.0 2.0 \u0026gt; summary(store.df, digits = 2) # 保留小数点后两位 storeNum Year Week p1sales p2sales p1price p2price p1prom p2prom country Min. :101 Min. :1.0 Min. : 1 Min. : 73 Min. : 51 Min. :2.2 Min. :2.3 Min. :0.0 Min. :0.00 AU:104 1st Qu.:106 1st Qu.:1.0 1st Qu.:14 1st Qu.:113 1st Qu.: 84 1st Qu.:2.3 1st Qu.:2.5 1st Qu.:0.0 1st Qu.:0.00 BR:208 Median :110 Median :1.5 Median :26 Median :129 Median : 96 Median :2.5 Median :2.6 Median :0.0 Median :0.00 CN:208 Mean :110 Mean :1.5 Mean :26 Mean :133 Mean :100 Mean :2.5 Mean :2.7 Mean :0.1 Mean :0.14 DE:520 3rd Qu.:115 3rd Qu.:2.0 3rd Qu.:39 3rd Qu.:150 3rd Qu.:113 3rd Qu.:2.8 3rd Qu.:3.0 3rd Qu.:0.0 3rd Qu.:0.00 GB:312 Max. :120 Max. :2.0 Max. :52 Max. :263 Max. :225 Max. :3.0 Max. :3.2 Max. :1.0 Max. :1.00 JP:416 US:312 在统计学中，\u0026ldquo;1st Qu\u0026rdquo; 和 \u0026ldquo;3rd Qu\u0026rdquo; 分别代表第一四分位数和第三四分位数。四分位数是将数据集分为四等份的值，它们用于描述数据分布的位置和分散程度。\n第一四分位数（1st Qu） 是数据集中所有数值排序后第25%位置的值，也就是数据中的较小的四分之一。\n第三四分位数（3rd Qu） 是数据集中所有数值排序后第75%位置的值，也就是数据中的较大的四分之三。\n3.2 describe() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 \u0026gt; library(psych) # install if needed \u0026gt; describe(store.df) vars n mean sd median trimmed mad min max range skew kurtosis se storeNum 1 2080 110.50 5.77 110.50 110.50 7.41 101.00 120.00 19.0 0.00 -1.21 0.13 Year 2 2080 1.50 0.50 1.50 1.50 0.74 1.00 2.00 1.0 0.00 -2.00 0.01 Week 3 2080 26.50 15.01 26.50 26.50 19.27 1.00 52.00 51.0 0.00 -1.20 0.33 p1sales 4 2080 133.05 28.37 129.00 131.08 26.69 73.00 263.00 190.0 0.74 0.66 0.62 p2sales 5 2080 100.16 24.42 96.00 98.05 22.24 51.00 225.00 174.0 0.99 1.51 0.54 p1price 6 2080 2.54 0.29 2.49 2.53 0.44 2.19 2.99 0.8 0.28 -1.44 0.01 p2price 7 2080 2.70 0.33 2.59 2.69 0.44 2.29 3.19 0.9 0.32 -1.40 0.01 p1prom 8 2080 0.10 0.30 0.00 0.00 0.00 0.00 1.00 1.0 2.66 5.10 0.01 p2prom 9 2080 0.14 0.35 0.00 0.05 0.00 0.00 1.00 1.0 2.09 2.38 0.01 country* 10 2080 4.55 1.72 4.50 4.62 2.22 1.00 7.00 6.0 -0.29 -0.81 0.04 # vars: 方差 # n: 观测数量（样本数量） # mean: 均值 # sd: 标准差 # median: 中位数 # trimmed: 修剪均值（去除异常值后的均值） # mad: 中位数绝对偏差（Median Absolute Deviation） # min: 最小值 # max: 最大值 # range: 范围（最大值与最小值的差） # skew: 偏度（Skewness，衡量分布偏斜程度） # kurtosis: 峰度（Kurtosis，衡量分布的尖峰程度） # se: 标准误差（Standard Error，均值的估计标准差） \u0026gt; describe(store.df[,c(2, 4:9)]) vars n mean sd median trimmed mad min max range skew kurtosis se Year 1 2080 1.50 0.50 1.50 1.50 0.74 1.00 2.00 1.0 0.00 -2.00 0.01 p1sales 2 2080 133.05 28.37 129.00 131.08 26.69 73.00 263.00 190.0 0.74 0.66 0.62 p2sales 3 2080 100.16 24.42 96.00 98.05 22.24 51.00 225.00 174.0 0.99 1.51 0.54 p1price 4 2080 2.54 0.29 2.49 2.53 0.44 2.19 2.99 0.8 0.28 -1.44 0.01 p2price 5 2080 2.70 0.33 2.59 2.69 0.44 2.29 3.19 0.9 0.32 -1.40 0.01 p1prom 6 2080 0.10 0.30 0.00 0.00 0.00 0.00 1.00 1.0 2.66 5.10 0.01 p2prom 7 2080 0.14 0.35 0.00 0.05 0.00 0.00 1.00 1.0 2.09 2.38 0.01 trimmed trimmed 是指去除了数据中一定比例的极端值（通常是尾部的极端值）后计算得到的均值。修剪均值（trimmed mean）是一种对均值的修正，旨在减少极端值对均值的影响，从而更好地反映数据的中心趋势。\n通常情况下，修剪均值的计算方式是将数据集中的一定比例的最小值和最大值去掉，然后计算剩余数据的算术平均值。这个比例可以根据实际情况而定，常见的修剪比例包括去掉5%、10%甚至更多的极端值。\n修剪均值适用于数据集中存在明显的极端值或异常值的情况。与简单的算术平均值相比，修剪均值更加稳健，因为它对异常值的影响更小，更能够反映数据的典型中心位置。然而，需要注意的是，修剪均值可能会导致一定程度上的信息损失，尤其是当数据中的极端值具有特殊意义或重要性时。\nmad MAD（Median Absolute Deviation，中位数绝对偏差）是一种用于衡量数据集散布度的统计量，它衡量了数据点与数据集中位数之间的典型偏差。\n具体而言，MAD是数据集中所有数据点与数据集中位数的绝对偏差的中位数。\nMAD相对于标准差的优势在于它对极端值（离群值）具有较强的鲁棒性。因为MAD是基于绝对偏差的中位数计算的，它不受极端值的影响，更能够准确地描述数据的离散程度，尤其是在存在离群值的情况下。\nMAD常用于金融领域和其他领域的数据分析中，特别是在需要考虑数据的异常值对结果的影响时。\nSkewness（偏度） 是描述数据分布形态偏斜程度的统计量。它衡量了数据分布相对于其平均值的不对称性，即数据集在平均值两侧的分布是否对称。\n正偏（Positive Skewness）：数据分布向右偏移，尾部向右延伸，即数据集右侧的尾部较长，平均值右侧的数据点较多，形成一个长尾。正偏分布意味着数据集中存在较多较大的值。\n负偏（Negative Skewness）：数据分布向左偏移，尾部向左延伸，即数据集左侧的尾部较长，平均值左侧的数据点较多，形成一个长尾。负偏分布意味着数据集中存在较多较小的值。\n偏度为0表示数据分布相对对称，大于0表示正偏，小于0表示负偏。偏度的绝对值越大，偏斜程度越明显。\n偏度是了解数据分布形态的重要指标，特别是在做数据分析和模型建立时，它可以帮助我们判断数据集是否需要进行对称化处理，以满足建模的假设条件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 对称化处理是指通过某种方法或技术使数据分布更加对称的过程。在统计学和数据分析中，对称化处理通常用于调整数据的分布形态，使其更接近对称分布，以满足统计方法或模型的假设条件。 常见的对称化处理方法包括： 1. 对数转换（Log Transformation）：对数转换是通过取数据的对数来减小数据的偏度。这种转换通常用于处理右偏分布的数据，将其转换为更接近对称分布的形式。 2. 方根转换（Square Root Transformation）：方根转换是通过取数据的平方根来减小数据的偏度。类似于对数转换，它也可以用于调整右偏分布的数据。 3. 幂转换（Power Transformation）：幂转换是通过取数据的某个幂次方来调整数据的分布形态。常见的幂次方包括0.5（平方根转换）、0.33、0.25等，具体选择取决于数据的分布特征。 4. 标准化（Standardization）：标准化是将数据转换为均值为0，标准差为1的标准正态分布。虽然标准化不会改变数据的分布形态，但它可以将数据转换为具有固定均值和方差的形式，有助于在一些情况下比较不同尺度的变量。 对称化处理的目的是使数据更符合统计模型的假设，例如线性模型对数据的正态性假设。通过对数据进行对称化处理，可以提高统计分析的准确性和稳健性，并提高模型的拟合效果。 Kurtosis（峰度） 是描述数据分布峰态（尖峰程度）的统计量。它衡量了数据分布中数据点在均值附近聚集的程度，即数据集的尖峰度。\n正峰（Positive Kurtosis）：数据分布的峰态较高且尖锐，尾部较长。正峰分布表明数据集中的数据点在均值附近聚集得更为密集，同时存在更多的极端值，尾部更长。\n负峰（Negative Kurtosis）：数据分布的峰态较低且平缓，尾部较短。负峰分布表明数据集中的数据点分布更为扁平，相对更加分散。\n正常的峰度为3。大于3的峰度表示分布尖峭（尖峰），而小于3的峰度表示分布平缓（扁平）。与偏度一样，峰度也是了解数据分布形态的重要指标，特别是在建模时需要考虑数据集的峰态特征。\nse se 代表的是标准误差（Standard Error）。标准误差是对样本统计量的抽样分布的离散程度进行估计的一种度量，用于衡量样本统计量与总体参数之间的差异。\n在统计学中，标准误差通常用于估计样本统计量的抽样分布的离散程度，例如样本均值的标准误差用于估计样本均值的抽样分布的离散程度。标准误差越小，表示样本统计量更接近于总体参数的真值；标准误差越大，表示样本统计量与总体参数之间的差异越大。\n标准误差的应用非常广泛，特别是在统计推断和假设检验中，它常用于计算置信区间、假设检验的统计量（如 t 统计量、z 统计量）\n检查数据集的通用方法 read.csv 读取数据 转换为data frame，给定合适的名称 通过dim()检查是否是预期的行数和列数 1 2 \u0026gt; dim(store.df) [1] 2080 10 使用head()、tail()检查前几行和末尾几行，确保开头标题行和末尾空白行没有包含在内 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \u0026gt; head(store.df) storeNum Year Week p1sales p2sales p1price p2price p1prom p2prom country 1 101 1 1 127 106 2.29 2.29 0 0 US 2 101 1 2 137 105 2.49 2.49 0 0 US 3 101 1 3 156 97 2.99 2.99 1 0 US 4 101 1 4 117 106 2.99 3.19 0 0 US 5 101 1 5 138 100 2.49 2.59 0 1 US 6 101 1 6 115 127 2.79 2.49 0 0 US \u0026gt; tail(store.df) storeNum Year Week p1sales p2sales p1price p2price p1prom p2prom country 2075 120 2 47 140 104 2.49 2.59 0 0 CN 2076 120 2 48 150 88 2.49 2.99 0 0 CN 2077 120 2 49 121 97 2.49 2.59 0 0 CN 2078 120 2 50 91 124 2.99 2.29 0 0 CN 2079 120 2 51 83 115 2.99 2.29 0 0 CN 2080 120 2 52 136 99 2.49 2.49 0 0 CN 使用Some()随机检查几行 1 2 3 4 5 6 7 8 9 10 11 12 \u0026gt; some(store.df) storeNum Year Week p1sales p2sales p1price p2price p1prom p2prom country 324 104 1 12 112 99 2.29 2.29 0 0 DE 644 107 1 20 131 92 2.49 2.59 0 0 DE 1032 110 2 44 211 70 2.19 3.19 0 1 GB 1183 112 1 39 155 80 2.29 2.59 1 0 BR 1274 113 1 26 121 117 2.29 2.29 0 0 BR 1518 115 2 10 157 82 2.49 3.19 0 0 JP 1525 115 2 17 167 72 2.49 3.19 0 0 JP 1565 116 1 5 186 63 2.19 3.19 0 0 JP 1974 119 2 50 144 111 2.99 3.19 0 0 CN 2036 120 2 8 125 103 2.79 2.99 0 0 CN str()检查data frame架构，确保变量的类型和值是合适的，尤其是factor 1 2 3 4 5 6 7 8 9 10 11 12 \u0026gt; str(store.df) \u0026#39;data.frame\u0026#39;:\t2080 obs. of 10 variables: $ storeNum: int 101 101 101 101 101 101 101 101 101 101 ... $ Year : int 1 1 1 1 1 1 1 1 1 1 ... $ Week : int 1 2 3 4 5 6 7 8 9 10 ... $ p1sales : int 127 137 156 117 138 115 116 106 116 145 ... $ p2sales : int 106 105 97 106 100 127 90 126 94 91 ... $ p1price : num 2.29 2.49 2.99 2.99 2.49 2.79 2.99 2.99 2.29 2.49 ... $ p2price : num 2.29 2.49 2.99 3.19 2.59 2.49 3.19 2.29 2.29 2.99 ... $ p1prom : int 0 0 1 0 0 0 0 0 0 0 ... $ p2prom : int 0 0 0 0 1 0 0 0 0 0 ... $ country : Factor w/ 7 levels \u0026#34;AU\u0026#34;,\u0026#34;BR\u0026#34;,\u0026#34;CN\u0026#34;,..: 7 7 7 7 7 7 7 7 7 7 ... 7.summary()查看预期外的值，尤其是Min/Max\n1 2 3 4 5 6 7 8 9 \u0026gt; summary(store.df) storeNum Year Week p1sales p2sales p1price p2price p1prom p2prom country Min. :101.0 Min. :1.0 Min. : 1.00 Min. : 73 Min. : 51.0 Min. :2.190 Min. :2.29 Min. :0.0 Min. :0.0000 AU:104 1st Qu.:105.8 1st Qu.:1.0 1st Qu.:13.75 1st Qu.:113 1st Qu.: 84.0 1st Qu.:2.290 1st Qu.:2.49 1st Qu.:0.0 1st Qu.:0.0000 BR:208 Median :110.5 Median :1.5 Median :26.50 Median :129 Median : 96.0 Median :2.490 Median :2.59 Median :0.0 Median :0.0000 CN:208 Mean :110.5 Mean :1.5 Mean :26.50 Mean :133 Mean :100.2 Mean :2.544 Mean :2.70 Mean :0.1 Mean :0.1385 DE:520 3rd Qu.:115.2 3rd Qu.:2.0 3rd Qu.:39.25 3rd Qu.:150 3rd Qu.:113.0 3rd Qu.:2.790 3rd Qu.:2.99 3rd Qu.:0.0 3rd Qu.:0.0000 GB:312 Max. :120.0 Max. :2.0 Max. :52.00 Max. :263 Max. :225.0 Max. :2.990 Max. :3.19 Max. :1.0 Max. :1.0000 JP:416 US:312 describe()查看各个item的n是否相同，并且检查trimmed和skew 1 2 3 4 5 6 7 8 9 10 11 12 \u0026gt; describe(store.df) vars n mean sd median trimmed mad min max range skew kurtosis se storeNum 1 2080 110.50 5.77 110.50 110.50 7.41 101.00 120.00 19.0 0.00 -1.21 0.13 Year 2 2080 1.50 0.50 1.50 1.50 0.74 1.00 2.00 1.0 0.00 -2.00 0.01 Week 3 2080 26.50 15.01 26.50 26.50 19.27 1.00 52.00 51.0 0.00 -1.20 0.33 p1sales 4 2080 133.05 28.37 129.00 131.08 26.69 73.00 263.00 190.0 0.74 0.66 0.62 p2sales 5 2080 100.16 24.42 96.00 98.05 22.24 51.00 225.00 174.0 0.99 1.51 0.54 p1price 6 2080 2.54 0.29 2.49 2.53 0.44 2.19 2.99 0.8 0.28 -1.44 0.01 p2price 7 2080 2.70 0.33 2.59 2.69 0.44 2.29 3.19 0.9 0.32 -1.40 0.01 p1prom 8 2080 0.10 0.30 0.00 0.00 0.00 0.00 1.00 1.0 2.66 5.10 0.01 p2prom 9 2080 0.14 0.35 0.00 0.05 0.00 0.00 1.00 1.0 2.09 2.38 0.01 country* 10 2080 4.55 1.72 4.50 4.62 2.22 1.00 7.00 6.0 -0.29 -0.81 0.04 番外 Trimmmed And Skewed Trimmed\n当数据集中存在极端值时，修剪均值可以更准确地反映数据的中心趋势。让我们通过一个具体的例子来说明这一点。\n假设我们有一个包含10个观测值的数据集：\n\\[ \\{2, 3, 4, 5, 6, 7, 8, 9, 10, 100\\} \\]这个数据集中有一个明显的极端值100。让我们计算一下这个数据集的简单算术平均值和修剪均值，并比较它们对数据中心趋势的反映。\n首先，计算简单算术平均值：\n$$ \\text{Mean} = \\frac{2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 + 100}{10} = 15.4 $$接下来，我们计算修剪均值，剔除最大和最小的10%的观测值（即剔除一个极端值100）：\n$$ \\text{Trimmed Mean} = \\frac{3 + 4 + 5 + 6 + 7 + 8 + 9 + 10}{8} = 6.875 $$从计算结果可以看出，简单算术平均值受到极端值100的影响，导致平均值达到了15.4，不够准确地反映了数据的中心趋势。而修剪均值剔除了极端值100后，得到的修剪均值为6.875，更接近数据的中心位置。\n因此，通过比较简单算术平均值和修剪均值，我们可以更准确地了解数据的中心趋势，尤其是在数据集中存在极端值的情况下。\nSkewed\n当数据集中存在极端值时，数据的分布可能会偏斜（skewed），修剪均值可以在一定程度上减少极端值对数据的影响，从而改善数据分布的偏斜情况。\n让我们继续使用上面的例子来说明修剪均值如何影响数据分布的偏斜情况。我们已经计算了简单算术平均值和修剪均值，现在我们将计算数据集的偏度（skewness），以了解修剪均值如何影响数据分布的偏斜性。\n在R语言中，我们可以使用 psych 包中的 describe() 函数来计算数据集的偏度。让我们来进行计算：\n1 2 3 4 5 6 7 8 9 # 安装并加载 psych 包 install.packages(\u0026#34;psych\u0026#34;) library(psych) # 创建数据集 data \u0026lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 100) # 计算数据集的偏度 describe(data)$skew 在这个例子中，数据集的偏度为4.07，这表明数据集是右偏（右偏的偏度值大于0）。右偏分布意味着数据集的尾部向右延伸，即数据集中存在极端值或较大的值。\n现在，我们剔除极端值100，并重新计算数据集的偏度：\n1 2 3 # 剔除极端值并重新计算偏度 trimmed_data \u0026lt;- data[data \u0026lt; 100] describe(trimmed_data)$skew 剔除极端值后，修剪后的数据集的偏度为0.32。与原始数据集相比，修剪后的数据集的偏度更接近于0，表明修剪均值有助于减少数据分布的偏斜性，使数据更接近于对称分布。\n因此，通过计算修剪均值和观察数据集的偏度，我们可以更好地理解极端值对数据分布的影响，以及修剪均值对于改善数据分布的偏斜性的作用。\n3.3 apply() 1 2 3 4 5 apply(x = DATA, MARGIN = MARGIN, FUN = FUNCTION) apply(store.df[2:9], MARGIN = 1, FUN = mean) apply(store.df[2:9], MARGIN = 2, FUN = mean) apply(store.df[2:9], MARGIN = c(1,2), FUN = mean) 在 apply() 函数中，MARGIN 参数指定了函数应该沿着哪个维度（行或列）应用。MARGIN 参数可以取以下值：\nMARGIN = 1：对每一行应用函数。 MARGIN = 2：对每一列应用函数。 MARGIN = c(1, 2)：同时对每一行和每一列应用函数。 在给定的例子中，我们使用了 apply() 函数来对数据集 store.df 中的子集进行函数的应用，子集包含列索引为2到9的数据。具体解释如下：\napply(store.df[2:9], MARGIN = 1, FUN = mean)： 这句代码意味着对数据集 store.df 中的每一行（沿着行的方向）的列索引为2到9的数据应用 mean() 函数，计算每一行的平均值。\napply(store.df[2:9], MARGIN = 2, FUN = mean)： 这句代码意味着对数据集 store.df 中的每一列（沿着列的方向）的行索引为2到9的数据应用 mean() 函数，计算每一列的平均值。\napply(store.df[2:9], MARGIN = c(1,2), FUN = mean)： 这句代码意味着同时对数据集 store.df 中的每一行和每一列的行列交叉点（即所有的单元格）的数据应用 mean() 函数，计算所有行和列的平均值。\n1 2 3 4 5 6 7 8 9 10 11 \u0026gt; apply(store.df[ 2:9], MARGIN = 2, FUN = mean) Year Week p1sales p2sales p1price p2price p1prom p2prom 1.5000000 26.5000000 133.0485577 100.1567308 2.5443750 2.6995192 0.1000000 0.1384615 \u0026gt; apply(store.df[ , 2:9], 2, sum) Year Week p1sales p2sales p1price p2price p1prom p2prom 3120.0 55120.0 276741.0 208326.0 5292.3 5615.0 208.0 288.0 \u0026gt; apply(store.df[ , 2:9], 2, sd) Year Week p1sales p2sales p1price p2price p1prom p2prom 0.5001202 15.0119401 28.3725990 24.4241905 0.2948819 0.3292181 0.3000721 0.3454668 4. Single variable visualisation 4.1 Histograms 柱状图 hist()\n1 hist(store.df$p1sales) {% asset_image hist1.png %}\n1 2 3 4 5 # 添加标题、x轴y轴说明 hist(store.df$p1sales, main = \u0026#34;Product 1 Weekly Sales Frequencies, All Stores\u0026#34;, xlab = \u0026#34;Product 1 Sales (Units)\u0026#34;, ylab = \u0026#34;Count\u0026#34;) {% asset_image hist2.png %}\n1 2 3 4 5 6 hist(store.df$p1sales, main = \u0026#34;Product 1 Weekly Sales Frequencies, All Stores\u0026#34;, xlab = \u0026#34;Product 1 Sales (Units)\u0026#34;, ylab = \u0026#34;Count\u0026#34;, breaks = 30, # more columns col = \u0026#34;lightblue\u0026#34; # colore the bars ) {% asset_image hist3.png %}\n1 2 3 4 5 6 7 8 hist(store.df$p1sales, main = \u0026#34;Product 1 Weekly Sales Frequencies, All Stores\u0026#34;, xlab = \u0026#34;Product 1 Sales (Units)\u0026#34;, ylab = \u0026#34;Count\u0026#34;, breaks = 30, col = \u0026#34;lightblue\u0026#34;, freq = FALSE, # means plot density, not counts xaxt=\u0026#34;n\u0026#34; # means x-axis tick mark is set to \u0026#34;none\u0026#34; ) {% asset_image hist4.png %}\nfreq\nfreq 参数用于控制直方图的纵轴显示的是频数（counts）还是密度（density）。当 freq 设置为 TRUE 时，默认值，纵轴显示的是频数，即每个分箱内数据的数量；当 freq 设置为 FALSE 时，纵轴显示的是密度，即每个分箱内数据的概率密度。\n概率密度是概率论中用于描述连续随机变量的概率分布的概念。它在数学上是一个非负实值函数，通常用于描述某个随机变量落在某个区间内的概率。\n对于一个连续随机变量，它可能取无穷多个可能的取值，因此我们不能像离散随机变量那样直接计算某个特定值的概率。相反，我们通常关注某个区间内的概率，这个区间越小，计算的概率越准确。概率密度函数（Probability Density Function，PDF）就是用来描述随机变量在某个区间内的概率密度的函数。\n在数学上，对于一个连续随机变量 X，其概率密度函数 f(x) 满足以下两个性质：\n非负性：对于所有的 x，有 f(x) ≥ 0。 总面积为1：整个定义域内的概率密度函数的积分等于1，即 ∫f(x)dx = 1。 因此，概率密度函数描述了随机变量落在某个区间内的概率密度，而不是具体的概率值。要计算某个区间内的概率，我们需要对概率密度函数在该区间上进行积分。 1 2 3 4 5 6 7 8 9 10 11 12 13 概率密度和频数之间的关系涉及到连续随机变量和离散随机变量的概率分布。下面分别讨论它们的关系： 1. 连续随机变量的概率密度和频数的关系： 对于连续随机变量，概率密度函数（PDF）描述了随机变量落在某个区间内的概率密度，而不是具体的概率值。概率密度函数在某个区间上的值越大，表示该区间内的数据更加密集，概率密度更高。 直方图是用于表示连续随机变量的分布的一种常用方法。在直方图中，将数据分成多个等宽的分箱，并统计每个分箱内数据的频数，然后通过归一化处理，得到每个分箱内的频率（即频数除以样本总数），从而得到一个近似的概率密度分布。 2. 离散随机变量的概率密度和频数的关系： 对于离散随机变量，概率质量函数（PMF）描述了随机变量取某个特定值的概率。每个可能的取值都有一个概率。 直方图同样可以用于表示离散随机变量的分布。在直方图中，每个柱子代表了随机变量取某个特定值的频数，频数表示了该值在样本中出现的次数。由于离散随机变量的取值是有限的，因此直方图中的柱子通常对应于每个可能的取值，每个柱子的高度表示了该值的频率。 总的来说，概率密度和频数都是用于描述随机变量的分布情况的概念，但在连续随机变量和离散随机变量的情境下有所不同。在连续情况下，我们使用概率密度函数描述概率分布，而在离散情况下，我们使用概率质量函数或频数来描述分布。 归一化\n归一化是一种数学或统计方法，用于将数据转换为统一的尺度或范围，以便进行比较、分析或处理。在不同的领域中，归一化可能具有不同的含义和方法，但其基本目标是将数据转换为标准形式，消除尺度差异或使数据具有特定的性质。\n在统计学和机器学习中，归一化通常指的是将数据按照一定规则进行缩放，使其落入特定的范围或分布。这有助于提高模型的稳定性和收敛速度，以及降低不同特征之间的影响。\n常见的归一化方法包括：\n最小-最大缩放（Min-Max Scaling）：将数据线性缩放到指定的范围（通常是 [0, 1]），公式为：\n$$ x_{\\text{norm}} = \\frac{x - \\text{min}(x)}{\\text{max}(x) - \\text{min}(x)} $$ Z-Score 标准化（Z-Score Normalization）：也称为标准化，将数据转换为均值为 0、标准差为 1 的正态分布，公式为： $$ x_{\\text{norm}} = \\frac{x - \\mu}{\\sigma} $$ 小数定标标准化（Decimal Scaling）：将数据除以一个适当的基数（例如 10 的幂），使得数据的绝对值最大不超过 1。\n归一化可以确保数据在不同特征之间具有可比性，并且有助于提高模型的性能和稳定性。\naxis()\n1 2 # side=1 x轴; side=2 y轴; at=sqp() 修改间隔 axis(side = 1, at=seq(60, 300, by=20)) # add \u0026#34;60\u0026#34;, \u0026#34;80\u0026#34;, ... {% asset_image hist5.png %}\nlines()\n1 2 lines(density(store.df$p1sales, bw=10), # \u0026#34;bw=...\u0026#34; adjusts the smoothing type=\u0026#34;l\u0026#34;, col = \u0026#34;darkred\u0026#34;, lwd=2) # lwd=line width 这是对 density 函数的调用，用于创建概率密度估计图（Kernel Density Estimation，KDE）。以下是其中的各个参数的解释：\ndensity(store.df$p1sales, bw=10)：这部分调用了 density 函数，对 store.df$p1sales 列进行概率密度估计。bw=10 指定了带宽参数，控制了估计的平滑程度。带宽越大，估计的曲线越平滑，带宽越小，曲线越精细。这里的 bw=10 表示带宽为 10。\ntype=\u0026quot;l\u0026quot;：这个参数指定了要绘制的图形类型。\u0026quot;l\u0026quot; 表示绘制一条折线图。折线图将估计的密度曲线作为折线绘制。\ncol=\u0026quot;darkred\u0026quot;：这个参数指定了绘制的折线的颜色。\u0026quot;darkred\u0026quot; 表示深红色。\nlwd=2：这个参数指定了绘制的折线的线宽。lwd=2 表示线宽为 2 像素。\n{% asset_image hist6.png %}\ntype\ntype 参数用于指定绘图的类型，除了 \u0026ldquo;l\u0026rdquo;（折线图）之外，常用的取值还有：\n\u0026ldquo;p\u0026rdquo;：绘制散点图，即仅绘制数据点而不连接它们。 \u0026ldquo;b\u0026rdquo;：绘制数据点并将它们连接成线段，从而形成折线图。与 \u0026ldquo;l\u0026rdquo; 类似，但是在每个数据点处绘制一个大点。 \u0026ldquo;o\u0026rdquo;：与 \u0026ldquo;b\u0026rdquo; 类似，但在每个数据点处绘制一个小圆圈，而不是大点。 \u0026ldquo;h\u0026rdquo;：绘制直方图，即垂直的线条用于表示频率。 \u0026ldquo;s\u0026rdquo;：绘制阶梯图，即通过垂直和水平线段连接每个数据点，从而形成阶梯状的图形。 1 2 lines(density(store.df$p1sales, bw=10), # \u0026#34;bw=...\u0026#34; adjusts the smoothing type=\u0026#34;o\u0026#34;, col = \u0026#34;darkred\u0026#34;, lwd=2) # lwd=line width {% asset_image hist7.png %}\n二、week1 code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 # install.packages (\u0026#34;readxl\u0026#34;) # install.packages (\u0026#34;psych\u0026#34;) # install.packages (\u0026#34;car\u0026#34;) # install.packages (\u0026#34;gpairs\u0026#34;) # install.packages (\u0026#34;grid\u0026#34;) # install.packages (\u0026#34;lattice\u0026#34;) # install.packages (\u0026#34;corrplot\u0026#34;) # install.packages (\u0026#34;gplots\u0026#34;) library(\u0026#34;readxl\u0026#34;) library(\u0026#34;psych\u0026#34;) library(\u0026#34;car\u0026#34;) library(\u0026#34;gpairs\u0026#34;) library(\u0026#34;grid\u0026#34;) library(\u0026#34;lattice\u0026#34;) library(\u0026#34;corrplot\u0026#34;) library(\u0026#34;gplots\u0026#34;) # 获取当前已加载文件的目录 file_dir \u0026lt;- dirname(parent.frame(2)$ofile) print(file_dir) # 将工作目录设置为当前已加载文件的目录 setwd(file_dir) store.df \u0026lt;- read.csv(\u0026#34;Data_Descriptive.csv\u0026#34;, stringsAsFactors=TRUE) str(store.df) table(store.df$p1price) p1.table \u0026lt;- table(store.df$p1price) p1.table str(p1.table) plot(p1.table) table(store.df$p1price, store.df$p1prom) p1.table2 \u0026lt;- table(store.df$p1price, store.df$p1prom) p1.table2[ ,2] / (p1.table2[ ,1] + p1.table2[ ,2]) plot(p1.table2) min(store.df$p1sales) max(store.df$p1sales) mean(store.df$p1prom) median(store.df$p2sales) var(store.df$p1sales) sd(store.df$p1sales) IQR(store.df$p1sales) mad(store.df$p1sales) quantile(store.df$p1sales, probs = c(0.25, 0.5, 0.75)) quantile(store.df$p1sales, probs = c(0.05, 0.95)) # central 90% data quantile(store.df$p1sales, probs = 0:10/10) quantile(store.df$p1sales, probs = seq(from=0, to=1, by=0.1)) mysummary.df \u0026lt;- data.frame(matrix(NA, nrow=2, ncol=2)) # 2 by 2 empty matrix names(mysummary.df) \u0026lt;- c(\u0026#34;Median Sales\u0026#34;, \u0026#34;IQR\u0026#34;) # name columns rownames(mysummary.df) \u0026lt;- c(\u0026#34;Product 1\u0026#34;, \u0026#34;Product 2\u0026#34;) # name rows mysummary.df[\u0026#34;Product 1\u0026#34;, \u0026#34;Median Sales\u0026#34;] \u0026lt;- median(store.df$p1sales) mysummary.df[\u0026#34;Product 2\u0026#34;, \u0026#34;Median Sales\u0026#34;] \u0026lt;- median(store.df$p2sales) mysummary.df[\u0026#34;Product 1\u0026#34;, \u0026#34;IQR\u0026#34;] \u0026lt;- IQR(store.df$p1sales) mysummary.df[\u0026#34;Product 2\u0026#34;, \u0026#34;IQR\u0026#34;] \u0026lt;- IQR(store.df$p2sales) mysummary.df summary(store.df) summary(store.df$Year) summary(store.df, digits = 2) # 保留小数点后两位 library(psych) # install if needed describe(store.df) describe(store.df[,c(2, 4:9)]) dim(store.df) head(store.df) tail(store.df) some(store.df) str(store.df) summary(store.df) describe(store.df) apply(store.df[ 2:9], MARGIN = 2, FUN = mean) apply(store.df[ , 2:9], 2, sum) apply(store.df[ , 2:9], 2, sd) hist(store.df$p1sales) # 添加标题、x轴y轴说明 hist(store.df$p1sales, main = \u0026#34;Product 1 Weekly Sales Frequencies, All Stores\u0026#34;, xlab = \u0026#34;Product 1 Sales (Units)\u0026#34;, ylab = \u0026#34;Count\u0026#34;) colors() hist(store.df$p1sales, main = \u0026#34;Product 1 Weekly Sales Frequencies, All Stores\u0026#34;, xlab = \u0026#34;Product 1 Sales (Units)\u0026#34;, ylab = \u0026#34;Count\u0026#34;, breaks = 30, # more columns col = \u0026#34;lightblue\u0026#34; # colore the bars ) hist(store.df$p1sales, main = \u0026#34;Product 1 Weekly Sales Frequencies, All Stores\u0026#34;, xlab = \u0026#34;Product 1 Sales (Units)\u0026#34;, ylab = \u0026#34;Count\u0026#34;, breaks = 30, col = \u0026#34;lightblue\u0026#34;, freq = FALSE, # means plot density, not counts xaxt=\u0026#34;n\u0026#34; # means x-axis tick mark is set to \u0026#34;none\u0026#34; ) hist(store.df$p1sales, main = \u0026#34;Product 1 Weekly Sales Frequencies, All Stores\u0026#34;, xlab = \u0026#34;Product 1 Sales (Units)\u0026#34;, ylab = \u0026#34;Count\u0026#34;, breaks = 30, col = \u0026#34;lightblue\u0026#34;, freq = FALSE, # means plot density, not counts xaxt=\u0026#34;n\u0026#34; # means x-axis tick mark is set to \u0026#34;none\u0026#34; ) # side=1 x轴; side=2 y轴; at=sqp() 修改间隔 axis(side = 1, at=seq(60, 300, by=20)) # add \u0026#34;60\u0026#34;, \u0026#34;80\u0026#34;, ... lines(density(store.df$p1sales, bw=10), # \u0026#34;bw=...\u0026#34; adjusts the smoothing type=\u0026#34;l\u0026#34;, col = \u0026#34;darkred\u0026#34;, lwd=2) # lwd=line width lines(density(store.df$p1sales, bw=10), # \u0026#34;bw=...\u0026#34; adjusts the smoothing type=\u0026#34;o\u0026#34;, col = \u0026#34;darkred\u0026#34;, lwd=2) # lwd=line width ","date":"2024-03-13T00:00:00Z","permalink":"https://MyLoveES.github.io/p/rweek1-desciptive-analysis/","title":"R[week1] Desciptive Analysis"},{"content":"本篇主要讲了R语言的几种类型，以及与数据之间的关系。\nR: 4.3.2 (2023-10-31)\nR studio: 2023.12.1+402 (2023.12.1+402)\n一、Basic objects 1. vectors 一维数据集，常用于number, boolean, string, etc.\n1 2 3 4 5 6 7 8 9 10 11 12 # define some variables \u0026gt; x \u0026lt;- c(2, 4, 6, 8) \u0026gt; xNum \u0026lt;- c(1, 3.14159, 5, 7) \u0026gt; xLog \u0026lt;- c(TRUE, FALSE, TRUE, TRUE) \u0026gt; xChar \u0026lt;- c(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;, \u0026#34;boo\u0026#34;, \u0026#34;far\u0026#34;) \u0026gt; xMix \u0026lt;- c(1, TRUE, 3, \u0026#34;Hello, world!\u0026#34;) # print variable \u0026gt; xNum # result [1] 1.00000 3.14159 5.00000 7.00000 1 2 3 4 5 6 7 8 9 # 可以使用vector作为新verctor创建的初始值 \u0026gt; x2 \u0026lt;- c(xNum, xMix) # print \u0026gt; x2 # result [1] \u0026#34;1\u0026#34; \u0026#34;3.14159\u0026#34; \u0026#34;5\u0026#34; \u0026#34;7\u0026#34; \u0026#34;1\u0026#34; \u0026#34;TRUE\u0026#34; [7] \u0026#34;3\u0026#34; \u0026#34;Hello, world!\u0026#34; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 下标与计算，需要注意R语言中下标从1开始 \u0026gt; xNum[2] [1] 3.14159 # 计算时，不指明元素的话，会将vec中所有元素各执行一遍计算 \u0026gt; x2 \u0026lt;- c(x, x) \u0026gt; x2 + 1 [1] 3 5 7 9 3 5 7 9 \u0026gt; x2 * pi [1] 6.283185 12.566371 18.849556 25.132741 6.283185 12.566371 18.849556 25.132741 \u0026gt; (x+cos(0.5)) * x2 [1] 5.755165 19.510330 41.265495 71.020660 5.755165 19.510330 41.265495 71.020660 注⚠️：由此可以看出一个特点，若两个vec计算，元素数量不相同时，会反复使用元素少的vec。（很像是矩阵的计算？（谁说vec不是一维矩阵呢？ 比如上面的x只有4个元素，而x2有8个元素，所以x的元素使用了两次。\n注⚠️：除了cos外，常用的数学函数还有：\n函数 描述 exp(x) 计算e的x次幂。 log(x) 计算x的自然对数。 log10(x) 计算x的以10为底的对数。 log2(x) 计算x的以2为底的对数。 sqrt(x) 计算x的平方根。 ^ 或 ** 进行幂运算，例如x^2 或 x**2表示x的平方。 sin(x) 计算x的正弦值。 cos(x) 计算x的余弦值。 tan(x) 计算x的正切值。 asin(x) 计算x的反正弦值。 acos(x) 计算x的反余弦值。 atan(x) 计算x的反正切值。 sinh(x) 计算x的双曲正弦值。 cosh(x) 计算x的双曲余弦值。 tanh(x) 计算x的双曲正切值。 ceiling(x) 对x进行向上取整。 floor(x) 对x进行向下取整。 abs(x) 计算x的绝对值。 factorial(x) 计算x的阶乘。 %% 计算x除以y的余数，例如5 %% 2等于1。 2. More on Vectors and Indexing 2.1 子集 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # 定义一个数字序列 \u0026gt; xSeq \u0026lt;- 1:10 # use 1:10 instead of typing 1,2,3,4 ...10. # 获取子集 \u0026gt; xNum [1] 1.00000 3.14159 5.00000 7.00000 # 第2到第4个元素 \u0026gt; xNum[2:4] [1] 3.14159 5.00000 7.00000 # 第2-3个元素 \u0026gt; myStart \u0026lt;- 2 \u0026gt; xNum[myStart:sqrt(myStart+7)] [1] 3.14159 5.00000 \u0026gt; xSeq [1] 1 2 3 4 5 6 7 8 9 10 # 负数代表排除掉第n个元素 \u0026gt; xSeq[-5:-7] [1] 1 2 3 4 8 9 10 # 用vec计算得出另一个vec \u0026gt; xSub \u0026lt;- xNum[2:4] [1] 3.14159 5.00000 7.00000 # 可以用boolean来筛选元素 \u0026gt; xNum[c(FALSE, TRUE, TRUE, TRUE)] [1] 3.14159 5.00000 7.00000 # 因此可以用表达式来获取logic value \u0026gt; xNum[xNum \u0026gt; 3] [1] 3.14159 5.00000 7.00000 3. Missing and interesting values - NA 3.1 重要的NA 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 \u0026gt; my.test.scores \u0026lt;- c(91, NA, NA) NA \u0026gt; mean(my.test.scores) NA \u0026gt; max(my.test.scores) NA # 筛选掉NA ## 方法1: na.rm=TRUE \u0026gt; mean(my.test.scores, na.rm=TRUE) 91 \u0026gt; max(my.test.scores, na.rm=TRUE) 91 ## 方法2：na.omit \u0026gt; mean(na.omit(my.test.scores)) 91 ## 方法3: !is.na(x) \u0026gt; is.na(my.test.scores) [1] FALSE TRUE TRUE \u0026gt; my.test.scores[!is.na(my.test.scores)] 91 4. Lists 多维的数据集合\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 \u0026gt; str(xChar) chr [1:4] \u0026#34;foo\u0026#34; \u0026#34;bar\u0026#34; \u0026#34;boo\u0026#34; \u0026#34;far\u0026#34; # 从vec合并为list \u0026gt; xList \u0026lt;- list(xNum, xChar) \u0026gt; xList [[1]] [1] 1.00000 3.14159 5.00000 7.00000 [[2]] [1] \u0026#34;foo\u0026#34; \u0026#34;bar\u0026#34; \u0026#34;boo\u0026#34; \u0026#34;far\u0026#34; \u0026gt; str(xList) List of 2 $ : num [1:4] 1 3.14 5 7 $ : chr [1:4] \u0026#34;foo\u0026#34; \u0026#34;bar\u0026#34; \u0026#34;boo\u0026#34; \u0026#34;far\u0026#34; \u0026gt; summary(xList[[1]]) Min. 1st Qu. Median Mean 3rd Qu. Max. 1.000 2.606 4.071 4.035 5.500 7.000 # 分配名字 ## 方式1 \u0026gt; names(xList) \u0026lt;- c(\u0026#34;itemnum\u0026#34;, \u0026#34;itemchar\u0026#34;) ## 方式2 xList \u0026lt;- list(itemnum=xNum, itemchar=xChar) \u0026gt; xList $itemnum [1] 1.00000 3.14159 5.00000 7.00000 $itemchar [1] \u0026#34;foo\u0026#34; \u0026#34;bar\u0026#34; \u0026#34;boo\u0026#34; \u0026#34;far\u0026#34; # 分配名字后，可以用名字来替代索引，获得元素 \u0026gt; xList$itemnum # method 2: $name reference [1] 1.00000 3.14159 5.00000 7.00000 \u0026gt; xList[[\u0026#34;itemnum\u0026#34;]] # method 3: quoted name [1] 1.00000 3.14159 5.00000 7.00000 5. Data frame 即为矩阵\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # 创建一个date frame, 使用多个vec作为数据源 \u0026gt; x.df \u0026lt;- data.frame(xNum, xLog, xChar) \u0026gt; x.df xNum xLog xChar 1 1.00000 TRUE foo 2 3.14159 FALSE bar 3 5.00000 TRUE boo 4 7.00000 TRUE far # 使用index获取特定元素 \u0026gt; x.df[2,1] [1] 3.14159 \u0026gt; x.df1 \u0026lt;- data.frame(xNum, xLog, xChar, stringsAsFactors=TRUE) \u0026gt; x.df1 xNum xLog xChar 1 1.00000 TRUE foo 2 3.14159 FALSE bar 3 5.00000 TRUE boo 4 7.00000 TRUE far \u0026gt; str(x.df1) \u0026#39;data.frame\u0026#39;:\t4 obs. of 3 variables: $ xNum : num 1 3.14 5 7 $ xLog : logi TRUE FALSE TRUE TRUE $ xChar: Factor w/ 4 levels \u0026#34;bar\u0026#34;,\u0026#34;boo\u0026#34;,\u0026#34;far\u0026#34;,..: 4 1 2 3 注⚠️：何为factor？\n个人理解类似于“枚举”，有限集合。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # 一些 \u0026#39;sub\u0026#39; data frame 方法 \u0026gt; x.df[2, ] # 第2行 all of row 2 xNum xLog xChar 2 3.14159 FALSE bar \u0026gt; x.df[ ,3] # 第3列 all of column 3 [1] \u0026#34;foo\u0026#34; \u0026#34;bar\u0026#34; \u0026#34;boo\u0026#34; \u0026#34;far\u0026#34; \u0026gt; x.df[2:3, ] # 第2-3行 xNum xLog xChar 2 3.14159 FALSE bar 3 5.00000 TRUE boo \u0026gt; x.df[ ,1:2] # two columns # 第1-2列 xNum xLog 1 1.00000 TRUE 2 3.14159 FALSE 3 5.00000 TRUE 4 7.00000 TRUE \u0026gt; x.df[-3, ] # omit the third observation # 排除掉第3行 xNum xLog xChar 1 1.00000 TRUE foo 2 3.14159 FALSE bar 4 7.00000 TRUE far \u0026gt; x.df[, -2] # omit the second column # 排除掉第2列 xNum xChar 1 1.00000 foo 2 3.14159 bar 3 5.00000 boo 4 7.00000 far 注⚠️：从data frame中筛选数据，如果是单个元素，返回值类型为元素数量为1的vec；如果是一行或者一列，则为vec；如果是多行或者多列，则为一个新的data frame。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # 清理掉上述内容 \u0026gt; rm(list=ls()) # caution, deletes all objects! # 新建一个data frame，填充元素 \u0026gt; store.num \u0026lt;- factor(c(3, 14, 21, 32, 54)) # store id \u0026gt; store.rev \u0026lt;- c(543, 654, 345, 678, 234) # store revenue, $1000 \u0026gt; store.visits \u0026lt;- c(45, 78, 32, 56, 34) # visits, 1000s \u0026gt; store.manager \u0026lt;- c(\u0026#34;Annie\u0026#34;, \u0026#34;Bert\u0026#34;, \u0026#34;Carla\u0026#34;, \u0026#34;Dave\u0026#34;, \u0026#34;Ella\u0026#34;) \u0026gt; (store.df \u0026lt;- data.frame(store.num, store.rev, store.visits,store.manager)) # 加括号，直接打印内容 store.num store.rev store.visits store.manager 1 3 543 45 Annie 2 14 654 78 Bert 3 21 345 32 Carla 4 32 678 56 Dave 5 54 234 34 Ella \u0026gt; store.df$store.manager # 查询manager [1] \u0026#34;Annie\u0026#34; \u0026#34;Bert\u0026#34; \u0026#34;Carla\u0026#34; \u0026#34;Dave\u0026#34; \u0026#34;Ella\u0026#34; \u0026gt; mean(store.df$store.rev) # 查询revenue 平均值 [1] 490.8 \u0026gt; cor(store.df$store.rev, store.df$store.visits) # caculate correlation [1] 0.8291032 \u0026gt; summary(store.df) # summary store.num store.rev store.visits store.manager 3 :1 Min. :234.0 Min. :32 Length:5 14:1 1st Qu.:345.0 1st Qu.:34 Class :character 21:1 Median :543.0 Median :45 Mode :character 32:1 Mean :490.8 Mean :49 54:1 3rd Qu.:654.0 3rd Qu.:56 Max. :678.0 Max. :78 cor()函数 用于计算相关系数的函数之一。相关系数是衡量两个变量之间线性关系强度和方向的统计量，通常用于探索变量之间的相关性。\n1 cor(x, y, method = c(\u0026#34;pearson\u0026#34;, \u0026#34;kendall\u0026#34;, \u0026#34;spearman\u0026#34;), use = c(\u0026#34;everything\u0026#34;, \u0026#34;all.obs\u0026#34;, \u0026#34;complete.obs\u0026#34;, \u0026#34;na.or.complete\u0026#34;)) 参数 x: 一个数值向量、矩阵或数据框，或者是一个因子。 y: （可选）一个数值向量、矩阵或数据框，或者是一个因子。如果指定了 y，则会计算 x 和 y 之间的相关系数；如果未指定 y，则会计算 x 中各个变量之间的相关系数。 method: （可选）指定计算相关系数的方法，可选值为 \u0026ldquo;pearson\u0026rdquo;（皮尔逊相关系数，默认）、\u0026ldquo;kendall\u0026rdquo;（肯德尔相关系数）和 \u0026ldquo;spearman\u0026rdquo;（斯皮尔曼相关系数）。 use: （可选）指定如何处理缺失值的参数，可选值包括 \u0026ldquo;everything\u0026rdquo;（默认，对所有数据点进行计算）、\u0026ldquo;all.obs\u0026rdquo;（对所有非缺失数据点进行计算）、\u0026ldquo;complete.obs\u0026rdquo;（对完全没有缺失值的数据点进行计算）、\u0026ldquo;na.or.complete\u0026rdquo;（对有缺失值但至少有一对完整观测值的数据点进行计算）。\n返回值 返回一个相关系数矩阵，其中包含了各个变量之间的相关系数。\n示例 假设有一个数据框 df 包含了两个数值变量 x 和 y\n1 2 3 4 5 6 7 8 # 创建数据框 df \u0026lt;- data.frame( x = c(1, 2, 3, 4, 5), y = c(2, 3, 5, 7, 11) ) # 计算两个变量之间的皮尔逊相关系数 cor(df$x, df$y) 将输出两个变量 x 和 y 之间的皮尔逊相关系数。\n皮尔逊相关系数（Pearson correlation coefficient） 是一种用来衡量两个变量之间线性关系强度和方向的统计量。它衡量的是两个变量之间的线性相关程度，取值范围在-1到1之间。\n特点： 当 r=1 时，表示两个变量完全正相关，即一个变量增大时，另一个变量也增大，呈线性关系； 当 r=-1 时，表示两个变量完全负相关，即一个变量增大时，另一个变量减小，也呈线性关系； 当 r=0 时，表示两个变量之间没有线性关系，但并不代表两个变量之间没有其他类型的关系。\n解释： 如果 r 的值接近于1或-1，表示两个变量之间有很强的线性关系，相关性较高；\n如果 r 的值接近于0，表示两个变量之间的线性关系很弱，相关性较低。\n适用范围： 皮尔逊相关系数要求两个变量的数据是连续变量； 它要求变量之间的关系是线性的。\n注意事项： 皮尔逊相关系数只能衡量两个变量之间的线性关系，对于非线性关系不敏感； 相关系数不能说明因果关系，只能说明两个变量之间的相关性。\n在实际数据分析中，皮尔逊相关系数经常用于探索变量之间的关系，特别是在探索两个连续变量之间的关系时，它是一个非常有用的工具。\n6. Saving, loading, and importing data 6.1 save and reload 1 2 3 4 5 6 7 8 # 保存data frame到文件 \u0026gt; save(store.df, file=\u0026#34;store-df-backup.RData\u0026#34;) # 删掉内存中的data frame \u0026gt; rm(store.df) # caution, only if save() gave no error # 重新从文件中加载 \u0026gt; load(\u0026#34;store-df-backup.RData\u0026#34;) 1 2 3 4 5 \u0026gt; save.image() # saves file \u0026#34;.RData\u0026#34; \u0026gt; save.image(\u0026#34;mywork.RData\u0026#34;) \u0026gt; load(\u0026#34;mywork.RData\u0026#34;) 注⚠️： save()和save.image()都是保存到文件；save()只保存指定的元素；save.image()保存所有的对象和数据。 load()加载.RData时，会把文件中保存的内存对象，覆盖掉当前内存中的\n6.2 import 6.2.1 excel 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # 安装并引用excel处理的依赖包 \u0026gt; install.packages (\u0026#34;readxl\u0026#34;) \u0026gt; library(readxl) \u0026gt; deospray.data \u0026lt;- read_excel(path = \u0026#34;deospray sales.xls\u0026#34;, sheet = \u0026#34;deospray\u0026#34;) # filename, sheet name \u0026gt; str(deospray.data) tibble [620 × 22] (S3: tbl_df/tbl/data.frame) $ chain : num [1:620] 1 1 1 1 1 1 1 1 1 1 ... $ week : chr [1:620] \u0026#34;W16046\u0026#34; \u0026#34;W16047\u0026#34; \u0026#34;W16048\u0026#34; \u0026#34;W16049\u0026#34; ... $ sales_brand1 : num [1:620] 24 28.1 25.8 24.6 23 ... $ sales_brand2 : num [1:620] 31.3 39.6 102.2 68.4 37.8 ... $ sales_brand3 : num [1:620] 21.1 24 26.8 22.7 20.6 ... $ sales_brand4 : num [1:620] 48.4 52.1 53 43 53.9 ... $ sales_brand5 : num [1:620] 65 64.6 64.7 64.2 52.7 ... $ price_brand1 : num [1:620] 1.09 1.08 1.08 1.08 1.08 ... $ price_brand2 : num [1:620] 0.688 0.642 0.478 0.569 0.576 ... $ price_brand3 : num [1:620] 1.05 1.05 1.05 1.04 1.06 ... $ price_brand4 : num [1:620] 1.12 1.12 1.12 1.12 1.12 ... $ price_brand5 : num [1:620] 1.32 1.32 1.32 1.32 1.32 ... $ display_brand1: num [1:620] 0.02 0.02 0.19 0.18 0.05 0.05 0.08 0.03 0.05 0.04 ... $ display_brand2: num [1:620] 0 0 0 0 0 0 0 0 0 0 ... $ display_brand3: num [1:620] 0 0 0 0 0 0 0 0 0 0 ... $ display_brand4: num [1:620] 0.01 0 0 0 0 0 0 0 0 0 ... $ display_brand5: num [1:620] 0 0 0 0 0 0 0 0 0 0 ... $ feature_brand1: num [1:620] 0 0 0 0 0 0 0 0 0 0 ... $ feature_brand2: num [1:620] 0 0 0 0 0 0 0 0 0 0 ... $ feature_brand3: num [1:620] 0 0 0 0 0 0 0 0 0 0 ... $ feature_brand4: num [1:620] 0 0 0 0 0 0 0 0 0 0 ... $ feature_brand5: num [1:620] 0 0 0 0 0 0 0 0 0 0 ... # 查看数据 \u0026gt; View(deospray.data) 6.2.2 csv 1 2 \u0026gt; store.df \u0026lt;- read.csv(\u0026#34;Data_descriptive.csv\u0026#34;) \u0026gt; store.df$storeNum \u0026lt;- factor(store.df$storeNum) 7. function 1 2 3 4 5 6 7 8 9 10 11 12 # 一个简单的function se \u0026gt; se \u0026lt;- function(x){sd(x) / sqrt(length(x))} \u0026gt; se(store.df$store.visits) [1] 8.42615 # 为什么PDF里面结果是NA???? \u0026gt; se \u0026lt;- function(x){ \u0026gt; # computes standard error of the mean \u0026gt; tmp.sd \u0026lt;- sd(x) # standard deviation \u0026gt; tmp.N \u0026lt;- length(x) # sample size \u0026gt; tmp.se \u0026lt;- tmp.sd / sqrt(tmp.N) #std error of the mean return(tmp.se) \u0026gt; } 注⚠️：其他的一些数学函数\n函数 描述 mean() 计算向量或数据框列的均值。 median() 计算向量或数据框列的中位数。 min() 计算向量或数据框列的最小值。 max() 计算向量或数据框列的最大值。 quantile() 计算向量或数据框列的四分位数。 cv() 或 coefficient_of_variation() 计算向量或数据框列的变异系数。 sd() 计算向量或数据框列的标准差。 var() 计算向量或数据框列的方差。 stderr() 计算向量或数据框列的标准误差。 cumsum() 计算向量中元素的累计和。 cumprod() 计算向量中元素的累计积。 sort() 对向量或数据框列进行排序。 rank() 计算向量或数据框列中元素的排名。 sum() 计算向量或数据框列的总和。 prod() 计算向量或数据框列的乘积。 cor()、cov() 分别计算相关系数和协方差矩阵。 scale() 对向量或数据框列进行标准化。 8. clean up 1 rm(list=ls()) # delete all visible objects in memory. 二、week1 code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 # 获取当前已加载文件的目录 file_dir \u0026lt;- dirname(parent.frame(2)$ofile) print(file_dir) # 将工作目录设置为当前已加载文件的目录 setwd(file_dir) # define some variables x \u0026lt;- c(2, 4, 6, 8) xNum \u0026lt;- c(1, 3.14159, 5, 7) xLog \u0026lt;- c(TRUE, FALSE, TRUE, TRUE) xChar \u0026lt;- c(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;, \u0026#34;boo\u0026#34;, \u0026#34;far\u0026#34;) xMix \u0026lt;- c(1, TRUE, 3, \u0026#34;Hello, world!\u0026#34;) xNum x2 \u0026lt;- c(xNum, xMix) x2 xNum[2] x2 \u0026lt;- c(x, x) x2+1 x2 * pi (x+cos(0.5)) * x2 xSeq \u0026lt;- 1:10 # use 1:10 instead of typing 1,2,3,4 ...10. xNum xNum[2:4] myStart \u0026lt;- 2 xNum[myStart:sqrt(myStart+7)] xSeq xSeq[-5:-7] xSub \u0026lt;- xNum[2:4] xSub xNum[c(FALSE, TRUE, TRUE, TRUE)] xNum[xNum \u0026gt; 3] my.test.scores \u0026lt;- c(91, NA, NA) mean(my.test.scores) max(my.test.scores) mean(my.test.scores, na.rm=TRUE) max(my.test.scores, na.rm=TRUE) mean(na.omit(my.test.scores)) is.na(my.test.scores) my.test.scores[!is.na(my.test.scores)] str(xChar) xList \u0026lt;- list(xNum, xChar) xList str(xList) summary(xList[[1]]) xList \u0026lt;- list(xNum, xChar) # method 1: create, then name names(xList) \u0026lt;- c(\u0026#34;itemnum\u0026#34;, \u0026#34;itemchar\u0026#34;) xList xList \u0026lt;- list(itemnum=xNum, itemchar=xChar) names(xList) xList xList$itemnum # method 2: $name reference x.df \u0026lt;- data.frame(xNum, xLog, xChar) x.df x.df[2,1] x.df1 \u0026lt;- data.frame(xNum, xLog, xChar, stringsAsFactors=TRUE) x.df1 str(x.df1) x.df[2, ] # all of row 2 x.df[ ,3] # all of column 3 x.df[2:3, ] x.df[ ,1:2] # two columns x.df[-3, ] # omit the third observation x.df[, -2] # omit the second column rm(list=ls()) # caution, deletes all objects! store.num \u0026lt;- factor(c(3, 14, 21, 32, 54)) # store id store.rev \u0026lt;- c(543, 654, 345, 678, 234) # store revenue, $1000 store.visits \u0026lt;- c(45, 78, 32, 56, 34) # visits, 1000s store.manager \u0026lt;- c(\u0026#34;Annie\u0026#34;, \u0026#34;Bert\u0026#34;, \u0026#34;Carla\u0026#34;, \u0026#34;Dave\u0026#34;, \u0026#34;Ella\u0026#34;) (store.df \u0026lt;- data.frame(store.num, store.rev, store.visits,store.manager)) store.df$store.manager mean(store.df$store.rev) cor(store.df$store.rev, store.df$store.visits) summary(store.df) save(store.df, file=\u0026#34;store-df-backup.RData\u0026#34;) rm(store.df) # caution, only if save() gave no error load(\u0026#34;store-df-backup.RData\u0026#34;) save.image() # saves file \u0026#34;.RData\u0026#34; save.image(\u0026#34;mywork_week1.RData\u0026#34;) load(\u0026#34;mywork_week1.RData\u0026#34;) # install.packages (\u0026#34;readxl\u0026#34;) # library(readxl) deospray.data \u0026lt;- read_excel(path = \u0026#34;deospray sales.xls\u0026#34;, sheet = \u0026#34;deospray\u0026#34;) # filename, sheet name str(deospray.data) # View(deospray.data) se \u0026lt;- function(x){sd(x) / sqrt(length(x))} se(store.df$store.visits) ","date":"2024-03-12T00:00:00Z","permalink":"https://MyLoveES.github.io/p/rweek1-getting-start/","title":"R[week1] Getting start"},{"content":"一、主要概念 1. 变量（variables） 1.1 创造一个变量 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 使用等号 = 号赋值 \u0026gt; var1 = c(0,1,2,3) \u0026gt; print(var1) [1] 0 1 2 3 # 使用左箭头 \u0026lt;- 赋值 \u0026gt; var2 \u0026lt;- c(\u0026#34;learn\u0026#34;,\u0026#34;R\u0026#34;) \u0026gt; print(var2) [1] \u0026#34;learn\u0026#34; \u0026#34;R\u0026#34; # 使用右箭头 -\u0026gt; 赋值 \u0026gt; c(TRUE,1) -\u0026gt; var3 \u0026gt; print(var3) [1] 1 1 1.2 打印一下定义过的所有变量 1 2 # 打印现在已经定义了的变量 # 使用 ls() 命令, ls 其实就是 list 的缩写 1.3 打印变量里的值 1 2 # 可以直接输入变量 xNum 1 2 # 也可以用print() print(xNum) 1 2 3 # 或者用str() [展示时可能会发生变化哦，比如小数点位数默认保留两位] str(xNum) 2. 向量（vector） 2.1 啥是向量 一组相同类型的元素的列表，比如都是数字，或者都是字符串\n是一维的\n2.2 创造一个向量 1 2 # 使用c()命令 x \u0026lt;- c(2, 4, 6, 8) # create a vector x using combine function c() 2.3 下标（index） 既然是列表，那么其中的每个元素自然是要有序号的，但是和常见的其他编程语言不同，元素序号是从1开始的！\n1 2 3 4 5 6 7 \u0026gt; x \u0026lt;- c(2, 4, 6, 8) # create a vector x using combine function c() \u0026gt; x [1] 2 4 6 8 \u0026gt; x[1] [1] 2 \u0026gt; x[4] [1] 8 2.4 NA（暂时不知道具体有啥用） 1 2 # 使用c()命令，可以用NA作为一个占位符，其实际值为空 x \u0026lt;- c(2, 4, NA, NA) 3. 列表（list） 3.1 和向量有什么不同？ 向量：一组一维的相同类型的元素的列表，比如都是数字，或者都是字符串\n数组：一组元素，可以包含不同类型的元素，比如数字、字符串以及矩阵的混合。除了一维外，也可以是多维的数组 [感觉不如向量用得广泛] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 列表变量赋值 \u0026gt; my_list \u0026lt;- list(matrix(c(1, 2, 3, 4), nrow = 2), c(5, 6, 7)) # 打印列表 \u0026gt; my_list # 数组里的第一个元素是矩阵 [[1]] [,1] [,2] [1,] 1 3 [2,] 2 4 [[2]] # 数组里的第二个元素是向量 [1] 5 6 7 # 打印列表str() \u0026gt; str(my_list) List of 2 $ : num [1:2, 1:2] 1 2 3 4 $ : num [1:3] 5 6 7 3.2 数组的命名 1 2 3 4 5 6 7 # 1.1 直接创建一个列表 xList \u0026lt;- list(xNum, xChar) # method 1: create, then name # 1.2 给列表元素命名 names(xList) \u0026lt;- c(\u0026#34;itemnum\u0026#34;, \u0026#34;itemchar\u0026#34;) # 2. 创建列表的同时，直接给元素命名 xList \u0026lt;- list(itemnum=xNum, itemchar=xChar) 随后可以使用元素下标，或者命名后的name获取对应的元素\n1 2 3 # 数组[下标] \u0026gt; xList[[1]] [1] 1.00000 3.14159 5.00000 7.00000 1 2 3 # 数组$名称 \u0026gt; xList$itemnum # method 2: $name reference [1] 1.00000 3.14159 5.00000 7.00000 1 2 3 # 数组[[名称]] \u0026gt; xList[[\u0026#34;itemnum\u0026#34;]] # method 3: quoted name [1] 1.00000 3.14159 5.00000 7.00000 4. 数据框 （Data frame） 也有叫数据帧、数据表格的，感觉框表达的更形象 实际上可以理解为excel里存储的表格结构 1 2 3 4 5 6 7 8 9 10 11 12 13 # 创建一个data frame df = data.frame( 姓名 = c(\u0026#34;张三\u0026#34;, \u0026#34;李四\u0026#34;, \u0026#34;王五\u0026#34;), 工号 = c(\u0026#34;001\u0026#34;,\u0026#34;002\u0026#34;,\u0026#34;003\u0026#34;), 月薪 = c(1000, 2000, 3000) ) print(df) # 查看 table 数据 姓名 工号 月薪 1 张三 001 1000 2 李四 002 2000 3 王五 003 3000 而后可以通过多种操作，读取frame中的所需数据 1 2 3 4 5 6 7 8 # 平均工资(R语言支持中文) \u0026gt; mean(df$月薪) [1] 2000 # 也可以直接summary看整体数据 \u0026gt; summary(df$月薪) Min. 1st Qu. Median Mean 3rd Qu. Max. 1000 1500 2000 2000 2500 3000 5. 依赖库（package） 一些函数操作，是可以直接使用，不需要引入外部依赖库的，比如mean/min/max等基本计算。而另一些比较复杂的操作，需要引入外部package，使用package中对应的方法实现：\n1 2 3 4 5 6 7 8 # 安装readxl库 install.packages(“readxl”) # 引用readxl库 library(\u0026#34;readxl\u0026#34;) # 借助readxl库读取excel数据，文件名为deospray sales.xls，sheet名为deospray deospray.data \u0026lt;- read_excel(path = \u0026#34;deospray sales.xls\u0026#34;, sheet = \u0026#34;deospray\u0026#34;) 6. 函数（function） 可以自定义一些func便于重复使用\n1 2 3 4 5 6 7 8 se \u0026lt;- function(x){ # computes standard error of the mean tmp.sd \u0026lt;- sd(x) # standard deviation tmp.N \u0026lt;- length(x) # sample size tmp.se \u0026lt;- tmp.sd / sqrt(tmp.N) #std error of the mean return(tmp.se) } se(store.df$store.visits) 二、数据读取 From excel/csv excel 1 2 library(readxl) deospray.data \u0026lt;- read_excel(path = \u0026#34;deospray sales.xls\u0026#34;, sheet = \u0026#34;deospray\u0026#34;) csv 1 2 3 4 read.csv(\u0026#34;Data_Descriptive.csv\u0026#34;) store.df \u0026lt;- read.csv(\u0026#34;Data_descriptive.csv\u0026#34;) # 将storeNum构建为R因子 store.df$storeNum \u0026lt;- factor(store.df$storeNum) 三、数据处理的汪洋大海～ ","date":"2024-03-11T00:00:00Z","permalink":"https://MyLoveES.github.io/p/r-language-introduce/","title":"R language introduce"},{"content":" 使用的版本：\nR: 4.3.2 (2023-10-31) R studio: 2023.12.1+402 (2023.12.1+402)\n一、建一个R script 1. 切换右下角的files tab 2. 新建R script 二、编写\u0026amp;运行代码 1. 编写与运行 2. 导入与写出到文件 如果使用相对路径，应该需要把文件放到work dir 三、技巧 1. 在脚本中，将work dir设置为文件所在目录 1 2 3 4 5 # 获取当前已加载文件的目录 file_dir \u0026lt;- dirname(parent.frame(2)$ofile) print(file_dir) # 将工作目录设置为当前已加载文件的目录 setwd(file_dir) ","date":"2024-03-10T00:00:00Z","permalink":"https://MyLoveES.github.io/p/r-studio-basic-usage/","title":"R studio - Basic usage"},{"content":"我想了两天 我们的关系，和困境\n真的一直在思考，为什么仿佛总很难理解你所执着做的事。现在好像有一些答案，你看说的对不对吧哈哈哈哈哈哈\n我发现，我始终忽略了你做很多事情的动机，背后不只是热爱，而都需要“收益”。\n或许我，一直都用太传统的角度看待你。很久很久，我一直想探索出你的目标，或者“爱好”是什么，能让你坐的住，长久的注入时间，最终实现成就，当然也能好好地独处的。问你，你也总说你不知道，喜新厌旧。估计你也没时间思考过这些吧。旅游，电影，演唱会，买买买？这些都是有钱后普罗大众的需求罢了，不算爱好。读书，写字，拍vlog，短视频？仿佛每一种，喜欢了一段时间以后，就黯然落幕。啥情况，人总不能没有爱好吧，总需要有一个追求的目标。抛开这些这些我们平常耳濡目染的答案，还会有其他的吗？\n想了两天，在你的那些话之后，结果我好像发现了一个有些好玩的答案\u0026mdash;-“赚钱”\n或许赚钱这件事，在很多人眼里都不能算作一种“爱好”，你可能心里也很难承认吧，这只是一种活的好的手段而已！但是看你做过的事背后，却发现总也逃不过它的影子。读书写字升职加薪，剧本杀开店商业收益，做自媒体发vlog带来流量，甚至逛三里屯，都看能不能带货赚差价。很多爱好的新鲜感渐渐消失后，背后总要有能够“挣钱”的动力来支撑。现在你找到了这个能够带来不少收益的工作室，持之以恒地付出了几乎所有。我却只当你是在辛劳地工作（当然这的确是），却忽视了它背后能让你持续追求的动力源。你说这是危机下的不安全感，没错，但也不止于此。我觉得，这也的确是来自于你本身的热爱吧，只有热爱，才能像这样做的足够久足够好。\n当我想到这里的时候，感觉很多很多事都能想通了。为什么你能坐在那里一整天，对着电脑枯燥“工作”忘记吃喝，甚至忽略我在你说的话；为什么你能在很多事上不断探索，却又不断地喜新厌旧；为什么去了英国，却又把新环境新朋友放在一旁，焦虑不已；为什么我总也想不通你在追求什么，就算照顾了你的方方面面，却总有被吐槽帮不上忙，让你失望。原来答案一直就在眼前，我却始终没有把它当作答案，做了无数遍选择题，又一遍遍地出错。\n赚钱可以作为一种爱好或者目标吗，当然！至少对于当下的我们来说，这的确是一种非常棒的选择。能够发现持续的途径并且做好它，幸运而又努力，原来你做到了，现在和将来，当然也能够做到。\n其实我想，这也是我们之间最大的不同之处吧。你会以赚钱为明确地目标而持续付出，而我只想把赚钱作为日常行为的附加属性。做的事能赚则赚，不能也罢，收获经历也行！我在“追求”这种思维上的差异，让我不断努力，却又不断的出错，误解你，不了解你。\n那在这段关系中，我的困境是什么呢？我想我是渴望关系的稳定。我愿意付出，愿意陪伴，愿意改变。你在我眼里足够好，以至于像你说的“怎样都行”。而你时常带给我的不稳定感，总让我焦虑，灰心，否定。现在我更明白了些，为什么持久以来的付出，总是无法让你满意，在一次次付出后的却又收获失望，最后一次次否定自己，害怕去主动改变。但现在我想，我的确在努力并且的确更加了解你，这的确比我想象的难了不少😆，但却仿佛有了答案。\n就如你走之前我说过的，英国消费很高，但是你的确也不要太担心，不管怎么说还有我支持你呢（已经浮现出了你的嘲讽脸“就你这点工资”）你也要强。你现在在英国，相信你也正在探索很多方法了，又要做跨境，又要做小红书。相距万里，我挺难帮上什么忙的，但我还是想说说看我想到的吧。\n公司实习\n我想如果可能，有一个行业相关，又不太忙的实习工作。这个选择挣钱之余，能够有机会认识一些对学习更是对未来生活有用的朋友们。知道你现在很忙，但是找点时间把简历补完？这两天改简历的时候，稍稍攒了一点经验吧。如果写完vogue的，可以发给我再补充进去。超级简历有时候用起来也挺抽象的，还是得微调。\n跨境\n这的确是很方便的方式。之前有一个大学同学学俄语的，到俄罗斯留学，做得比较好。但是她当时占了几个优势：1）做俄罗斯方向的人少 2）留学时间长，攒下的用户盘子越来越大 3）在最后一年，大量用户+熟悉的物流，最终才到了成本低+量大，收益高了起来。英国这个方向，如果采购成本一样的话，要拼物流成本和实物价格。这方面如果做的人多，可能很难拿下优势。而你留学时间短，如果短期量上不去，收益很难高起来，就要回国了。\n所以除了拼日常化妆品上的价格，有没有可能做一些高端品牌上的呢？就像你当初黑五淘的漂亮正品大衣，或者其他的时尚品牌，正好可以凭借真人在英国的优势，充分服务好消费者，打消他们对于假货、瑕疵、物流不可靠这些方面的顾虑（想想你当时担心哪些因素）。\n另外，是不是可以尝试做一些电子商品，比如在国外可以轻松注册的chatgpt账号。国内由于墙、没有国外手机号，一般人很难直接自己注册上账号。现在淘宝大概要卖10-30块钱一个号，看销量有的能卖几百个。而注册实际上是免费的。或者探索一些其他的APP。\n再另外，你看在那边的国人们对国内的哪些需求比较高呢？或许可以寄过去卖卖试试。\n工作室\n这个想必你也想到过了吧。如果国内品牌能做一些国外明星，或者国内艺人来宣传一下国外品牌，或许是一个不错的机会。国内艺人宣传改善消费者认知+跨境带货一体，如果能打通的话想必不止眼下的收益这么小。或许趁机都能试试一直想过的直播带货。\n小红书 我看现在挺好的，主要是形象美，我就不多说了。但是你也能看到，小红书上英国做同样做出行攻略内容的实在是不少，如果有可能，是不是可以发现一些其他地方的特色商品比如美食啦之类的，做到利兹和周边城镇商品的代购。\n最后吧，我真的还是非常不想放弃我们的关系，总在想怎么才能帮到你，能做的更好。你太要强了，真的太辛苦了。以往我只想着照顾好你的生活，吃好喝好睡好。我想我的确在不断“成长”地看待这段关系，不再盲目地等待，而要再多在你的角度为你想想。抱抱你。\n就像你说的，虽然你不怎么搭理我了，但至少还有联系，总让我以为有希望。但我还是不止想要有希望，想和你见面，和你聊天。想和猫猫一起，等大宝贝回家。希望你能别太辛苦，猫猫会心疼的。\n","date":"2023-10-16T10:38:00Z","permalink":"https://MyLoveES.github.io/p/letter-for-my-dear/","title":"Letter for my dear"},{"content":"不玩贴吧之后，很久没跟自己说过话了。Hi，李锐，好久不见，你还好吗？\n周末两天，打开了无数遍微信聊天框，想看有没有期待的消息，哦当然，没有。“还在期待什么呢？”继续反问自己。或许这段关系，当我租下这个两居室，当那间次卧变成了一条退路，末章就已开始徐徐写下。\n现在坐在电脑前，依然思念和牵挂着她，想念她的活泼美丽，她的敏锐智慧。想念能和她拥抱在一起的甜蜜，欢笑时的开心，去海边去沙漠时的自由。出国前，我一遍一遍地刷小红书，看微博看知乎，只怕有准备不周的地方，期望能做好万全的准备，不想她受一丁点委屈。那天在机场送别，脑海里就在想，“每一次见面都当做最后一次，这不会一语成谶了吧”。多年感情里，我一直拼命变化着，甚至自己都感到陌生。我学会去跟人理论，去索要，去发声，只想能够让她满足，得到快乐。但与此同时，我也始终躲在她的庇护之下，看她在前面冲锋陷阵，在心里默默爱着她，享受着她的照顾。\n她说的总是对的。她很坚持自己的想法，对未来，对伴侣，有明确的期望。符合，则交往；不合，就分开。这应该才是正确的解法吧！我总甘于平庸，关系尚好时，不思改变，一次又一次挽回，却也终于丢掉了一次又一次的机会。她出国了，学费高，公寓贵，衣食住行处处花钱，消费上的压力扑面而来。我意识到了这个问题，但我做了什么呢？劝她放轻松一些，先出去玩玩。这当然没有解决问题，扪心自问，我会有心情去玩吗？恐怕更糟。\n但几年来，我最遗憾的，还是没能找到让她能够充实独处的方法。即使建议过无数种方案，却都和她的爱好大相径庭。她到底喜欢什么呢，这个问题我不再能得到答案了，但我来回答，“挣钱”或许是她最大的爱好。\n或许她所需要的，是一个能无微不至照顾她的人，生活，和思想。苦恼的时候，能敏感地察觉到，安慰她，帮助她。又一定要有一个强大的内心，抗住压力，共同成长。我天性愚笨，很难共情到所思所想；即使猜到，也会犹犹豫豫，无所作为。\n诚然，在这些思念中，也掺杂着一丝讨厌。为什么在我付出了几乎所有的时候，却依然能决然地放下我，每次都把这段关系首当其冲的置于刀下；为什么总轻而易举地大发雷霆，又忽视我日常的努力；为什么只顾自己过得潇潇洒洒，却总践踏我的诉求，让我感到更加绝望，更加幽闭。我讨厌这一切。当我翻遍相册却难找到一张合影，当我对她说话却被埋头无视的时候，仿佛在这个两人世界中，我只是一个无人问津，随时丢弃的摆件而已，无人中意我的存在。我好厌恶自己，放下了我想要的一切，去维持一段跌跌宕宕的感情生活。\n而我对关系所期待的，应该只是关系的“稳定”。我对她有着无穷的信任，希望彼此能始终在不牺牲彼此关系的前提上沟通和解决问题，哪有问题是不可以被解决的呢，有问题只要说出来就好了嘛！殊不知，彼此间默契的共情，却也是衡量爱的尺度。或许我们早已都感觉到，彼此的不适从。一段时间以来，我总沉溺于网络、直播来宣泄压力，对于她，对于生活，早已不抱希望；对无止境的付出和改变早已感到疲惫；对不知道藏在哪个角落的雷霆，感到恐惧慌乱。每天的生活都如履薄冰，生怕有丝毫的不如意，又引来关系上的鞭挞。说来好笑，你敢相信吗，为了让她开心，为了维持这段关系，因为很难和她坐在一起规划，所以很多事很多物，我都要准备PLAN DEFG，买好各种备用品，来供她挑选。李锐，以后当你再看到这段话，再想起这些事，你依然会觉得好笑吗？这当然不是我所期望的生活。我希望能和她，平等地沟通，为如何成长，为我们的未来一起谋划。但在这段关系中，我看不到希望，我将是无声的。我已经不知道多久，我已经快要忘了我自己，忘了我的诉求，忘了我的原则，只剩下想要和她在一起的欲望。照照镜子吧李锐，去镜中世界冒险吧，你要找到你自己。\n快要结束了！这段彼此不适，无法满足的日子。现在我心底最最最希望的事，是看到她在国外过的独立，充实，自由，快乐，满足。轻松挣钱，无忧无虑，享受生活。未来，希望你一切都好，最后在爱你一次，宝。我们未来再见的时候，希望我是一个找到了自己的人，我们一定都能变得更好！\n关系糟糕时，我常常去定义哪个瞬间将是这段关系的最后一舞。临到时却总又服软，吞噬掉我自己的诉求，求得一夕安稳。\n而在我猝不及防的时候，现在，终于，舞，已然跳起来了。\n","date":"2023-10-15T22:38:00Z","permalink":"https://MyLoveES.github.io/p/last-dance-for-the-relationship/","title":"Last dance for the relationship"},{"content":"宝，我真的很想你，有一肚子话想说，却又畏畏缩缩，我先用文字表达一下吧\n我真的很能理解你现在的感受，这段时间日复一日的百无聊赖，让你烦躁了吧。我没能做好，真是抱歉。当然我也真的相信随着宝的努力，这样的日子不会很长，当有了新的工作，或者社团活动多多的时候，感受很快就会好起来了，你的社交天赋可是MAX呢。虽然你总说人难以深交，但也不妨多与身边的人交交心呀。\n在你面前，我很少谈及我对于我们关系的想法吧，既然现在有机会，让我来表(tu)达(cao)表(tu)达(cao)吧：\n在我心里，你的确是一个自我意识非常(x10)强的人，就说你工作的时候，甚至能忽略我在耳边说的话，忽略我正在做的事，Unbelievable，怎么会有这种人呢！！！！\n即使这样，但是在你全神贯注的时候，在我眼里你真的好美呀，那雷厉风行的状态，和得到收获时快乐的样子，我也感到很开心。临期半小时拿下大单，牛！我打心底不愿意让你改变，愿意看你保持自我。（结果我怎么感觉被反噬了呢哈哈哈哈哈，张信哲唱得好，“是我給你自由過了火！😭”\n在我们的关系中，你的确保持住了！不过就像你说的，就算我想也改变不了你哈哈哈哈哈哈哈哈。你凭借自己的努力完成一个又一个挑战。结果，我们迎来了一种“不平等”的关系。我既愿意看到你自由自在地做自己，看着你独立和开心。但我心里也有要求，这样的日子长了，我出去旅游想和你合照，总也张不开口提，我真的好希望能多一些合照来回忆一起的快乐！想多分享分享心底的想法和感受，却怕会被你嘲笑和驳斥。有不理解时，我有时觉得像是在深井里一般幽闭；但是和你在一起开心时，心情又像蜻蜓一样，飞舞和跃动。我知道我做的不好的时候，让你在快乐和失望的瞬间，也会有类似的感受吧！我真的好想抱抱你哦，宝。\n其实有一件事我打心底特别不希望宝做，很久很久很久了。就是我们不高兴的时候，感觉宝有时会把我们的关系首当其冲地拿出来，作为让我痛苦和难过的武器，戳我的心窝子。时间久了，我的心里总会感觉“不安全”，会长时间地影响着我去想，\u0026ldquo;我真的是一个重要的人吗，我们的关系真的这么不值一提吗，真的可以随时被抛弃吗？那就让我被抛弃吧！\u0026ldquo;但我想你时，这些想法却又像枷锁一样牢牢困住我，我拼命否定自己，我好痛苦。我真的真的希望，宝不要做这样的事情。不知道我们的关系，真的可以自由的表达吗。如果让我许愿的话，这是我唯一的期望了\n对于宝这段时间的枯燥，我还有些其他的话想说，宝耐心看完哦\n宝能独立处世，聪明，又自我，真的很好。但是，你不擅长处理一个人的时光。总是让繁忙的工作填满自己，却又总在一个人的时候无所事事。我总舍不得离开你，想在你没事的时候，随时能出现在你面前，陪伴你，一起出行。\n虽然有时很喜欢这种“依赖”，即使我很不想割舍掉，但是我希望，你能够找到真正喜欢的爱好，就算是不依赖我又如何。希望这些能让你变的更独立更自我！你很聪明很敏感，或许心理学和你所感兴趣的法学都是不错的选择！（利兹的心理学好像也不错，或许可以旁听旁听）有些事是需要随着时间不断流逝，坚持住才能发现其中的美妙呀。\n我觉得我是属于很能给自己找事的人，脑子里想要学的东西估计足够堆到下辈子了。想和你一起看历史，学语言，下围棋，甚至最近还想看心理学（竟然能把复杂的人性解释并归类，太神奇了！！！真的推荐宝也看看）好奇心简直泛滥成灾。甚至会为了写漂亮的代码付出多几倍的努力，从这些碎片中汲取一些快乐和满足。宝在国外，我能分享的机会少了，但是新的环境或许更有新的主题呢。即使在国内，我也想和宝一起上课读PPT，还一直希望和宝一起学一门外语，法语西班牙语日语韩语俄语，我们可以一起去巴塞罗那，德国看风景\n宝，像你这样能坚持自我的人格真的很好，你一定能够发现兴趣并且坚持下去的，变得更独立更加自我。我也希望能够和你一起度过这段枯燥而又难过的日子，我会努力做好牵起你的手来\n我知道，我非常非常不会分享，不会哄人，也很难和你一起享受共同的时间（不能一起出去旅游拍照😭）。我想，我会努力让自己的生活充盈起来，变得鲜活，和你分享喜悦。或许当我做到的时候，我都要变成一个外向的人了，一个想只对宝外向的人\n我是个大直男，整天只知道问问题，不会聊天不会讲故事哈哈哈哈哈哈哈 这闭塞的状态也让我难受 我一定能变成更好的人 让我们的关系变得更好\n我爱你，很想你，抱抱你 宝\n","date":"2023-10-13T10:38:00Z","permalink":"https://MyLoveES.github.io/p/letter-for-my-dear/","title":"Letter for my dear"},{"content":"一、背景 手机原生通话应用，实现流式语音翻译。\n二、差异 和以往客户接入不同，需要我们自行监听RTP流(通过SDP描述文件)，并且终端输入的音频流并非我们可接受的WAV/PCM格式，而是AMR-WB，需要我们监听RTP流获取音频数据。终端通信设备是现网设备，编码方式无法变更，需要我们来实现音频数据的解码。\n初版实现 借助FFmpeg输入SDP监听对应地址的RTP，并根据SDP中描述的音频编码进行解码以及转格式，直接输出到WAV文件中。Java程序读取WAV文件，做ASR+翻译。\n三、预期外的困难与解决方式 1. AMR-WB编码音频，FFmpeg不能完全支持解码 AMR-WB编码分为两种，Bandwidth-efficient 和 Octet-aligned。\n阅读RFC 4867协议后，发现bandwidth-efficient和octet-aligned之间的差距并不大，内容不变，只是排除了中间的填充字节，更紧凑了些：\nbandwidth efficient header: table: payload: octet aligned header: table: payload: 终端所采用的AMR-WB并非常见的Octet-aligned，而是Bandwidth-efficient。不幸，FFmpeg并不支持这种模式的编码：rtpdec_amr.c 方案1: python(RTP解码+重编码) -\u0026gt; ffmpeg转格式 -\u0026gt; ASR 运维帮忙找到了一个脚本，可以实现AMR-WB Bandwidth-efficient音频的解码，并提取出其中有效的payload，输出到文件中。最终生成的文件可以通过ffmpeg进一步转格式，得到可接受的WAV。基于这个脚本，运维开发了一个简单的解码器，针对于这种格式，监听RTP流并转码。 但是，这种方式会带来新的问题：\n如果终端的数据不是AMR格式，不适用于这条路，还需要走老流程，所以对于音频处理会分成两条路： 如果在通信过程中发生音频编码的变化（SDP信息更新），Java服务需要在两条路之间来回切换，增加了稳定性风险和复杂度。所以在有了保底方案之后，继续尝试另一种方式。 方案2: 修改ffmpeg，支持AMR Bandwidth-efficient编码 1.简单分析一下FFmpeg对amr的解码部分，对于octet-aligned模式，实际上就是按照字节读取，逐帧剥离数据，送到后面的decoder： 最终送去转码的audio-data结构： 那么对于bandwidth-efficient，payload一致的前提下，我们只需要重写解析逻辑，按位读取，并在适当的位置填充0bit，即可转化成octet-aligned模式的数据，继续后面的解码： 2. 静音期间，解码器不工作 当碰到SID的时候，AMR-decoder不会对音频数据进行解码，而是选择“忽略”。造成在静音的一段时间内，服务无法收到解码后的数据，也就无法向ASR发送音频数据。而ASR无法收到足够的音频，也无法进行识别，从终端体验来看，像是一句话的最后几个字“被吞了”，直到开始说下一句话才继续识别： 方案1: 让FFmpeg支持SID解码时填充静音数据 难度大，否\n方案2: 让ASR支持客户端主动发起断句，断句后继续维持websocket识别 算法改动麻烦，否\n方案3: NIO异步从socket读取数据，加设超时时间 在静音时段内，将无法收到bytes，一旦触发超时，客户端主动断开websocket，触发ASR断句，然后再重新建立新的链接\n四、最终实现 ","date":"2023-08-29T00:00:00Z","permalink":"https://MyLoveES.github.io/p/ffmpeg-develop-for-amrwb-bandwidth-efficient/","title":"ffmpeg develop for amrwb bandwidth efficient"},{"content":" R: 4.3.2 (2023-10-31)\nR studio: 2023.12.1+402 (2023.12.1+402)\n1. Manage customer hierarchical 1.1 segmentation According to the demographics data provided, \u0026lsquo;Emplyment\u0026rsquo; data was excluded because it was related to salary. The data with \u0026lsquo;salary=7\u0026rsquo; was removed because it was invalid. \u0026lsquo;Regions\u0026rsquo; are simplified to 3. Finally the continuous vectors are normalized and the ordinal variables are factored.\nConsidering data contains ordinal varibles and continous variables, we segment customers into four groups based on hierarchical clustering, which differentiation between groups was more appropriate than three or five groups.\n1.1.1 import and check data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 \u0026gt; seg.df \u0026lt;- read.csv(\u0026#34;1_demographics.csv\u0026#34;, stringsAsFactors = TRUE) \u0026gt; head(seg.df, n = 5) Consumer_id Age_Group Gender Salary Education Employment Location_by_region Choco_Consumption 1 352 1 2 1 2 3 4 0 2 103 3 2 3 1 1 1 1 3 17 1 1 1 1 4 1 1 4 66 1 1 1 1 3 1 1 5 329 2 1 1 1 8 1 1 Sustainability_Score 1 -1.15059 2 0.50287 3 -1.14517 4 -0.46688 5 -2.08817 \u0026gt; summary(seg.df, digits = 2) Consumer_id Age_Group Gender Salary Education Employment Location_by_region Min. : 1 Min. :1.0 Min. :1.0 Min. :0.0 Min. :1.0 Min. : 1.0 Min. :1.0 1st Qu.: 95 1st Qu.:1.0 1st Qu.:1.0 1st Qu.:1.0 1st Qu.:2.0 1st Qu.: 1.0 1st Qu.:1.0 Median :190 Median :2.0 Median :2.0 Median :2.0 Median :2.0 Median : 2.0 Median :1.0 Mean :190 Mean :2.2 Mean :1.7 Mean :2.3 Mean :2.3 Mean : 2.4 Mean :1.3 3rd Qu.:284 3rd Qu.:3.0 3rd Qu.:2.0 3rd Qu.:3.0 3rd Qu.:3.0 3rd Qu.: 3.0 3rd Qu.:1.0 Max. :378 Max. :4.0 Max. :2.0 Max. :7.0 Max. :4.0 Max. :10.0 Max. :8.0 Choco_Consumption Sustainability_Score Min. :0 Min. :-3.26 1st Qu.:2 1st Qu.:-0.61 Median :3 Median : 0.14 Mean :3 Mean : 0.00 3rd Qu.:4 3rd Qu.: 0.68 Max. :5 Max. : 2.04 \u0026gt; \u0026gt; ### 1.1.1 remove consumer_id in data set, and set consumer_id as row name \u0026gt; rownames(seg.df) \u0026lt;- seg.df[, 1] \u0026gt; seg.df \u0026lt;- seg.df[, -1] \u0026gt; #### remove salary = 7, invalid data \u0026gt; seg.df \u0026lt;- seg.df[seg.df$Salary != 7, ] \u0026gt; #### remove Employment, which related to Salary \u0026gt; seg.df \u0026lt;- subset(seg.df, select = -Employment) \u0026gt; #### just split region to 3 groups \u0026gt; seg.df$Location_by_region[seg.df$Location_by_region \u0026gt; 2] \u0026lt;- 3 1 2 3 4 5 6 hist(seg.df$Sustainability_Score, main = \u0026#34;All customers\u0026#34;, xlab = \u0026#34;Sustainability_Score\u0026#34;, ylab = \u0026#34;Count\u0026#34;, col = \u0026#34;lightblue\u0026#34; # colore the bars ) {% asset_image final_1.png %}\n1 2 3 4 5 6 hist(seg.df$Choco_Consumption, main = \u0026#34;All customers\u0026#34;, xlab = \u0026#34;Choco_Consumption\u0026#34;, ylab = \u0026#34;Count\u0026#34;, col = \u0026#34;lightblue\u0026#34; # colore the bars ) {% asset_image final_2.png %}\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 \u0026gt; head(seg.df, n = 5) Age_Group Gender Salary Education Location_by_region Choco_Consumption Sustainability_Score 352 1 2 1 2 3 0 -1.15059 103 3 2 3 1 1 1 0.50287 17 1 1 1 1 1 1 -1.14517 66 1 1 1 1 1 1 -0.46688 329 2 1 1 1 1 1 -2.08817 \u0026gt; summary(seg.df, digits = 2) Age_Group Gender Salary Education Location_by_region Choco_Consumption Min. :1.0 Min. :1.0 Min. :0.0 Min. :1.0 Min. :1.0 Min. :0 1st Qu.:1.0 1st Qu.:1.0 1st Qu.:1.0 1st Qu.:2.0 1st Qu.:1.0 1st Qu.:2 Median :2.0 Median :2.0 Median :2.0 Median :2.0 Median :1.0 Median :3 Mean :2.1 Mean :1.7 Mean :2.2 Mean :2.3 Mean :1.2 Mean :3 3rd Qu.:3.0 3rd Qu.:2.0 3rd Qu.:3.0 3rd Qu.:3.0 3rd Qu.:1.0 3rd Qu.:4 Max. :4.0 Max. :2.0 Max. :6.0 Max. :4.0 Max. :3.0 Max. :5 Sustainability_Score Min. :-3.3e+00 1st Qu.:-6.1e-01 Median : 1.4e-01 Mean :-9.6e-05 3rd Qu.: 6.8e-01 Max. : 2.0e+00 \u0026gt; str(seg.df) \u0026#39;data.frame\u0026#39;:\t373 obs. of 7 variables: $ Age_Group : int 1 3 1 1 2 4 2 1 1 1 ... $ Gender : int 2 2 1 1 1 1 2 2 1 1 ... $ Salary : int 1 3 1 1 1 4 2 3 3 3 ... $ Education : int 2 1 1 1 1 1 1 2 2 2 ... $ Location_by_region : num 3 1 1 1 1 1 1 1 1 1 ... $ Choco_Consumption : int 0 1 1 1 1 1 1 1 1 1 ... $ Sustainability_Score: num -1.151 0.503 -1.145 -0.467 -2.088 ... \u0026gt; seg.df.sc \u0026lt;- seg.df \u0026gt; #### just scale continous variables \u0026gt; seg.df.sc[, c(6,7)] \u0026lt;- scale(seg.df[ , c(6,7)]) \u0026gt; #### factor ordered ordinals \u0026gt; seg.df.sc$Age_Group \u0026lt;- factor(seg.df.sc$Age_Group, ordered = TRUE) \u0026gt; seg.df.sc$Salary \u0026lt;- factor(seg.df.sc$Salary, ordered = TRUE) \u0026gt; seg.df.sc$Education \u0026lt;- factor(seg.df.sc$Education, ordered = TRUE) \u0026gt; seg.df.sc$Gender \u0026lt;- factor(seg.df.sc$Gender, ordered = FALSE) \u0026gt; seg.df.sc$Location_by_region \u0026lt;- factor(seg.df.sc$Location_by_region, ordered = FALSE) \u0026gt; head(seg.df.sc, n = 5) Age_Group Gender Salary Education Location_by_region Choco_Consumption Sustainability_Score 352 1 2 1 2 3 -2.449983 -1.1457402 103 3 2 3 1 1 -1.627442 0.5008876 17 1 1 1 1 1 -1.627442 -1.1403426 66 1 1 1 1 1 -1.627442 -0.4648553 329 2 1 1 1 1 -1.627442 -2.0794461 \u0026gt; summary(seg.df.sc, digits = 2) Age_Group Gender Salary Education Location_by_region Choco_Consumption Sustainability_Score 1:140 1:119 0: 34 1: 76 1:299 Min. :-2.450 Min. :-3.25 2:120 2:254 1:116 2:183 2: 58 1st Qu.:-0.805 1st Qu.:-0.61 3: 35 2: 40 3: 43 3: 16 Median : 0.018 Median : 0.14 4: 78 3:123 4: 71 Mean : 0.000 Mean : 0.00 4: 41 3rd Qu.: 0.840 3rd Qu.: 0.68 5: 14 Max. : 1.663 Max. : 2.03 6: 5 \u0026gt; str(seg.df.sc) \u0026#39;data.frame\u0026#39;:\t373 obs. of 7 variables: $ Age_Group : Ord.factor w/ 4 levels \u0026#34;1\u0026#34;\u0026lt;\u0026#34;2\u0026#34;\u0026lt;\u0026#34;3\u0026#34;\u0026lt;\u0026#34;4\u0026#34;: 1 3 1 1 2 4 2 1 1 1 ... $ Gender : Factor w/ 2 levels \u0026#34;1\u0026#34;,\u0026#34;2\u0026#34;: 2 2 1 1 1 1 2 2 1 1 ... $ Salary : Ord.factor w/ 7 levels \u0026#34;0\u0026#34;\u0026lt;\u0026#34;1\u0026#34;\u0026lt;\u0026#34;2\u0026#34;\u0026lt;\u0026#34;3\u0026#34;\u0026lt;..: 2 4 2 2 2 5 3 4 4 4 ... $ Education : Ord.factor w/ 4 levels \u0026#34;1\u0026#34;\u0026lt;\u0026#34;2\u0026#34;\u0026lt;\u0026#34;3\u0026#34;\u0026lt;\u0026#34;4\u0026#34;: 2 1 1 1 1 1 1 2 2 2 ... $ Location_by_region : Factor w/ 3 levels \u0026#34;1\u0026#34;,\u0026#34;2\u0026#34;,\u0026#34;3\u0026#34;: 3 1 1 1 1 1 1 1 1 1 ... $ Choco_Consumption : num -2.45 -1.63 -1.63 -1.63 -1.63 ... $ Sustainability_Score: num -1.146 0.501 -1.14 -0.465 -2.079 ... 1.1.2 hclust() Considering data contains ordinal varibles and continous variables, use hclust().\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026gt; ### 1.1.2 Hierarchical clustering: hclust() \u0026gt; seg.dist \u0026lt;- dist(seg.df.sc) \u0026gt; # seg.dist \u0026lt;- daisy(seg.df.sc, metric = \u0026#34;gower\u0026#34;) \u0026gt; as.matrix(seg.dist)[1:7, 1:7] 352 103 17 66 329 241 34 352 0.000000 4.048204 2.5839125 2.6721113 2.923762 4.977689 2.821760 103 4.048204 0.000000 3.4195960 3.1516122 3.557825 2.618749 1.799080 17 2.583913 3.419596 0.0000000 0.6754873 1.371829 4.254911 1.811081 66 2.672111 3.151612 0.6754873 0.0000000 1.899185 4.358532 1.738220 329 2.923762 3.557825 1.3718292 1.8991849 0.000000 3.657826 2.038581 241 4.977689 2.618749 4.2549114 4.3585316 3.657826 0.000000 3.118659 34 2.821760 1.799080 1.8110812 1.7382205 2.038581 3.118659 0.000000 \u0026gt; seg.hc \u0026lt;- hclust(seg.dist, method=\u0026#34;complete\u0026#34;) \u0026gt; plot(seg.hc) {% asset_image final_3.png %}\n1 \u0026gt; rect.hclust(seg.hc, k=4, border = \u0026#34;red\u0026#34;) {% asset_image final_4.png %}\n1 2 3 4 5 \u0026gt; seg.hc.segment \u0026lt;- cutree(seg.hc, k=4) #membership vector for 4 groups \u0026gt; table(seg.hc.segment) #counts seg.hc.segment 1 2 3 4 78 172 88 35 data size of groups\n1 2 3 4 5 6 7 \u0026gt; clusplot(seg.df, seg.hc.segment, + color = TRUE, #color the groups + shade = TRUE, #shade the ellipses for group membership + labels = 4, #label only the groups, not the individual points + lines = 0, #omit distance lines between groups + main = \u0026#34;Hierarchical cluster plot\u0026#34; # figure title + ) {% asset_image final_5.png %}\nGraph split to 4 groups\n1 2 3 4 5 6 \u0026gt; aggregate(seg.df, list(seg.hc.segment), mean) Group.1 Age_Group Gender Salary Education Location_by_region Choco_Consumption Sustainability_Score 1 1 1.692308 1.602564 1.756410 2.000000 1.282051 3.333333 -1.1191478 2 2 1.441860 1.686047 1.616279 1.819767 1.377907 2.668605 0.3161955 3 3 3.272727 1.659091 3.818182 2.977273 1.011364 3.397727 0.1015306 4 4 3.685714 1.885714 2.228571 3.542857 1.057143 2.657143 0.6839269 Well-differentiated: Salary(Group3)、Education(Group4)、Choco_Consumption(Group1、Group3)\n1 \u0026gt; boxplot(seg.df$Salary ~ seg.hc.segment, ylab = \u0026#34;Salary\u0026#34;, xlab = \u0026#34;Cluster\u0026#34;) {% asset_image final_6.png %}\nWell-differentiated: Salary(Group3)\n1 \u0026gt; boxplot(seg.df$Education ~ seg.hc.segment, ylab = \u0026#34;Education\u0026#34;, xlab = \u0026#34;Cluster\u0026#34;) {% asset_image final_7.png %}\nWell-differentiated: Education(Group4)\n1 \u0026gt; boxplot(seg.df$Choco_Consumption ~ seg.hc.segment, ylab = \u0026#34;Choco_Consumption\u0026#34;, xlab = \u0026#34;Cluster\u0026#34;) {% asset_image final_8.png %}\nDifferentiated not welled, ignore\n1.1.3 Conclusion Group. Age_Group Gender Salary Education Location_by_region Choco_Consumption Sustainability_Score 1 1.692308 1.602564 1.75641 2.000000 1.282051 3.333333 -1.1191478 2 1.441860 1.686047 1.61628 1.819767 1.377907 2.668605 0.3161955 3 3.272727 1.659091 3.81818 2.977273 1.011364 3.397727 0.1015306 4 3.685714 1.885714 2.22857 3.542857 1.057143 2.657143 0.6839269 Graph split to 4 groups(Only explained 50.93% of the data), we see that group3 is modestly well-differentiated. 88 customers with higher salary and higher consumption of choco, means more potential to pay at a higher price. Second target is group4, higher educated rate and more sensitive to sustainability, companies can move towards sustainability and attract potential highly educated customers.\n1.2 Factor analysis Remove unnecesssary columns from data: product_id,company_location,review_date,country_of_bean_origin,first_taste,second_taste,third_taste. Use heatmap and corrplot to visualizing the relation between factors. Finally using PCA find the position of brand, and looking for future brand positioning.\n1.2.1 import and check data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 \u0026gt; brand.ratings \u0026lt;- read.csv(\u0026#34;2_chocolate_rating.csv\u0026#34;, stringsAsFactors = TRUE) \u0026gt; \u0026gt; #### remove unnecessary columns \u0026gt; brand.ratings \u0026lt;- subset(brand.ratings, select = -c(product_id,company_location,review_date,country_of_bean_origin,first_taste,second_taste,third_taste)) \u0026gt; head(brand.ratings) brand cocoa_percent rating counts_of_ingredients cocoa_butter vanilla organic salt sugar sweetener 1 A. Morin 70 4.00 2 4 8 8 2 9 7 2 A. Morin 70 3.75 1 1 4 7 1 1 1 3 A. Morin 70 3.50 2 3 5 9 2 9 5 4 A. Morin 70 2.75 1 6 10 8 3 4 5 5 A. Morin 70 3.50 1 1 5 8 1 9 9 6 A. Morin 63 3.75 2 8 9 5 3 8 7 \u0026gt; summary(brand.ratings) brand cocoa_percent rating counts_of_ingredients cocoa_butter vanilla organic salt Soma : 11 Min. : 60.00 Min. :2.250 Min. : 1.000 Min. : 1.000 Min. : 1.000 Min. : 1.000 Min. : 1.000 Arete : 10 1st Qu.: 70.00 1st Qu.:3.000 1st Qu.: 2.000 1st Qu.: 3.000 1st Qu.: 6.000 1st Qu.: 3.000 1st Qu.: 2.000 Cacao de Origen : 10 Median : 70.00 Median :3.250 Median : 4.000 Median : 6.000 Median : 8.000 Median : 5.000 Median : 5.500 hexx : 8 Mean : 71.53 Mean :3.285 Mean : 5.078 Mean : 5.663 Mean : 7.407 Mean : 5.275 Mean : 5.384 Smooth Chocolator, The: 8 3rd Qu.: 74.00 3rd Qu.:3.500 3rd Qu.: 9.000 3rd Qu.: 8.000 3rd Qu.: 9.000 3rd Qu.: 8.000 3rd Qu.: 9.000 A. Morin : 7 Max. :100.00 Max. :4.000 Max. :10.000 Max. :10.000 Max. :10.000 Max. :10.000 Max. :10.000 (Other) :204 sugar sweetener Min. : 1.000 Min. : 1.000 1st Qu.: 2.000 1st Qu.: 3.000 Median : 4.000 Median : 4.000 Mean : 4.155 Mean : 4.415 3rd Qu.: 6.000 3rd Qu.: 6.000 Max. :10.000 Max. :10.000 \u0026gt; str(brand.ratings) \u0026#39;data.frame\u0026#39;:\t258 obs. of 10 variables: $ brand : Factor w/ 56 levels \u0026#34;A. Morin\u0026#34;,\u0026#34;Altus aka Cao Artisan\u0026#34;,..: 1 1 1 1 1 1 1 2 2 2 ... $ cocoa_percent : num 70 70 70 70 70 63 70 60 60 60 ... $ rating : num 4 3.75 3.5 2.75 3.5 3.75 3.5 3 2.75 2.5 ... $ counts_of_ingredients: int 2 1 2 1 1 2 1 2 2 3 ... $ cocoa_butter : int 4 1 3 6 1 8 1 1 1 1 ... $ vanilla : int 8 4 5 10 5 9 5 7 8 9 ... $ organic : int 8 7 9 8 8 5 7 5 10 8 ... $ salt : int 2 1 2 3 1 3 1 2 1 1 ... $ sugar : int 9 1 9 4 9 8 5 8 7 3 ... $ sweetener : int 7 1 5 5 9 7 1 7 7 3 ... 1.2.2 Rescaling the data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 \u0026gt; brand.sc \u0026lt;- brand.ratings \u0026gt; brand.sc[,2:10] \u0026lt;- scale (brand.ratings[,2:10]) \u0026gt; head(brand.sc) brand cocoa_percent rating counts_of_ingredients cocoa_butter vanilla organic salt sugar sweetener 1 A. Morin -0.315311 1.8112654 -0.8883386 -0.5795367 0.2596824 0.96349072 -0.9958673 1.7971812 1.2538261 2 A. Morin -0.315311 1.1780588 -1.1769927 -1.6251343 -1.4919010 0.60989099 -1.2901786 -1.1703244 -1.6561032 3 A. Morin -0.315311 0.5448522 -0.8883386 -0.9280692 -1.0540051 1.31709044 -0.9958673 1.7971812 0.2838497 4 A. Morin -0.315311 -1.3547676 -1.1769927 0.1175284 1.1354741 0.96349072 -0.7015560 -0.0575098 0.2838497 5 A. Morin -0.315311 0.5448522 -1.1769927 -1.6251343 -1.0540051 0.96349072 -1.2901786 1.7971812 2.2238026 6 A. Morin -1.755138 1.1780588 -0.8883386 0.8145935 0.6975783 -0.09730845 -0.7015560 1.4262430 1.2538261 \u0026gt; summary(brand.sc) brand cocoa_percent rating counts_of_ingredients cocoa_butter vanilla organic Soma : 11 Min. :-2.3722 Min. :-2.62118 Min. :-1.1770 Min. :-1.6251 Min. :-2.8056 Min. :-1.51171 Arete : 10 1st Qu.:-0.3153 1st Qu.:-0.72156 1st Qu.:-0.8883 1st Qu.:-0.9281 1st Qu.:-0.6161 1st Qu.:-0.80451 Cacao de Origen : 10 Median :-0.3153 Median :-0.08835 Median :-0.3110 Median : 0.1175 Median : 0.2597 Median :-0.09731 hexx : 8 Mean : 0.0000 Mean : 0.00000 Mean : 0.0000 Mean : 0.0000 Mean : 0.0000 Mean : 0.00000 Smooth Chocolator, The: 8 3rd Qu.: 0.5074 3rd Qu.: 0.54485 3rd Qu.: 1.1322 3rd Qu.: 0.8146 3rd Qu.: 0.6976 3rd Qu.: 0.96349 A. Morin : 7 Max. : 5.8554 Max. : 1.81127 Max. : 1.4209 Max. : 1.5117 Max. : 1.1355 Max. : 1.67069 (Other) :204 salt sugar sweetener Min. :-1.29018 Min. :-1.17032 Min. :-1.6561 1st Qu.:-0.99587 1st Qu.:-0.79939 1st Qu.:-0.6861 Median : 0.03422 Median :-0.05751 Median :-0.2011 Mean : 0.00000 Mean : 0.00000 Mean : 0.0000 3rd Qu.: 1.06431 3rd Qu.: 0.68437 3rd Qu.: 0.7688 Max. : 1.35862 Max. : 2.16812 Max. : 2.7088 \u0026gt; str(brand.sc) \u0026#39;data.frame\u0026#39;:\t258 obs. of 10 variables: $ brand : Factor w/ 56 levels \u0026#34;A. Morin\u0026#34;,\u0026#34;Altus aka Cao Artisan\u0026#34;,..: 1 1 1 1 1 1 1 2 2 2 ... $ cocoa_percent : num -0.315 -0.315 -0.315 -0.315 -0.315 ... $ rating : num 1.811 1.178 0.545 -1.355 0.545 ... $ counts_of_ingredients: num -0.888 -1.177 -0.888 -1.177 -1.177 ... $ cocoa_butter : num -0.58 -1.625 -0.928 0.118 -1.625 ... $ vanilla : num 0.26 -1.49 -1.05 1.14 -1.05 ... $ organic : num 0.963 0.61 1.317 0.963 0.963 ... $ salt : num -0.996 -1.29 -0.996 -0.702 -1.29 ... $ sugar : num 1.7972 -1.1703 1.7972 -0.0575 1.7972 ... $ sweetener : num 1.254 -1.656 0.284 0.284 2.224 ... \u0026gt; cor(brand.sc[,2:10]) cocoa_percent rating counts_of_ingredients cocoa_butter vanilla organic salt sugar sweetener cocoa_percent 1.000000000 -0.052546644 0.07123417 0.077370649 -0.02206531 -0.04113143 0.17036000 -0.03957938 0.003483658 rating -0.052546644 1.000000000 0.01437280 0.005272191 -0.09133055 0.04277047 -0.04554864 0.03054233 0.003668166 counts_of_ingredients 0.071234169 0.014372799 1.00000000 0.711183570 0.05157357 -0.57011292 0.72006935 -0.10961457 -0.106926187 cocoa_butter 0.077370649 0.005272191 0.71118357 1.000000000 -0.02054327 -0.50977572 0.74932496 -0.13708717 -0.040725138 vanilla -0.022065307 -0.091330547 0.05157357 -0.020543269 1.00000000 0.08983402 -0.00917223 0.07187530 0.111933243 organic -0.041131426 0.042770467 -0.57011292 -0.509775721 0.08983402 1.00000000 -0.61074144 0.10206892 0.091787605 salt 0.170360004 -0.045548635 0.72006935 0.749324961 -0.00917223 -0.61074144 1.00000000 -0.13863007 -0.134994219 sugar -0.039579384 0.030542330 -0.10961457 -0.137087169 0.07187530 0.10206892 -0.13863007 1.00000000 0.629589888 sweetener 0.003483658 0.003668166 -0.10692619 -0.040725138 0.11193324 0.09178760 -0.13499422 0.62958989 1.000000000 \u0026gt; corrplot(cor(brand.sc[,2:10]), order = \u0026#34;hclust\u0026#34;) {% asset_image final_9.png %}\norganic un-related with countsOfIngredients、cocoaButter and salt salt related with countsOfIngredients and cocoaButter\n1.2.3 Mean rating by brand 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 \u0026gt; brand.mean \u0026lt;- aggregate(. ~ brand, data=brand.sc, mean) \u0026gt; brand.mean brand cocoa_percent rating counts_of_ingredients cocoa_butter vanilla organic salt sugar sweetener 1 A. Morin -0.521000636 0.63531027 -1.05328377 -0.7786981 -0.36588308 0.76143373 -1.03791178 0.84334011 0.283849693 2 Altus aka Cao Artisan -1.857983139 -1.19646594 -0.81617505 -1.6251343 -0.06873946 0.69829093 -1.21660078 0.22069385 0.041355584 3 Ambrosia -0.366733424 0.06994724 -0.88833857 -1.0152024 -0.28768739 1.05189065 -1.14302296 -0.61391709 -0.686126742 4 Amedei -0.109621404 -0.59491968 -0.77287693 -0.6492432 0.34726159 0.96349072 -0.93700505 0.38761604 1.156828484 5 Arete -0.253604135 0.67149350 -1.06153103 -0.9629225 0.25968242 0.68061094 -1.08416070 -0.28007272 -0.007143237 6 Bonnat 0.198913020 1.17805877 -1.03266562 -0.8409361 -0.39716135 1.05189065 -1.06944513 0.03522475 0.162602639 7 Brasstown aka It\u0026#39;s Chocolate -0.191897250 0.92477613 -1.11926184 -0.7886562 0.43484076 0.89277077 -0.99586731 0.01667784 0.574842623 8 Brazen -0.315311020 -0.56325935 -1.03266562 -0.8409361 0.04073450 0.52149106 -1.14302296 -0.15024435 -0.443632633 9 Bright -0.041058199 0.12271446 -1.08077463 -1.2766018 -1.34593569 1.31709044 -1.19207484 1.67353512 0.607175171 10 Burnt Fork Bend -0.726690252 -0.51049213 -0.88833857 -0.3471816 -0.76207456 0.84562414 -0.99586731 0.18978233 -0.686126742 11 Cacao de Origen 0.507447444 -0.40495770 -1.06153103 -1.1023355 -0.17821343 1.06957063 -1.14302296 0.16505312 0.332348515 12 Castronovo -0.212466212 0.06994724 -1.17699266 -0.5795367 -0.17821343 1.14029058 -0.99586731 0.31342840 0.283849693 13 Chocolarder -0.109621404 -1.51306923 -1.10482914 -0.3181373 0.58810431 1.22869051 -0.84871166 0.31342840 0.283849693 14 Chocolate Con Amor -0.058199000 -0.56325935 -0.96050209 -0.1438710 0.25968242 0.87509079 -0.84871166 0.68436660 0.041355584 15 Chocolate Makers 1.227361100 0.54485218 -1.03266562 -1.6251343 0.69757827 0.96349072 -1.29017861 -1.17032439 -0.686126742 16 Domori -0.315311020 1.17805877 -0.88833857 -1.2766018 0.47863035 0.60989099 -0.99586731 0.31342840 0.041355584 17 Dormouse 0.850263471 -0.93262986 -1.08077463 -1.2766018 0.40564770 0.37415785 -1.09397107 -0.67574013 -0.201138524 18 Durci -0.315311020 0.75592104 -0.79212054 -0.9861580 -0.03224814 0.96349072 -1.14302296 1.24077389 0.930500649 19 East Van Roasters -0.315311020 0.54485218 -1.08077463 -1.1604242 0.84354355 0.84562414 -1.09397107 0.06613627 0.283849693 20 Fossa 0.044645808 -0.40495770 -0.74401152 -0.9280692 0.36915638 0.87509079 -0.92228948 0.59163205 0.283849693 21 Franceschi -0.109621404 -0.17881249 -1.05328377 -0.8782789 -0.05310033 1.21606195 -1.03791178 0.31342840 0.075997600 22 Fresco -0.178184609 0.22824889 -0.16670333 -0.6957142 0.18669978 -0.27410831 0.03422224 0.37525143 -0.524464002 23 Georgia Ramon 0.370321033 0.12271446 0.84358600 0.6403272 0.69757827 -0.92237447 0.67189673 0.37525143 -0.039475785 24 Habitual 0.164631417 -0.61602657 0.60304092 0.5241497 0.40564770 -0.33304160 0.77000049 0.18978233 0.283849693 25 hexx 0.070357010 -0.87986264 0.41060485 0.2917947 0.09547148 -0.62770803 0.36532246 -0.24297890 0.405096747 26 Hogarth 0.233194623 -0.29942327 0.89169501 0.5822385 0.40564770 -0.21517503 0.42663731 -0.11933283 0.203018324 27 Holy Cacao -0.315311020 0.06994724 0.41060485 0.7274604 0.47863035 -0.71610797 1.13788962 -0.33571345 -0.079891470 28 Johnny Iuzzini 0.219481982 0.41821086 0.84358600 0.2569414 0.60999910 -0.73378795 0.71113823 0.46180368 0.186852050 29 Kto 0.096068212 -0.72156100 0.84358600 0.8145935 0.69757827 -0.59234806 0.59341371 -0.42844799 0.089854406 30 Kyya 0.198913020 -1.14369872 0.55493190 0.5822385 0.69757827 -0.45090817 1.26051933 0.93165873 0.122186954 31 Laia aka Chat-Noir 0.233194623 -0.93262986 0.65114993 0.3498834 0.25968242 -0.92237447 0.86810426 1.05530479 0.768837910 32 Letterpress -0.315311020 0.75592104 -0.11859432 0.4660610 0.69757827 -0.09730845 0.96620803 -0.30480193 0.283849693 33 Mana 0.507447444 -1.77690531 0.26627781 0.1175284 0.11371714 -0.92237447 0.47568919 0.06613627 -0.201138524 34 Map Chocolate -0.521000636 0.33378332 0.50682289 0.2917947 0.25968242 -0.98130776 0.32853354 -0.42844799 0.122186954 35 Maverick -0.315311020 0.06994724 1.06007657 0.9888598 0.47863035 -0.98130776 0.54926702 0.22069385 -0.079891470 36 Mike \u0026amp; Becky -0.315311020 -0.51049213 0.07384175 0.8145935 0.69757827 -0.92237447 0.08327413 -0.18115586 0.283849693 37 Milton -0.315311020 0.86145548 1.42089418 0.4660610 0.47863035 0.07949141 0.32853354 0.68436660 0.041355584 38 Naive 0.198913020 -1.03816429 0.84358600 0.8145935 0.47863035 -0.62770803 1.35862310 -0.42844799 -0.443632633 39 Nibble 0.096068212 -1.35476759 0.77142247 0.7274604 0.25968242 -0.89290783 0.99073397 0.03522475 -0.079891470 40 Nuance -0.315311020 -0.08835441 1.34873066 0.3789278 0.36915638 -1.06970769 0.10780007 -0.24297890 0.405096747 41 Pacari -0.315311020 0.86145548 1.42089418 0.9017266 0.58810431 -0.80450790 0.99073397 0.68436660 -0.201138524 42 Palette de Bine 0.541729047 -0.29942327 0.55493190 0.4079722 0.55161299 -0.39197489 1.06431180 0.49889750 0.688006541 43 Pangea 0.576010649 0.12271446 1.22845812 1.0469485 0.40564770 -0.80450790 0.47568919 -0.18115586 0.122186954 44 Pralus 0.713137060 -0.24665605 0.91574952 0.7274604 0.58810431 -0.36250824 0.40211137 -0.15024435 0.041355584 45 Pump Street Bakery 0.987389881 -0.40495770 -0.02237629 0.8145935 -0.98102248 -0.21517503 0.96620803 -0.42844799 -0.281969894 46 Pura Delizia -0.521000636 0.54485218 -0.59968447 0.4660610 -1.49190097 -0.45090817 0.47568919 -0.24297890 -0.201138524 47 Qantu -0.315311020 0.33378332 -0.11859432 0.9307710 -0.17821343 -0.45090817 1.16241556 -0.42844799 -0.039475785 48 Sirene 1.712200909 0.63531027 0.80234970 0.5158513 -0.67866582 -0.34987968 0.72795602 -0.42844799 -0.478274648 49 Smooth Chocolator, The -0.469578232 0.78230465 0.33844133 0.8145935 -0.94453116 -0.98130776 0.69642267 -0.70665164 -0.504256160 50 Soma -0.090922348 0.89023759 0.44996678 0.5294305 -0.41706571 0.22414584 0.26164461 -0.66449957 -0.730216579 51 Soul 0.198913020 0.22824889 0.77142247 0.6403272 -1.05400512 -0.89290783 1.06431180 -0.52118254 -0.564879687 52 Szanto Tibor -0.315311020 0.38655053 0.84358600 0.2917947 -0.72558324 -0.89290783 0.62284484 -0.89212074 -0.928620850 53 Taste Artisan 0.713137060 0.33378332 1.22845812 0.6984160 0.25968242 0.37415785 0.77000049 -0.79938619 -0.362801263 54 Terroir -0.178184609 0.22824889 0.55493190 0.5822385 -1.49190097 -0.68664132 0.62284484 -0.30480193 -0.766958111 55 Tribar -0.006776596 -0.08835441 -0.02237629 0.2917947 -0.39716135 -0.98130776 1.21146745 -0.42844799 0.041355584 56 Zak\u0026#39;s -0.829535060 -0.24665605 1.27656714 1.3373923 -1.05400512 -0.71610797 0.77000049 -0.52118254 -0.443632633 \u0026gt; \u0026gt; rownames(brand.mean) \u0026lt;- brand.mean[, 1] \u0026gt; #### Use brand for the row name \u0026gt; brand.mean \u0026lt;- brand.mean [, -1] \u0026gt; brand.mean cocoa_percent rating counts_of_ingredients cocoa_butter vanilla organic salt sugar sweetener A. Morin -0.521000636 0.63531027 -1.05328377 -0.7786981 -0.36588308 0.76143373 -1.03791178 0.84334011 0.283849693 Altus aka Cao Artisan -1.857983139 -1.19646594 -0.81617505 -1.6251343 -0.06873946 0.69829093 -1.21660078 0.22069385 0.041355584 Ambrosia -0.366733424 0.06994724 -0.88833857 -1.0152024 -0.28768739 1.05189065 -1.14302296 -0.61391709 -0.686126742 Amedei -0.109621404 -0.59491968 -0.77287693 -0.6492432 0.34726159 0.96349072 -0.93700505 0.38761604 1.156828484 Arete -0.253604135 0.67149350 -1.06153103 -0.9629225 0.25968242 0.68061094 -1.08416070 -0.28007272 -0.007143237 Bonnat 0.198913020 1.17805877 -1.03266562 -0.8409361 -0.39716135 1.05189065 -1.06944513 0.03522475 0.162602639 Brasstown aka It\u0026#39;s Chocolate -0.191897250 0.92477613 -1.11926184 -0.7886562 0.43484076 0.89277077 -0.99586731 0.01667784 0.574842623 Brazen -0.315311020 -0.56325935 -1.03266562 -0.8409361 0.04073450 0.52149106 -1.14302296 -0.15024435 -0.443632633 Bright -0.041058199 0.12271446 -1.08077463 -1.2766018 -1.34593569 1.31709044 -1.19207484 1.67353512 0.607175171 Burnt Fork Bend -0.726690252 -0.51049213 -0.88833857 -0.3471816 -0.76207456 0.84562414 -0.99586731 0.18978233 -0.686126742 Cacao de Origen 0.507447444 -0.40495770 -1.06153103 -1.1023355 -0.17821343 1.06957063 -1.14302296 0.16505312 0.332348515 Castronovo -0.212466212 0.06994724 -1.17699266 -0.5795367 -0.17821343 1.14029058 -0.99586731 0.31342840 0.283849693 Chocolarder -0.109621404 -1.51306923 -1.10482914 -0.3181373 0.58810431 1.22869051 -0.84871166 0.31342840 0.283849693 Chocolate Con Amor -0.058199000 -0.56325935 -0.96050209 -0.1438710 0.25968242 0.87509079 -0.84871166 0.68436660 0.041355584 Chocolate Makers 1.227361100 0.54485218 -1.03266562 -1.6251343 0.69757827 0.96349072 -1.29017861 -1.17032439 -0.686126742 Domori -0.315311020 1.17805877 -0.88833857 -1.2766018 0.47863035 0.60989099 -0.99586731 0.31342840 0.041355584 Dormouse 0.850263471 -0.93262986 -1.08077463 -1.2766018 0.40564770 0.37415785 -1.09397107 -0.67574013 -0.201138524 Durci -0.315311020 0.75592104 -0.79212054 -0.9861580 -0.03224814 0.96349072 -1.14302296 1.24077389 0.930500649 East Van Roasters -0.315311020 0.54485218 -1.08077463 -1.1604242 0.84354355 0.84562414 -1.09397107 0.06613627 0.283849693 Fossa 0.044645808 -0.40495770 -0.74401152 -0.9280692 0.36915638 0.87509079 -0.92228948 0.59163205 0.283849693 Franceschi -0.109621404 -0.17881249 -1.05328377 -0.8782789 -0.05310033 1.21606195 -1.03791178 0.31342840 0.075997600 Fresco -0.178184609 0.22824889 -0.16670333 -0.6957142 0.18669978 -0.27410831 0.03422224 0.37525143 -0.524464002 Georgia Ramon 0.370321033 0.12271446 0.84358600 0.6403272 0.69757827 -0.92237447 0.67189673 0.37525143 -0.039475785 Habitual 0.164631417 -0.61602657 0.60304092 0.5241497 0.40564770 -0.33304160 0.77000049 0.18978233 0.283849693 hexx 0.070357010 -0.87986264 0.41060485 0.2917947 0.09547148 -0.62770803 0.36532246 -0.24297890 0.405096747 Hogarth 0.233194623 -0.29942327 0.89169501 0.5822385 0.40564770 -0.21517503 0.42663731 -0.11933283 0.203018324 Holy Cacao -0.315311020 0.06994724 0.41060485 0.7274604 0.47863035 -0.71610797 1.13788962 -0.33571345 -0.079891470 Johnny Iuzzini 0.219481982 0.41821086 0.84358600 0.2569414 0.60999910 -0.73378795 0.71113823 0.46180368 0.186852050 Kto 0.096068212 -0.72156100 0.84358600 0.8145935 0.69757827 -0.59234806 0.59341371 -0.42844799 0.089854406 Kyya 0.198913020 -1.14369872 0.55493190 0.5822385 0.69757827 -0.45090817 1.26051933 0.93165873 0.122186954 Laia aka Chat-Noir 0.233194623 -0.93262986 0.65114993 0.3498834 0.25968242 -0.92237447 0.86810426 1.05530479 0.768837910 Letterpress -0.315311020 0.75592104 -0.11859432 0.4660610 0.69757827 -0.09730845 0.96620803 -0.30480193 0.283849693 Mana 0.507447444 -1.77690531 0.26627781 0.1175284 0.11371714 -0.92237447 0.47568919 0.06613627 -0.201138524 Map Chocolate -0.521000636 0.33378332 0.50682289 0.2917947 0.25968242 -0.98130776 0.32853354 -0.42844799 0.122186954 Maverick -0.315311020 0.06994724 1.06007657 0.9888598 0.47863035 -0.98130776 0.54926702 0.22069385 -0.079891470 Mike \u0026amp; Becky -0.315311020 -0.51049213 0.07384175 0.8145935 0.69757827 -0.92237447 0.08327413 -0.18115586 0.283849693 Milton -0.315311020 0.86145548 1.42089418 0.4660610 0.47863035 0.07949141 0.32853354 0.68436660 0.041355584 Naive 0.198913020 -1.03816429 0.84358600 0.8145935 0.47863035 -0.62770803 1.35862310 -0.42844799 -0.443632633 Nibble 0.096068212 -1.35476759 0.77142247 0.7274604 0.25968242 -0.89290783 0.99073397 0.03522475 -0.079891470 Nuance -0.315311020 -0.08835441 1.34873066 0.3789278 0.36915638 -1.06970769 0.10780007 -0.24297890 0.405096747 Pacari -0.315311020 0.86145548 1.42089418 0.9017266 0.58810431 -0.80450790 0.99073397 0.68436660 -0.201138524 Palette de Bine 0.541729047 -0.29942327 0.55493190 0.4079722 0.55161299 -0.39197489 1.06431180 0.49889750 0.688006541 Pangea 0.576010649 0.12271446 1.22845812 1.0469485 0.40564770 -0.80450790 0.47568919 -0.18115586 0.122186954 Pralus 0.713137060 -0.24665605 0.91574952 0.7274604 0.58810431 -0.36250824 0.40211137 -0.15024435 0.041355584 Pump Street Bakery 0.987389881 -0.40495770 -0.02237629 0.8145935 -0.98102248 -0.21517503 0.96620803 -0.42844799 -0.281969894 Pura Delizia -0.521000636 0.54485218 -0.59968447 0.4660610 -1.49190097 -0.45090817 0.47568919 -0.24297890 -0.201138524 Qantu -0.315311020 0.33378332 -0.11859432 0.9307710 -0.17821343 -0.45090817 1.16241556 -0.42844799 -0.039475785 Sirene 1.712200909 0.63531027 0.80234970 0.5158513 -0.67866582 -0.34987968 0.72795602 -0.42844799 -0.478274648 Smooth Chocolator, The -0.469578232 0.78230465 0.33844133 0.8145935 -0.94453116 -0.98130776 0.69642267 -0.70665164 -0.504256160 Soma -0.090922348 0.89023759 0.44996678 0.5294305 -0.41706571 0.22414584 0.26164461 -0.66449957 -0.730216579 Soul 0.198913020 0.22824889 0.77142247 0.6403272 -1.05400512 -0.89290783 1.06431180 -0.52118254 -0.564879687 Szanto Tibor -0.315311020 0.38655053 0.84358600 0.2917947 -0.72558324 -0.89290783 0.62284484 -0.89212074 -0.928620850 Taste Artisan 0.713137060 0.33378332 1.22845812 0.6984160 0.25968242 0.37415785 0.77000049 -0.79938619 -0.362801263 Terroir -0.178184609 0.22824889 0.55493190 0.5822385 -1.49190097 -0.68664132 0.62284484 -0.30480193 -0.766958111 Tribar -0.006776596 -0.08835441 -0.02237629 0.2917947 -0.39716135 -0.98130776 1.21146745 -0.42844799 0.041355584 Zak\u0026#39;s -0.829535060 -0.24665605 1.27656714 1.3373923 -1.05400512 -0.71610797 0.77000049 -0.52118254 -0.443632633 \u0026gt; heatmap.2(as.matrix(brand.mean),main = \u0026#34;Brand attributes\u0026#34;, trace = \u0026#34;none\u0026#34;, key = FALSE, dend = \u0026#34;none\u0026#34; + #turn off some options {% asset_image final_10.png %}\nSome brands related with salt, such as Letterpress、Qantu、Pacari、Kyya、Naive\nSome brands related with organic, such as Amedei, Fossa, Castronovo\n1.2.4 Principal component analysis (PCA) using princomp() 1 2 3 4 5 6 7 8 9 10 \u0026gt; brand.pc\u0026lt;- princomp(brand.mean, cor = TRUE) \u0026gt; #We added \u0026#34;cor =TRUE\u0026#34; to use correlation-based one. \u0026gt; summary(brand.pc) Importance of components: Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9 Standard deviation 1.9484877 1.3240155 1.0553760 0.94838484 0.84633623 0.57860956 0.41928231 0.37770849 0.259982952 Proportion of Variance 0.4218449 0.1947797 0.1237576 0.09993709 0.07958722 0.03719878 0.01953307 0.01585152 0.007510126 Cumulative Proportion 0.4218449 0.6166246 0.7403822 0.84031927 0.91990650 0.95710528 0.97663835 0.99248987 1.000000000 \u0026gt; plot(brand.pc,type=\u0026#34;l\u0026#34;) # scree plot {% asset_image final_11.png %}\nThe elbow occurs at component three. This suggests that the first two components explain most of the variation in the observed brand rating.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 \u0026gt; loadings(brand.pc) # pc loadings Loadings: Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9 cocoa_percent 0.121 0.765 0.271 0.553 rating -0.329 -0.242 0.903 counts_of_ingredients 0.466 0.144 0.133 0.274 -0.149 0.748 -0.283 cocoa_butter 0.477 -0.140 -0.122 -0.595 -0.185 0.579 vanilla 0.502 0.376 0.207 -0.686 0.230 -0.185 organic -0.471 0.131 0.102 -0.778 -0.359 salt 0.484 0.106 0.107 -0.557 -0.649 sugar -0.215 0.482 -0.384 0.416 0.593 -0.151 0.113 sweetener -0.190 0.596 -0.134 0.202 0.137 -0.700 0.183 Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9 SS loadings 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 Proportion Var 0.111 0.111 0.111 0.111 0.111 0.111 0.111 0.111 0.111 Cumulative Var 0.111 0.222 0.333 0.444 0.556 0.667 0.778 0.889 1.000 \u0026gt; brand.pc$scores # the principal components Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9 A. Morin -2.6821054 -0.082708580 -1.40994821 0.570725489 0.49085251 0.18338461 0.117281714 -0.138449624 0.222125496 Altus aka Cao Artisan -2.9741352 -0.051591141 -1.85995919 -2.563339244 -1.66300987 0.13097903 0.303201868 0.526449188 -0.685437305 Ambrosia -1.9137525 -2.368899919 0.43991523 -0.678527355 -0.77889642 0.36199971 -0.253511595 0.148898733 -0.152691868 Amedei -2.5823723 1.971553363 0.06548182 -0.167118353 0.21991151 -1.30388873 -0.375674781 0.417123292 -0.136679953 Arete -2.1702056 -0.945790574 0.23190842 0.654926657 -0.90339862 -0.27205032 0.180399208 -0.080326426 0.100535236 Bonnat -2.4005578 -1.197899532 0.06190293 1.469628715 0.58042613 -0.38552569 -0.087080145 0.039968656 0.074068756 Brasstown aka It\u0026#39;s Chocolate -2.5497691 0.133527483 -0.04061230 1.395805810 -0.61251377 -0.84180356 -0.089560874 -0.144162160 0.067272450 Brazen -1.8184009 -1.005611722 0.43555261 -1.230147922 -0.63002787 0.45912632 0.170468440 -0.064477406 0.311590190 Bright -3.7037423 0.414163869 -1.63190725 0.102700490 2.86213684 0.36906420 0.152388138 0.310865000 -0.197363762 Burnt Fork Bend -1.6625728 -1.709402032 -0.88207876 -1.644843428 0.11059053 0.87662539 -0.500865703 -0.110214495 0.338455701 Cacao de Origen -2.5921821 0.055824774 1.09974476 -0.257029615 0.95579544 -0.36550650 0.021096186 0.264035914 -0.075674123 Castronovo -2.5325352 -0.126752242 -0.25831283 -0.004046483 0.33156085 -0.30554860 -0.467607448 -0.168878318 0.136666647 Chocolarder -2.2207668 1.307802587 0.85864663 -1.705624676 -0.19506501 -0.01428509 -0.914520915 -0.321239544 0.159779149 Chocolate Con Amor -1.9007761 0.670491949 0.11709196 -0.620348573 0.29421634 0.61769812 -0.560130738 -0.436771908 0.551468923 Chocolate Makers -1.8579245 -2.204259441 3.65327660 0.944749264 -0.79385429 0.31025351 0.457780690 0.080184237 -0.112091537 Domori -2.4717189 -0.418555050 -0.30112024 1.476626951 -0.84493987 0.44418967 0.556201865 -0.176971531 -0.065225214 Dormouse -1.6163747 -0.573790680 2.78616294 -0.984778126 -0.16073317 -0.22828856 0.696346544 0.154284014 0.109237073 Durci -3.2467009 1.366316064 -1.36938755 1.343686021 0.79119360 -0.14124389 0.102989715 0.247099878 -0.003073146 East Van Roasters -2.6837728 0.239396951 0.28472215 0.845396828 -1.26174552 -0.10762521 0.162505518 -0.184706997 -0.053798374 Fossa -2.3465473 0.884673550 0.38912206 -0.217906869 0.17706864 0.37032870 -0.012113749 0.052158396 -0.059241914 Franceschi -2.5865116 -0.208023319 0.17214188 -0.311579190 0.23054852 0.18080777 -0.351825508 -0.065850200 -0.063028223 Fresco -0.3431384 -0.507481093 -0.23229647 0.070751997 -0.33561536 1.27981256 0.841690012 -0.383017742 -0.156670600 Georgia Ramon 1.6232155 1.150676748 0.26498134 0.776042896 -0.08787525 0.73021028 0.360040767 -0.145401030 0.344721165 Habitual 1.0620088 1.408417040 0.20679968 -0.256329143 0.14603119 -0.09654839 -0.199263082 -0.042347238 -0.204890356 hexx 0.8945803 0.986405829 0.27911766 -0.823341471 0.05290067 -0.91137130 0.215896857 0.391037893 -0.053665334 Hogarth 1.1469231 0.888298143 0.44801300 0.146795869 -0.11225173 -0.17267348 -0.413312452 0.421796760 -0.079016305 Holy Cacao 1.7193074 0.226734338 -0.30964828 0.129600701 -0.98426850 -0.25298514 -0.041078309 -0.590556142 -0.164351653 Johnny Iuzzini 1.1253895 1.247567957 -0.14097718 1.171762685 -0.05453896 0.49259097 0.472104322 -0.020333808 -0.091573476 Kto 1.7272265 0.973823067 0.71344746 -0.485155153 -0.79385313 -0.32876367 -0.322610016 0.219131057 0.094206766 Kyya 1.2196443 2.392140992 0.08609881 -0.785365838 0.47176421 0.99700206 -0.064627360 -0.702134952 -0.261062705 Laia aka Chat-Noir 0.8806390 2.923105952 -0.50273622 -0.361966792 1.15349356 -0.04033924 0.617548018 0.031480014 -0.089609639 Letterpress 0.6051745 0.398736717 -0.33394102 1.207590405 -1.13574976 -0.76005043 -0.322059257 -0.881592716 -0.346321885 Mana 1.2424524 0.909518098 1.17916492 -2.030151126 0.60130333 0.37155938 0.705169097 0.001056193 0.138418715 Map Chocolate 1.1256970 -0.001899391 -0.73005688 0.332513635 -1.15181151 -0.66036279 0.518289540 0.244298956 0.097515381 Maverick 1.8456138 0.805863125 -0.78667732 0.280711960 -0.66679650 0.50954160 0.004065933 0.125501441 0.473074791 Mike \u0026amp; Becky 0.9510565 1.163163024 -0.11474326 -0.450192192 -1.05889582 -0.66072297 0.126707167 -0.134120447 0.814806996 Milton 0.7017978 0.850904718 -1.16300943 1.527503410 -0.35819662 1.14825504 -0.584746995 0.519632202 -0.228518041 Naive 2.4417443 0.319089869 0.92180468 -1.140138484 -0.48419779 0.39789802 -0.287084848 -0.378335636 -0.356746232 Nibble 1.9734880 1.140361527 0.32957158 -1.481479461 0.07908308 0.20080213 0.090557597 -0.062747307 -0.042790815 Nuance 1.4332957 0.986854337 -0.52270722 0.204845388 -0.84737962 -0.60103767 0.416043810 1.168117101 0.091201132 Pacari 1.9470450 0.841192378 -1.30359005 1.428220851 -0.52541881 1.35461573 -0.045551375 -0.117106120 0.063566252 Palette de Bine 0.9298584 2.257452252 0.37466152 0.618514285 0.71206947 -0.35624249 0.067529550 -0.227881300 -0.381700241 Pangea 2.0918919 0.743086202 0.64457687 0.845258973 0.09714474 -0.12746214 -0.154811282 0.523133595 0.462947830 Pralus 1.5025425 0.848741299 1.24672256 0.437250083 0.09124140 0.12844654 -0.313421202 0.312496177 0.232607192 Pump Street Bakery 1.6307924 -1.119099536 1.05475875 -0.464568575 1.97645672 -0.54624494 -0.286472980 -0.427151024 0.025087747 Pura Delizia 0.5045011 -2.031682772 -1.77111102 -0.238237657 0.96235532 -0.92302206 0.202111967 -0.518488381 0.184936862 Qantu 1.4325823 -0.555242055 -0.70143518 0.194149325 -0.26356820 -0.83060181 -0.317473645 -0.896013710 -0.059255689 Sirene 2.0287743 -1.465866212 1.95083466 1.357465166 2.09862837 0.23708772 0.107912320 0.223293081 0.027220665 Smooth Chocolator, The 1.9426612 -2.219944073 -1.28110363 0.197984376 -0.10148814 -0.56237841 0.234920584 -0.116630110 0.250077177 Soma 1.0272560 -2.276609319 -0.13465029 0.682516433 -0.32209432 0.33522067 -0.710966281 0.028455663 -0.046039440 Soul 2.3573617 -1.820518931 -0.34243984 -0.138235420 0.91362197 -0.10331072 0.292826044 0.090846725 -0.195176426 Szanto Tibor 2.1190874 -2.573833100 -0.46454741 -0.315723730 -0.48273243 0.27114066 0.403885143 0.332333083 -0.201129415 Taste Artisan 1.8160636 -0.801618403 1.48053048 0.899555134 -0.11734995 0.13028873 -1.102099959 0.394653864 -0.541862665 Terroir 1.7764533 -2.426405123 -1.13302992 -0.574480808 1.08742023 0.25221874 0.173655136 0.175666615 0.038227332 Tribar 1.5130001 -0.385955737 -0.26682521 -0.265539271 0.26746465 -0.96064380 0.684999665 -0.541489005 -0.348867793 Zak\u0026#39;s 2.5174379 -1.426444229 -1.78790183 -1.117084841 -0.02701300 -0.28062025 -0.678142919 0.633397547 0.043738507 \u0026gt; \u0026gt; biplot(brand.pc, main = \u0026#34;Brand positioning\u0026#34;) {% asset_image final_12.png %}\nIf the brand wants to enhance its differentiation from other brands, brand can focus on balancing the organic and rating aspects. This is more in line with the sustainable development mentioned earlier, and there are fewer brands in this position.\nAnd then, we can use brand.mean or colMeans to calculate the difference distance to target brand.\n1.2.5 move forward Suppose we already have the basis for organic, such as brand Casttronovo. If we want to move forward rating, such as brand Fresco, we should increasing its emphasis on salt, counts_of_ingredients and cocoa_butter. And decrease organic and sweetener.\n1 2 3 \u0026gt; colMeans(brand.mean[c(\u0026#34;Fresco\u0026#34;, \u0026#34;Burnt Fork Bend\u0026#34;, \u0026#34;Pura Delizia\u0026#34;), ]) - brand.mean[\u0026#34;Castronovo\u0026#34;,] cocoa_percent rating counts_of_ingredients cocoa_butter vanilla organic salt sugar sweetener Castronovo -0.2628256 0.01758907 0.6254172 0.3872584 -0.5108785 -1.100088 0.833882 -0.2060768 -0.7544261 1.2.6 Factor analysis using factanal() * 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \u0026gt; nScree(brand.mean) noc naf nparallel nkaiser 1 3 1 3 3 \u0026gt; eigen(cor(brand.mean)) eigen() decomposition $values [1] 3.79660426 1.75301693 1.11381847 0.89943381 0.71628501 0.33478902 0.17579766 0.14266370 0.06759114 $vectors [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [1,] 0.12098228 -0.05345466 0.76473607 -0.27140650 -0.55335160 -0.042715830 -0.083872256 -0.007471336 -0.09439350 [2,] -0.03942608 0.32895808 -0.24216956 -0.90269635 0.08112631 0.003969369 -0.055740640 0.075143751 -0.03707461 [3,] 0.46644017 -0.14377172 -0.07741629 -0.13320854 0.03448633 -0.274086437 0.149381901 -0.747725837 0.28337944 [4,] 0.47713568 -0.09241339 -0.13970215 -0.02062616 -0.07299798 0.121853132 0.595019898 0.184888897 -0.57918020 [5,] -0.01644575 -0.50210677 0.37649227 -0.20692681 0.68649303 -0.230097021 0.034521520 0.185411405 -0.05715541 [6,] -0.47064300 0.09168746 0.13106717 -0.05057970 -0.07718575 -0.101646053 0.778103705 0.009240990 0.35851266 [7,] 0.48412835 -0.10647051 -0.08160011 -0.02823560 -0.10696045 0.059388537 0.008685290 0.557037340 0.64922598 [8,] -0.21518798 -0.48173066 -0.38411284 -0.08077891 -0.41618156 -0.593141261 -0.082256717 0.151363976 -0.11340603 [9,] -0.19005934 -0.59649010 -0.13377738 -0.20151517 -0.13720262 0.699691310 0.004010208 -0.182634411 0.08836494 \u0026gt; brand.fa \u0026lt;- factanal(brand.mean, factors = 2, rotation = \u0026#34;varimax\u0026#34;, scores = \u0026#34;regression\u0026#34;) \u0026gt; \u0026gt; brand.fl\u0026lt;- brand.fa$loadings[, 1:2] \u0026gt; plot(brand.fl,type=\u0026#34;n\u0026#34;) # set up plot \u0026gt; text(brand.fl,labels=names(brand.mean),cex=.7) {% asset_image final_13.png %}\n1 2 3 \u0026gt; brand.fs \u0026lt;- brand.fa$scores \u0026gt; plot(brand.fl,type=\u0026#34;n\u0026#34;) # set up plot \u0026gt; text(brand.fl,labels=rownames(brand.mean),cex=.7) {% asset_image final_14.png %}\n2. Managing Sustainable Competitive Advantage 2.1 Choice-Based Conjoint Analysis 2.1.1 import and check data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 \u0026gt; cbc.df \u0026lt;- read.csv(\u0026#34;5_conjoint.csv\u0026#34;, stringsAsF .... [TRUNCATED] \u0026gt; head(cbc.df, n = 5) Consumer_id Block Choice_id Alternative Choice Origin Manufacture Energy Nuts Tokens Organic Premium Fairtrade Sugar Price 1 1 1 1 1 1 Venezuela Developed Low Nuts only No No Yes Yes High 3 2 1 1 1 2 0 Venezuela UnderDeveloped Low Nuts only Keep \u0026amp; Use No No Yes Low 5 3 1 1 1 3 0 Peru Developing High No Donate Yes No No High 7 4 1 1 2 1 0 Ecuador UnderDeveloped Low Nuts and Fruit Donate Yes No No High 3 5 1 1 2 2 0 Venezuela Developed Low No No No Yes Yes Low 3 Age_Group Gender Salary Education Employment Location_by_region Choco_Consumption Sustainability_Score 1 1 2 1 2 1 1 2 -0.6645 2 1 2 1 2 1 1 2 -0.6645 3 1 2 1 2 1 1 2 -0.6645 4 1 2 1 2 1 1 2 -0.6645 5 1 2 1 2 1 1 2 -0.6645 \u0026gt; summary(cbc.df, digits = 2) Consumer_id Block Choice_id Alternative Choice Origin Manufacture Energy Nuts Min. : 1 Min. :1.0 Min. : 1 Min. :1 Min. :0.00 Ecuador :1902 Developed :1418 High:2356 No :2317 1st Qu.: 95 1st Qu.:3.0 1st Qu.: 473 1st Qu.:1 1st Qu.:0.00 Peru :1506 Developing :2311 Low :3314 Nuts and Fruit:1420 Median :190 Median :4.0 Median : 946 Median :2 Median :0.00 Venezuela:2262 UnderDeveloped:1941 Nuts only :1933 Mean :190 Mean :4.5 Mean : 946 Mean :2 Mean :0.33 3rd Qu.:284 3rd Qu.:6.0 3rd Qu.:1418 3rd Qu.:3 3rd Qu.:1.00 Max. :378 Max. :8.0 Max. :1890 Max. :3 Max. :1.00 Tokens Organic Premium Fairtrade Sugar Price Age_Group Gender Salary Education Employment Donate :1418 No :2664 No :2652 No :2646 High:2670 Min. :2.0 Min. :1.0 Min. :0.0 Min. :0.0 Min. :1.0 Min. : 1.0 Keep \u0026amp; Use:1945 Yes:3006 Yes:3018 Yes:3024 Low :3000 1st Qu.:3.0 1st Qu.:1.0 1st Qu.:1.0 1st Qu.:1.0 1st Qu.:2.0 1st Qu.: 1.0 No :2307 Median :4.0 Median :1.0 Median :2.0 Median :2.0 Median :2.0 Median : 2.0 Mean :4.5 Mean :1.5 Mean :1.6 Mean :2.3 Mean :1.8 Mean : 2.4 3rd Qu.:5.0 3rd Qu.:2.0 3rd Qu.:2.0 3rd Qu.:3.0 3rd Qu.:2.0 3rd Qu.: 3.0 Max. :7.0 Max. :2.0 Max. :2.0 Max. :7.0 Max. :2.0 Max. :10.0 Location_by_region Choco_Consumption Sustainability_Score Min. :1.0 Min. :1 Min. :-3.26 1st Qu.:1.0 1st Qu.:2 1st Qu.:-0.61 Median :1.0 Median :2 Median : 0.14 Mean :1.2 Mean :2 Mean : 0.00 3rd Qu.:1.0 3rd Qu.:2 3rd Qu.: 0.68 Max. :2.0 Max. :2 Max. : 2.04 \u0026gt; str(cbc.df) \u0026#39;data.frame\u0026#39;:\t5670 obs. of 23 variables: $ Consumer_id : int 1 1 1 1 1 1 1 1 1 1 ... $ Block : int 1 1 1 1 1 1 1 1 1 1 ... $ Choice_id : int 1 1 1 2 2 2 3 3 3 4 ... $ Alternative : int 1 2 3 1 2 3 1 2 3 1 ... $ Choice : int 1 0 0 0 0 1 0 0 1 0 ... $ Origin : Factor w/ 3 levels \u0026#34;Ecuador\u0026#34;,\u0026#34;Peru\u0026#34;,..: 3 3 2 1 3 3 1 2 3 1 ... $ Manufacture : Factor w/ 3 levels \u0026#34;Developed\u0026#34;,\u0026#34;Developing\u0026#34;,..: 1 3 2 3 1 2 1 2 3 2 ... $ Energy : Factor w/ 2 levels \u0026#34;High\u0026#34;,\u0026#34;Low\u0026#34;: 2 2 1 2 2 1 2 1 2 1 ... $ Nuts : Factor w/ 3 levels \u0026#34;No\u0026#34;,\u0026#34;Nuts and Fruit\u0026#34;,..: 3 3 1 2 1 2 3 1 2 3 ... $ Tokens : Factor w/ 3 levels \u0026#34;Donate\u0026#34;,\u0026#34;Keep \u0026amp; Use\u0026#34;,..: 3 2 1 1 3 2 3 2 2 1 ... $ Organic : Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 1 1 2 2 1 1 2 2 1 2 ... $ Premium : Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 2 1 1 1 2 2 2 1 2 1 ... $ Fairtrade : Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 2 2 1 1 2 2 2 1 2 2 ... $ Sugar : Factor w/ 2 levels \u0026#34;High\u0026#34;,\u0026#34;Low\u0026#34;: 1 2 1 1 2 1 1 2 2 2 ... $ Price : num 3 5 7 3 3 7 7 4 3 5 ... $ Age_Group : int 1 1 1 1 1 1 1 1 1 1 ... $ Gender : int 2 2 2 2 2 2 2 2 2 2 ... $ Salary : int 1 1 1 1 1 1 1 1 1 1 ... $ Education : int 2 2 2 2 2 2 2 2 2 2 ... $ Employment : int 1 1 1 1 1 1 1 1 1 1 ... $ Location_by_region : int 1 1 1 1 1 1 1 1 1 1 ... $ Choco_Consumption : int 2 2 2 2 2 2 2 2 2 2 ... $ Sustainability_Score: num -0.664 -0.664 -0.664 -0.664 -0.664 ... \u0026gt; cbc.df \u0026lt;- subset(cbc.df, select = -c(Block,Age_Group,Gender,Salary, Education, Employment, Location_by_region, Choco_Consumption, Sustainability_Score)) \u0026gt; head(cbc.df, n = 5) Consumer_id Choice_id Alternative Choice Origin Manufacture Energy Nuts Tokens Organic Premium Fairtrade Sugar Price 1 1 1 1 1 Venezuela Developed Low Nuts only No No Yes Yes High 3 2 1 1 2 0 Venezuela UnderDeveloped Low Nuts only Keep \u0026amp; Use No No Yes Low 5 3 1 1 3 0 Peru Developing High No Donate Yes No No High 7 4 1 2 1 0 Ecuador UnderDeveloped Low Nuts and Fruit Donate Yes No No High 3 5 1 2 2 0 Venezuela Developed Low No No No Yes Yes Low 3 \u0026gt; summary(cbc.df, digits = 2) Consumer_id Choice_id Alternative Choice Origin Manufacture Energy Nuts Tokens Organic Min. : 1 Min. : 1 Min. :1 Min. :0.00 Ecuador :1902 Developed :1418 High:2356 No :2317 Donate :1418 No :2664 1st Qu.: 95 1st Qu.: 473 1st Qu.:1 1st Qu.:0.00 Peru :1506 Developing :2311 Low :3314 Nuts and Fruit:1420 Keep \u0026amp; Use:1945 Yes:3006 Median :190 Median : 946 Median :2 Median :0.00 Venezuela:2262 UnderDeveloped:1941 Nuts only :1933 No :2307 Mean :190 Mean : 946 Mean :2 Mean :0.33 3rd Qu.:284 3rd Qu.:1418 3rd Qu.:3 3rd Qu.:1.00 Max. :378 Max. :1890 Max. :3 Max. :1.00 Premium Fairtrade Sugar Price No :2652 No :2646 High:2670 Min. :2.0 Yes:3018 Yes:3024 Low :3000 1st Qu.:3.0 Median :4.0 Mean :4.5 3rd Qu.:5.0 Max. :7.0 \u0026gt; str(cbc.df) \u0026#39;data.frame\u0026#39;:\t5670 obs. of 14 variables: $ Consumer_id: int 1 1 1 1 1 1 1 1 1 1 ... $ Choice_id : int 1 1 1 2 2 2 3 3 3 4 ... $ Alternative: int 1 2 3 1 2 3 1 2 3 1 ... $ Choice : int 1 0 0 0 0 1 0 0 1 0 ... $ Origin : Factor w/ 3 levels \u0026#34;Ecuador\u0026#34;,\u0026#34;Peru\u0026#34;,..: 3 3 2 1 3 3 1 2 3 1 ... $ Manufacture: Factor w/ 3 levels \u0026#34;Developed\u0026#34;,\u0026#34;Developing\u0026#34;,..: 1 3 2 3 1 2 1 2 3 2 ... $ Energy : Factor w/ 2 levels \u0026#34;High\u0026#34;,\u0026#34;Low\u0026#34;: 2 2 1 2 2 1 2 1 2 1 ... $ Nuts : Factor w/ 3 levels \u0026#34;No\u0026#34;,\u0026#34;Nuts and Fruit\u0026#34;,..: 3 3 1 2 1 2 3 1 2 3 ... $ Tokens : Factor w/ 3 levels \u0026#34;Donate\u0026#34;,\u0026#34;Keep \u0026amp; Use\u0026#34;,..: 3 2 1 1 3 2 3 2 2 1 ... $ Organic : Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 1 1 2 2 1 1 2 2 1 2 ... $ Premium : Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 2 1 1 1 2 2 2 1 2 1 ... $ Fairtrade : Factor w/ 2 levels \u0026#34;No\u0026#34;,\u0026#34;Yes\u0026#34;: 2 2 1 1 2 2 2 1 2 2 ... $ Sugar : Factor w/ 2 levels \u0026#34;High\u0026#34;,\u0026#34;Low\u0026#34;: 1 2 1 1 2 1 1 2 2 2 ... $ Price : num 3 5 7 3 3 7 7 4 3 5 ... 1 2 3 4 \u0026gt; xtabs(Choice~Origin, data=cbc.df) Origin Ecuador Peru Venezuela 709 495 686 Ecuador(38%) \u0026gt; Venezuela(36%) \u0026gt; Peru(26%)\n1 2 3 4 \u0026gt; xtabs(Choice~Manufacture, data=cbc.df) Manufacture Developed Developing UnderDeveloped 436 893 561 Developing \u0026gt; UnderDeveloped \u0026gt; Developed\n1 2 3 4 \u0026gt; xtabs(Choice~Energy, data=cbc.df) Energy High Low 922 968 Energy low nearly equals high\n1 2 3 4 \u0026gt; xtabs(Choice~Nuts, data=cbc.df) Nuts No Nuts and Fruit Nuts only 568 520 802 Nuts Only(42%) \u0026gt; No(30%) \u0026gt; with Fruit(28%)\n1 2 3 4 \u0026gt; xtabs(Choice~Tokens, data=cbc.df) Tokens Donate Keep \u0026amp; Use No 532 740 618 Tokens Keep\u0026amp;Use(39%) \u0026gt; No(32%) \u0026gt; Donate(28%)\n1 2 3 4 \u0026gt; xtabs(Choice~Organic, data=cbc.df) Organic No Yes 739 1151 Organic Yes(60%) \u0026gt; No(40%)\n1 2 3 4 \u0026gt; xtabs(Choice~Premium, data=cbc.df) Premium No Yes 394 1496 Premium Yes(80%) \u0026gt; No(20%)\n1 2 3 4 \u0026gt; xtabs(Choice~Fairtrade, data=cbc.df) Fairtrade No Yes 846 1044 Fairtrade Yes(55%) \u0026gt; No(45%)\n1 2 3 4 \u0026gt; xtabs(Choice~Sugar, data=cbc.df) Sugar High Low 1205 685 Sugar High(64%) \u0026gt; Low(36%)\n2.1.2 prepare the data 1 2 3 4 5 6 7 8 9 cbc.df$Origin \u0026lt;- relevel(cbc.df$Origin, ref = \u0026#34;Venezuela\u0026#34;) cbc.df$Manufacture \u0026lt;- relevel(cbc.df$Manufacture, ref = \u0026#34;UnderDeveloped\u0026#34;) cbc.df$Energy \u0026lt;- relevel(cbc.df$Energy, ref = \u0026#34;Low\u0026#34;) cbc.df$Nuts \u0026lt;- relevel(cbc.df$Nuts, ref = \u0026#34;No\u0026#34;) cbc.df$Tokens \u0026lt;- relevel(cbc.df$Tokens, ref = \u0026#34;No\u0026#34;) cbc.df$Organic \u0026lt;- relevel(cbc.df$Organic, ref = \u0026#34;No\u0026#34;) cbc.df$Premium \u0026lt;- relevel(cbc.df$Premium, ref = \u0026#34;No\u0026#34;) cbc.df$Fairtrade \u0026lt;- relevel(cbc.df$Fairtrade, ref = \u0026#34;No\u0026#34;) cbc.df$Sugar \u0026lt;- relevel(cbc.df$Sugar, ref = \u0026#34;Low\u0026#34;) 2.1.3 Multinomial conjoint model estimation with mlogit() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 \u0026gt; cbc.mlogit \u0026lt;- dfidx(cbc.df, choice=\u0026#34;Choice\u0026#34;, + idx=list(c(\u0026#34;Choice_id\u0026#34;, \u0026#34;Consumer_id\u0026#34;), \u0026#34;Alternative\u0026#34;)) \u0026gt; model\u0026lt;-mlogit(Choice ~ 0+Origin+Manufacture+Energy+Nuts+Tokens+Organic+Premium+Fairtrade+Sugar+Price, data=cbc.mlogit) \u0026gt; kable(summary(model)$CoefTable) | | Estimate| Std. Error| z-value| Pr(\u0026gt;\u0026amp;#124;z\u0026amp;#124;)| |:---------------------|----------:|----------:|----------:|------------------:| |OriginEcuador | 0.2265894| 0.0836676| 2.7082082| 0.0067648| |OriginPeru | 0.2194174| 0.0753628| 2.9114832| 0.0035972| |ManufactureDeveloped | 0.0396353| 0.0812293| 0.4879435| 0.6255898| |ManufactureDeveloping | -0.2157812| 0.1743582| -1.2375742| 0.2158740| |EnergyHigh | 0.4153950| 0.1814621| 2.2891551| 0.0220703| |NutsNuts and Fruit | 0.3435879| 0.0820605| 4.1870067| 0.0000283| |NutsNuts only | 0.2629300| 0.0751140| 3.5004111| 0.0004645| |TokensDonate | 0.5203778| 0.0821382| 6.3353922| 0.0000000| |TokensKeep \u0026amp; Use | 0.1345164| 0.0721562| 1.8642397| 0.0622881| |OrganicYes | 0.3947520| 0.0614207| 6.4270217| 0.0000000| |PremiumYes | 1.3982032| 0.0666952| 20.9640710| 0.0000000| |FairtradeYes | 0.4648390| 0.0613058| 7.5822972| 0.0000000| |SugarHigh | 0.6121655| 0.0632706| 9.6753519| 0.0000000| |Price | -0.0824375| 0.0207685| -3.9693491| 0.0000721| Demonstrated that positive value of utility means prefer than reference value, meanwhile negative value indicates that they prefer reference level.\nIn case of the Nuts attribute, customers prefer more nuts, etc.\n2.1.4 Model fit 1 2 3 4 5 6 7 8 9 10 11 12 \u0026gt; model.constraint \u0026lt;-mlogit(Choice ~ 0+Nuts, data = cbc.mlogit) \u0026gt; lrtest(model, model.constraint) Likelihood ratio test Model 1: Choice ~ 0 + Origin + Manufacture + Energy + Nuts + Tokens + Organic + Premium + Fairtrade + Sugar + Price Model 2: Choice ~ 0 + Nuts #Df LogLik Df Chisq Pr(\u0026gt;Chisq) 1 14 -1566.0 2 2 -2021.2 -12 910.45 \u0026lt; 2.2e-16 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Means the larger model (our first model) fits the data better. So, we should keep all the variables.\n2.1.5 Interpreting Conjoint Analysis Findings According to mlogit() results, customers prefer:\nHigher energy More Nuts Donate loyalty points with chocolates Be Organic Farmers paid a premium price Faire trade certified Higher sugar Lower price Origin Ecuador or Peru Developed Manufacture We test the prediction for the first six choice sets in the data.\n1 2 3 4 5 6 7 8 9 10 11 \u0026gt; kable(head(predict(model,cbc.mlogit))) | 1| 2| 3| |---------:|---------:|---------:| | 0.6954121| 0.0868465| 0.2177414| | 0.2571232| 0.2116132| 0.5312636| | 0.6276737| 0.0693051| 0.3030212| | 0.3751338| 0.3188902| 0.3059760| | 0.2320737| 0.6858870| 0.0820393| | 0.6530199| 0.2196068| 0.1273733| We can see that, in group 2, choice 3 is more prefered, which means customers may pay more for higher energy and higher nuts and fruit.\nAnd then, Measure the accuracy of prediction across all data:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 \u0026gt; predicted_alternative \u0026lt;- apply(predict(model,cbc.mlogit),1,which.max) \u0026gt; selected_alternative \u0026lt;- cbc.mlogit$Alternative[cbc.mlogit$Choice\u0026gt;0] \u0026gt; confusionMatrix(table(predicted_alternative,selected_alternative),positive = \u0026#34;1\u0026#34;) Confusion Matrix and Statistics selected_alternative predicted_alternative 1 2 3 1 315 104 102 2 142 705 140 3 102 61 219 Overall Statistics Accuracy : 0.6556 95% CI : (0.6336, 0.677) No Information Rate : 0.4603 P-Value [Acc \u0026gt; NIR] : \u0026lt; 2.2e-16 Kappa : 0.4522 Mcnemar\u0026#39;s Test P-Value : 4.785e-08 Statistics by Class: Class: 1 Class: 2 Class: 3 Sensitivity 0.5635 0.8103 0.4751 Specificity 0.8452 0.7235 0.8859 Pos Pred Value 0.6046 0.7143 0.5733 Neg Pred Value 0.8218 0.8173 0.8395 Prevalence 0.2958 0.4603 0.2439 Detection Rate 0.1667 0.3730 0.1159 Detection Prevalence 0.2757 0.5222 0.2021 Balanced Accuracy 0.7044 0.7669 0.6805 If the predictions were random, the accuracy would be 33.3% (for three alternatives). Our simple model is doing much better than that – although it is not perfect.\n2.2 Willingness to pay 2.2.1 What is the Nuts\u0026rsquo; value 1 2 3 \u0026gt; (coef(model)[\u0026#34;NutsNuts and Fruit\u0026#34;]-coef(model)[\u0026#34;NutsNuts only\u0026#34;]) / (-coef(model)[\u0026#34;Price\u0026#34;]) NutsNuts and Fruit 0.9784127 The dollar value of an upgrade from Nuts only to Nuts and Fruit.\n2.2.2 Willingness to Pay for an Attribute Upgrade 1 2 3 \u0026gt; coef(model)[\u0026#34;NutsNuts and Fruit\u0026#34;] / (-coef(model)[\u0026#34;Price\u0026#34;]) NutsNuts and Fruit 4.167859 The dollar value of an upgrade from No Nuts to Nuts and Fruit (No Nuts is reference level. Hence its coeff is 0)\n1 2 3 \u0026gt; coef(model)[\u0026#34;EnergyHigh\u0026#34;] / (-coef(model)[\u0026#34;Price\u0026#34;]) EnergyHigh 5.038907 The dollar value of an upgrade from Energy Low to Energy High (Energy Low is reference level. Hence its coeff is 0)\n2.3 Market Basket 2.3.1 Retail Transaction Data: Groceries 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 \u0026gt; retail.raw \u0026lt;- readLines(\u0026#34;6_groceries.dat\u0026#34;) \u0026gt; head(retail.raw) [1] \u0026#34;fruit, semi-finished bread, margarine, ready soups\u0026#34; \u0026#34;crisps and nuts, yogurt, coffee\u0026#34; [3] \u0026#34;whole milk\u0026#34; \u0026#34;pip fruit, yogurt, cream cheese, meat spreads\u0026#34; [5] \u0026#34;milk chocolate, whole milk, condensed milk, dark chocolate\u0026#34; \u0026#34;whole milk, butter, yogurt, rice, abrasive cleaner\u0026#34; \u0026gt; tail(retail.raw) [1] \u0026#34;crisps and nuts, milk chocolate, domestic eggs, zwieback, ketchup, soda, dishes\u0026#34; [2] \u0026#34;sausage, chicken, sweet, hamburger meat, fruit, grapes, biscuits and crackers, whole milk, butter, whipped/sour cream, flour, coffee, red/blush wine, salty snack, milk chocolate, hygiene articles, napkins\u0026#34; [3] \u0026#34;cooking milk chocolate\u0026#34; [4] \u0026#34;chicken, fruit, milk chocolate, butter, yogurt, frozen dessert, domestic eggs, bread, rum, cling film/bags\u0026#34; [5] \u0026#34;semi-finished bread, bottled water, soda, bottled beer\u0026#34; [6] \u0026#34;chicken, crisps and nuts, milk chocolate, vinegar, shopping bags\u0026#34; \u0026gt; summary(retail.raw) Length Class Mode 9835 character character \u0026gt; retail.list \u0026lt;- strsplit(retail.raw, \u0026#34;,\u0026#34;) \u0026gt; names(retail.list) \u0026lt;- paste(\u0026#34;Trans\u0026#34;, 1:length(retail.list)) \u0026gt; str(retail.list) List of 9835 $ Trans 1 : chr [1:4] \u0026#34;fruit\u0026#34; \u0026#34; semi-finished bread\u0026#34; \u0026#34; margarine\u0026#34; \u0026#34; ready soups\u0026#34; $ Trans 2 : chr [1:3] \u0026#34;crisps and nuts\u0026#34; \u0026#34; yogurt\u0026#34; \u0026#34; coffee\u0026#34; $ Trans 3 : chr \u0026#34;whole milk\u0026#34; $ Trans 4 : chr [1:4] \u0026#34;pip fruit\u0026#34; \u0026#34; yogurt\u0026#34; \u0026#34; cream cheese\u0026#34; \u0026#34; meat spreads\u0026#34; $ Trans 5 : chr [1:4] \u0026#34;milk chocolate\u0026#34; \u0026#34; whole milk\u0026#34; \u0026#34; condensed milk\u0026#34; \u0026#34; dark chocolate\u0026#34; $ Trans 6 : chr [1:5] \u0026#34;whole milk\u0026#34; \u0026#34; butter\u0026#34; \u0026#34; yogurt\u0026#34; \u0026#34; rice\u0026#34; ... $ Trans 7 : chr \u0026#34;bread\u0026#34; $ Trans 8 : chr [1:5] \u0026#34;milk chocolate\u0026#34; \u0026#34; UHT-milk\u0026#34; \u0026#34; bread\u0026#34; \u0026#34; bottled beer\u0026#34; ... $ Trans 9 : chr \u0026#34;potted plants\u0026#34; $ Trans 10 : chr [1:2] \u0026#34;whole milk\u0026#34; \u0026#34; cereals\u0026#34; $ Trans 11 : chr [1:5] \u0026#34;crisps and nuts\u0026#34; \u0026#34; milk chocolate\u0026#34; \u0026#34; white bread\u0026#34; \u0026#34; bottled water\u0026#34; ... $ Trans 12 : chr [1:9] \u0026#34;fruit\u0026#34; \u0026#34; crisps and nuts\u0026#34; \u0026#34; whole milk\u0026#34; \u0026#34; butter\u0026#34; ... $ Trans 13 : chr \u0026#34;sweet\u0026#34; $ Trans 14 : chr [1:3] \u0026#34;frankfurter\u0026#34; \u0026#34; bread\u0026#34; \u0026#34; soda\u0026#34; $ Trans 15 : chr [1:2] \u0026#34;chicken\u0026#34; \u0026#34; crisps and nuts\u0026#34; $ Trans 16 : chr [1:4] \u0026#34;butter\u0026#34; \u0026#34; sugar\u0026#34; \u0026#34; fruit/vegetable juice\u0026#34; \u0026#34; newspapers\u0026#34; $ Trans 17 : chr \u0026#34;fruit/vegetable juice\u0026#34; $ Trans 18 : chr \u0026#34;packaged fruit/vegetables\u0026#34; $ Trans 19 : chr \u0026#34;milk chocolate\u0026#34; $ Trans 20 : chr \u0026#34;specialty bar\u0026#34; $ Trans 21 : chr \u0026#34;milk chocolate\u0026#34; $ Trans 22 : chr [1:2] \u0026#34;butter milk\u0026#34; \u0026#34; pastry\u0026#34; $ Trans 23 : chr \u0026#34;whole milk\u0026#34; $ Trans 24 : chr [1:5] \u0026#34;crisps and nuts\u0026#34; \u0026#34; cream cheese\u0026#34; \u0026#34; processed cheese\u0026#34; \u0026#34; detergent\u0026#34; ... $ Trans 25 : chr [1:11] \u0026#34;crisps and nuts\u0026#34; \u0026#34; biscuits and crackers\u0026#34; \u0026#34; milk chocolate\u0026#34; \u0026#34; frozen dessert\u0026#34; ... $ Trans 26 : chr [1:2] \u0026#34;bottled water\u0026#34; \u0026#34; canned beer\u0026#34; $ Trans 27 : chr \u0026#34;yogurt\u0026#34; $ Trans 28 : chr [1:4] \u0026#34;sausage\u0026#34; \u0026#34; bread\u0026#34; \u0026#34; soda\u0026#34; \u0026#34; milk chocolate\u0026#34; $ Trans 29 : chr \u0026#34;milk chocolate\u0026#34; $ Trans 30 : chr [1:6] \u0026#34;brown bread\u0026#34; \u0026#34; soda\u0026#34; \u0026#34; fruit/vegetable juice\u0026#34; \u0026#34; canned beer\u0026#34; ... $ Trans 31 : chr [1:4] \u0026#34;yogurt\u0026#34; \u0026#34; beverages\u0026#34; \u0026#34; bottled water\u0026#34; \u0026#34; specialty bar\u0026#34; $ Trans 32 : chr [1:7] \u0026#34;hamburger meat\u0026#34; \u0026#34; milk chocolate\u0026#34; \u0026#34; bread\u0026#34; \u0026#34; spices\u0026#34; ... $ Trans 33 : chr [1:5] \u0026#34;biscuits and crackers\u0026#34; \u0026#34; milk chocolate\u0026#34; \u0026#34; whole milk\u0026#34; \u0026#34; beverages\u0026#34; ... $ Trans 34 : chr [1:8] \u0026#34;beef\u0026#34; \u0026#34; berries\u0026#34; \u0026#34; milk chocolate\u0026#34; \u0026#34; whole milk\u0026#34; ... $ Trans 35 : chr [1:3] \u0026#34;sweet\u0026#34; \u0026#34; grapes\u0026#34; \u0026#34; detergent\u0026#34; $ Trans 36 : chr [1:2] \u0026#34;pastry\u0026#34; \u0026#34; soda\u0026#34; $ Trans 37 : chr \u0026#34;fruit/vegetable juice\u0026#34; $ Trans 38 : chr \u0026#34;canned beer\u0026#34; $ Trans 39 : chr [1:4] \u0026#34;biscuits and crackers\u0026#34; \u0026#34; milk chocolate\u0026#34; \u0026#34; whole milk\u0026#34; \u0026#34; dessert\u0026#34; $ Trans 40 : chr [1:3] \u0026#34;fruit\u0026#34; \u0026#34; zwieback\u0026#34; \u0026#34; newspapers\u0026#34; $ Trans 41 : chr [1:6] \u0026#34;sausage\u0026#34; \u0026#34; bread\u0026#34; \u0026#34; soda\u0026#34; \u0026#34; canned beer\u0026#34; ... $ Trans 42 : chr [1:13] \u0026#34;crisps and nuts\u0026#34; \u0026#34; biscuits and crackers\u0026#34; \u0026#34; whole milk\u0026#34; \u0026#34; yogurt\u0026#34; ... $ Trans 43 : chr [1:2] \u0026#34;berries\u0026#34; \u0026#34; yogurt\u0026#34; $ Trans 44 : chr \u0026#34;canned beer\u0026#34; $ Trans 45 : chr [1:8] \u0026#34;butter milk\u0026#34; \u0026#34; yogurt\u0026#34; \u0026#34; cream cheese\u0026#34; \u0026#34; spread cheese\u0026#34; ... $ Trans 46 : chr \u0026#34;coffee\u0026#34; $ Trans 47 : chr [1:2] \u0026#34;pastry\u0026#34; \u0026#34; bottled water\u0026#34; $ Trans 48 : chr \u0026#34;bread\u0026#34; $ Trans 49 : chr \u0026#34;misc. beverages\u0026#34; $ Trans 50 : chr [1:10] \u0026#34;biscuits and crackers\u0026#34; \u0026#34; milk chocolate\u0026#34; \u0026#34; butter\u0026#34; \u0026#34; curd\u0026#34; ... $ Trans 51 : chr [1:4] \u0026#34;sausage\u0026#34; \u0026#34; bread\u0026#34; \u0026#34; cat food\u0026#34; \u0026#34; newspapers\u0026#34; $ Trans 52 : chr \u0026#34;canned beer\u0026#34; $ Trans 53 : chr [1:4] \u0026#34;ham\u0026#34; \u0026#34; grapes\u0026#34; \u0026#34; milk chocolate\u0026#34; \u0026#34; whole milk\u0026#34; $ Trans 54 : chr [1:10] \u0026#34;turkey\u0026#34; \u0026#34; crisps and nuts\u0026#34; \u0026#34; milk chocolate\u0026#34; \u0026#34; curd\u0026#34; ... $ Trans 55 : chr [1:5] \u0026#34;whole milk\u0026#34; \u0026#34; yogurt\u0026#34; \u0026#34; processed cheese\u0026#34; \u0026#34; pickled vegetables\u0026#34; ... $ Trans 56 : chr [1:4] \u0026#34;whole milk\u0026#34; \u0026#34; curd\u0026#34; \u0026#34; yogurt\u0026#34; \u0026#34; pastry\u0026#34; $ Trans 57 : chr [1:3] \u0026#34;packaged fruit/vegetables\u0026#34; \u0026#34; brown bread\u0026#34; \u0026#34; canned beer\u0026#34; $ Trans 58 : chr [1:7] \u0026#34;bread\u0026#34; \u0026#34; oil\u0026#34; \u0026#34; bottled water\u0026#34; \u0026#34; chewing gum\u0026#34; ... $ Trans 59 : chr [1:6] \u0026#34;ham\u0026#34; \u0026#34; sweet\u0026#34; \u0026#34; whipped/sour cream\u0026#34; \u0026#34; ice cream\u0026#34; ... $ Trans 60 : chr [1:3] \u0026#34;bread\u0026#34; \u0026#34; pastry\u0026#34; \u0026#34; sugar\u0026#34; $ Trans 61 : chr [1:7] \u0026#34;milk chocolate\u0026#34; \u0026#34; whole milk\u0026#34; \u0026#34; frozen vegetables\u0026#34; \u0026#34; canned fish\u0026#34; ... $ Trans 62 : chr [1:2] \u0026#34;sausage\u0026#34; \u0026#34; pastry\u0026#34; $ Trans 63 : chr [1:3] \u0026#34;sausage\u0026#34; \u0026#34; sweet\u0026#34; \u0026#34; whole milk\u0026#34; $ Trans 64 : chr [1:5] \u0026#34;frankfurter\u0026#34; \u0026#34; crisps and nuts\u0026#34; \u0026#34; bread\u0026#34; \u0026#34; brown bread\u0026#34; ... $ Trans 65 : chr [1:3] \u0026#34;bread\u0026#34; \u0026#34; pastry\u0026#34; \u0026#34; soda\u0026#34; $ Trans 66 : chr \u0026#34;whole milk\u0026#34; $ Trans 67 : chr [1:2] \u0026#34;curd cheese\u0026#34; \u0026#34; coffee\u0026#34; $ Trans 68 : chr [1:2] \u0026#34;red/blush wine\u0026#34; \u0026#34; newspapers\u0026#34; $ Trans 69 : chr [1:3] \u0026#34;sausage\u0026#34; \u0026#34; whole milk\u0026#34; \u0026#34; curd\u0026#34; $ Trans 70 : chr [1:8] \u0026#34;crisps and nuts\u0026#34; \u0026#34; pip fruit\u0026#34; \u0026#34; berries\u0026#34; \u0026#34; whole milk\u0026#34; ... $ Trans 71 : chr \u0026#34;red/blush wine\u0026#34; $ Trans 72 : chr [1:7] \u0026#34;whole milk\u0026#34; \u0026#34; butter\u0026#34; \u0026#34; margarine\u0026#34; \u0026#34; specialty fat\u0026#34; ... $ Trans 73 : chr [1:8] \u0026#34;frankfurter\u0026#34; \u0026#34; fruit\u0026#34; \u0026#34; whole milk\u0026#34; \u0026#34; domestic eggs\u0026#34; ... $ Trans 74 : chr [1:3] \u0026#34;whole milk\u0026#34; \u0026#34; meat spreads\u0026#34; \u0026#34; soda\u0026#34; $ Trans 75 : chr \u0026#34;frozen potato products\u0026#34; $ Trans 76 : chr [1:4] \u0026#34;milk chocolate\u0026#34; \u0026#34; whole milk\u0026#34; \u0026#34; bread\u0026#34; \u0026#34; sugar\u0026#34; $ Trans 77 : chr [1:5] \u0026#34;fruit\u0026#34; \u0026#34; whole milk\u0026#34; \u0026#34; curd\u0026#34; \u0026#34; butter milk\u0026#34; ... $ Trans 78 : chr [1:5] \u0026#34;flour\u0026#34; \u0026#34; salt\u0026#34; \u0026#34; bottled water\u0026#34; \u0026#34; fruit/vegetable juice\u0026#34; ... $ Trans 79 : chr [1:4] \u0026#34;sugar\u0026#34; \u0026#34; bottled water\u0026#34; \u0026#34; soda\u0026#34; \u0026#34; bottled beer\u0026#34; $ Trans 80 : chr [1:2] \u0026#34;frozen meals\u0026#34; \u0026#34; coffee\u0026#34; $ Trans 81 : chr \u0026#34;milk chocolate\u0026#34; $ Trans 82 : chr [1:10] \u0026#34;biscuits and crackers\u0026#34; \u0026#34; whole milk\u0026#34; \u0026#34; frozen vegetables\u0026#34; \u0026#34; domestic eggs\u0026#34; ... $ Trans 83 : chr [1:4] \u0026#34;biscuits and crackers\u0026#34; \u0026#34; onions\u0026#34; \u0026#34; hard cheese\u0026#34; \u0026#34; frozen vegetables\u0026#34; $ Trans 84 : chr [1:7] \u0026#34;herbs\u0026#34; \u0026#34; condensed milk\u0026#34; \u0026#34; frozen vegetables\u0026#34; \u0026#34; salt\u0026#34; ... $ Trans 85 : chr \u0026#34;bottled water\u0026#34; $ Trans 86 : chr [1:7] \u0026#34;sausage\u0026#34; \u0026#34; biscuits and crackers\u0026#34; \u0026#34; onions\u0026#34; \u0026#34; yogurt\u0026#34; ... $ Trans 87 : chr [1:2] \u0026#34;coffee\u0026#34; \u0026#34; newspapers\u0026#34; $ Trans 88 : chr [1:3] \u0026#34;beef\u0026#34; \u0026#34; milk chocolate\u0026#34; \u0026#34; whipped/sour cream\u0026#34; $ Trans 89 : chr [1:2] \u0026#34;berries\u0026#34; \u0026#34; yogurt\u0026#34; $ Trans 90 : chr \u0026#34;soda\u0026#34; $ Trans 91 : chr \u0026#34;berries\u0026#34; $ Trans 92 : chr [1:3] \u0026#34;fruit/vegetable juice\u0026#34; \u0026#34; salty snack\u0026#34; \u0026#34; candles\u0026#34; $ Trans 93 : chr [1:4] \u0026#34;fruit\u0026#34; \u0026#34; butter milk\u0026#34; \u0026#34; yogurt\u0026#34; \u0026#34; cream cheese\u0026#34; $ Trans 94 : chr [1:9] \u0026#34;beef\u0026#34; \u0026#34; hamburger meat\u0026#34; \u0026#34; fruit\u0026#34; \u0026#34; berries\u0026#34; ... $ Trans 95 : chr \u0026#34;detergent\u0026#34; $ Trans 96 : chr [1:2] \u0026#34;grapes\u0026#34; \u0026#34; photo/film\u0026#34; $ Trans 97 : chr [1:9] \u0026#34;sausage\u0026#34; \u0026#34; sliced cheese\u0026#34; \u0026#34; bread\u0026#34; \u0026#34; brown bread\u0026#34; ... $ Trans 98 : chr [1:10] \u0026#34;chicken\u0026#34; \u0026#34; hamburger meat\u0026#34; \u0026#34; fruit\u0026#34; \u0026#34; crisps and nuts\u0026#34; ... $ Trans 99 : chr [1:3] \u0026#34;whole milk\u0026#34; \u0026#34; yogurt\u0026#34; \u0026#34; brown bread\u0026#34; [list output truncated] \u0026gt; \u0026gt; some(retail.list) #note: random sample; your results may vary $`Trans 918` [1] \u0026#34;beef\u0026#34; \u0026#34; fruit\u0026#34; \u0026#34; UHT-milk\u0026#34; \u0026#34; brown bread\u0026#34; \u0026#34; soda\u0026#34; \u0026#34; bottled beer\u0026#34; [7] \u0026#34; canned beer\u0026#34; \u0026#34; dark chocolate\u0026#34; $`Trans 1371` [1] \u0026#34;bottled beer\u0026#34; $`Trans 1563` [1] \u0026#34;bread\u0026#34; $`Trans 2440` [1] \u0026#34;shopping bags\u0026#34; $`Trans 3235` [1] \u0026#34;bottled water\u0026#34; \u0026#34; bottled beer\u0026#34; $`Trans 3260` [1] \u0026#34;frankfurter\u0026#34; \u0026#34; milk chocolate\u0026#34; \u0026#34; whole milk\u0026#34; \u0026#34; spread cheese\u0026#34; [5] \u0026#34; sugar\u0026#34; \u0026#34; soda\u0026#34; \u0026#34; bottled beer\u0026#34; \u0026#34; house keeping products\u0026#34; $`Trans 8306` [1] \u0026#34;beef\u0026#34; \u0026#34; sweet\u0026#34; \u0026#34; grapes\u0026#34; \u0026#34; berries\u0026#34; [5] \u0026#34; biscuits and crackers\u0026#34; \u0026#34; milk chocolate\u0026#34; \u0026#34; packaged fruit/vegetables\u0026#34; \u0026#34; yogurt\u0026#34; [9] \u0026#34; newspapers\u0026#34; $`Trans 9069` [1] \u0026#34;berries\u0026#34; \u0026#34; biscuits and crackers\u0026#34; \u0026#34; whipped/sour cream\u0026#34; \u0026#34; soda\u0026#34; $`Trans 9315` [1] \u0026#34;whole milk\u0026#34; $`Trans 9404` [1] \u0026#34;sausage\u0026#34; \u0026#34; bottled water\u0026#34; \u0026#34; canned beer\u0026#34; \u0026#34; hygiene articles\u0026#34; \u0026#34; shopping bags\u0026#34; \u0026gt; retail.trans \u0026lt;- as(retail.list, \u0026#34;transactions\u0026#34;) #takes a few seconds Warning message: In asMethod(object) : removing duplicated items in transactions \u0026gt; summary(retail.trans) transactions as itemMatrix in sparse format with 9835 rows (elements/itemsets/transactions) and 324 columns (items) and a density of 0.01357711 most frequent items: whole milk milk chocolate bread soda yogurt (Other) 1796 1782 1473 1421 1147 35645 element (itemset/transaction) length distribution: sizes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 2159 1643 1301 1007 854 649 553 433 346 250 178 116 80 73 56 46 27 13 16 10 9 4 4 1 1 27 28 29 32 1 3 1 1 Min. 1st Qu. Median Mean 3rd Qu. Max. 1.000 2.000 3.000 4.399 6.000 32.000 includes extended item information - examples: labels 1 abrasive cleaner 2 artif. sweetener 3 baby cosmetics includes extended transaction information - examples: transactionID 1 Trans 1 2 Trans 2 3 Trans 3 Looking at the summary() of the resulting object, we see that the transaction-by-item matrix is 9,835 rows by 324 columns. Of those 3.1 million intersections, only 1% have positive data (density) because most items are not purchased in most transactions. Item whole milk appears the most frequently and occurs in 1,796 baskets of all transactions. 2,159 of the transactions contain only a single item (“sizes” = 1) and the median basket size is 3 items.\n2.3.2 Retail Transaction Data: Groceries We now use apriori(data, parameters = \u0026hellip;) to find association rules with the apriori algorithm. At a conceptual level, the apriori algorithm searches through the item sets that frequently occur in a list of transactions. For each item set, it evaluates the various possible rules that express associations among the items at or above a particular level of support, and then retains the rules that show confidence above some threshold value. To control the extent that apriori() searches, we use the parameter=list() control to instruct the algorithm to search rules that have a minimum support of 0.01 (1% transactions) and extract the ones that further demonstrate a minimum confidence of 0.3. The resulting rules set is assigned to the groc.rules object:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \u0026gt; inspect(head(retail.trans,3)) items transactionID [1] { margarine, ready soups, semi-finished bread, fruit} Trans 1 [2] { coffee, yogurt, crisps and nuts} Trans 2 [3] {whole milk} Trans 3 \u0026gt; # Finding rules \u0026gt; groc.rules \u0026lt;- apriori(retail.trans, parameter = list(supp=0.01, conf=0.3, target=\u0026#34;rules\u0026#34;)) Apriori Parameter specification: confidence minval smax arem aval originalSupport maxtime support minlen maxlen target ext 0.3 0.1 1 none FALSE TRUE 5 0.01 1 10 rules TRUE Algorithmic control: filter tree heap memopt load sort verbose 0.1 TRUE TRUE FALSE TRUE 2 TRUE Absolute minimum support count: 98 set item appearances ...[0 item(s)] done [0.00s]. set transactions ...[324 item(s), 9835 transaction(s)] done [0.01s]. sorting and recoding items ... [97 item(s)] done [0.00s]. creating transaction tree ... done [0.01s]. checking subsets of size 1 2 3 4 done [0.00s]. writing ... [118 rule(s)] done [0.00s]. creating S4 object ... done [0.00s]. \u0026ldquo;sorting and recoding items \u0026hellip; [97 item(s)] done [0.00s].\u0026rdquo;: tells us that the rules found are using 97 of the total number of items. If this number is too small (only a tiny set of your items) or too large (almost all of them), then you might wish to adjust the support and confidence levels.\n\u0026ldquo;writing \u0026hellip; [118 rule(s)] done [0.00s].\u0026rdquo;: Next, check the number of rules found, as indicated on the “writing \u0026hellip;” line. In this case, the algorithm found 118 rules. If this number is too low, it suggests the need to lower the support or confidence levels; if it is too high (such as many more rules than items), you might increase the support or confidence levels.\nOnce we have a rule set from apriori(), we use inspect(rules) to examine the association rules. The complete list of 118 from above is too long to examine here, so we select a subset of them with high lift, lift \u0026gt; 3. We find that five of the rules in our set have lift greater than 3.0:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \u0026gt; inspect(subset(groc.rules, lift \u0026gt; 3)) lhs rhs support confidence coverage lift count [1] { sausage} =\u0026gt; {frankfurter} 0.01006609 1.0000000 0.01006609 16.956897 99 [2] {sweet} =\u0026gt; { biscuits and crackers} 0.01006609 0.3256579 0.03091002 4.085262 99 [3] { onions} =\u0026gt; { milk chocolate} 0.01301474 0.5446809 0.02389426 3.006137 128 [4] { fruit} =\u0026gt; { biscuits and crackers} 0.01128622 0.3074792 0.03670564 3.857217 111 [5] { butter, milk chocolate} =\u0026gt; { whole milk} 0.01260803 0.5876777 0.02145399 3.218157 124 [6] { pip fruit, whole milk} =\u0026gt; { milk chocolate} 0.01189629 0.5652174 0.02104728 3.119480 117 [7] { domestic eggs, milk chocolate} =\u0026gt; { whole milk} 0.01230300 0.5654206 0.02175902 3.096276 121 [8] { crisps and nuts, yogurt} =\u0026gt; { whole milk} 0.01047280 0.5953757 0.01759024 3.260312 103 [9] { crisps and nuts, whole milk} =\u0026gt; { yogurt} 0.01047280 0.4186992 0.02501271 3.590154 103 [10] { crisps and nuts, yogurt} =\u0026gt; { milk chocolate} 0.01006609 0.5722543 0.01759024 3.158317 99 [11] { crisps and nuts, milk chocolate} =\u0026gt; { yogurt} 0.01006609 0.3750000 0.02684291 3.215453 99 [12] { crisps and nuts, whole milk} =\u0026gt; { milk chocolate} 0.01372649 0.5487805 0.02501271 3.028763 135 [13] { biscuits and crackers, yogurt} =\u0026gt; { whole milk} 0.01230300 0.5960591 0.02064057 3.264054 121 [14] { biscuits and crackers, yogurt} =\u0026gt; { milk chocolate} 0.01148958 0.5566502 0.02064057 3.072197 113 The first rule tells us that if a transaction contains {sausage} then it is also relatively more likely to contain {frankfurter}. The support shows that the combination appears in 1% of baskets, and the lift shows that the combination is 17× more likely to occur together than one would expect from the individual rates of incidence alone.\nA store might form several insights on the basis of such information. For instance, the store might create a display for frankfurter near the sausage to encourage shoppers who are examining sausage to purchase those frankfurter with them. It might also suggest putting coupons for frankfurter in the sausage area or featuring recipe cards somewhere in the store.\n1 plot(groc.rules) {% asset_image final_15.png %}\nIn that chart, we see that most rules involve item combinations that infrequently occur (that is, they have low support) while confidence is relatively smoothly distributed.\nSimply showing points is not very useful, and a key feature with arulesViz is interactive plotting. In the above figure, there are some rules on the upper left with a high lift. We can use interactive plotting to inspect those rules. To do this, add interactive=TRUE to the plot() command:\n1 plot(groc.rules, engine = \u0026#34;plotly\u0026#34;, interactive=TRUE) {% asset_image final_16.png %}\nOne rule tells us that the combination {crisps and nuts, yogurt} occurs in about 1.0 % of baskets (support=0.0105), and when it occurs, it highly likely includes {whole milk} (confidence= 0.595). The combination occurs 3 times more often than we would expect from the individual incidence rates of {crisps and nuts, yogurt} and {whole milk} considered separately (lift=3.26).\nA common goal in market basket analysis is to find rules with high lift. We can find such rules easily by sorting the larger set of rules by lift. We extract the 15 rules with the highest lift using sort() to order the rules by lift and to take 50 from the head():\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \u0026gt; # Finding and Plotting Subsets of Rules \u0026gt; groc.hi \u0026lt;- head(sort(groc.rules, by=\u0026#34;lift\u0026#34;), 15) \u0026gt; inspect(groc.hi) lhs rhs support confidence coverage lift count [1] { sausage} =\u0026gt; {frankfurter} 0.01006609 1.0000000 0.01006609 16.956897 99 [2] {sweet} =\u0026gt; { biscuits and crackers} 0.01006609 0.3256579 0.03091002 4.085262 99 [3] { fruit} =\u0026gt; { biscuits and crackers} 0.01128622 0.3074792 0.03670564 3.857217 111 [4] { crisps and nuts, whole milk} =\u0026gt; { yogurt} 0.01047280 0.4186992 0.02501271 3.590154 103 [5] { biscuits and crackers, yogurt} =\u0026gt; { whole milk} 0.01230300 0.5960591 0.02064057 3.264054 121 [6] { crisps and nuts, yogurt} =\u0026gt; { whole milk} 0.01047280 0.5953757 0.01759024 3.260312 103 [7] { butter, milk chocolate} =\u0026gt; { whole milk} 0.01260803 0.5876777 0.02145399 3.218157 124 [8] { crisps and nuts, milk chocolate} =\u0026gt; { yogurt} 0.01006609 0.3750000 0.02684291 3.215453 99 [9] { crisps and nuts, yogurt} =\u0026gt; { milk chocolate} 0.01006609 0.5722543 0.01759024 3.158317 99 [10] { pip fruit, whole milk} =\u0026gt; { milk chocolate} 0.01189629 0.5652174 0.02104728 3.119480 117 [11] { domestic eggs, milk chocolate} =\u0026gt; { whole milk} 0.01230300 0.5654206 0.02175902 3.096276 121 [12] { biscuits and crackers, yogurt} =\u0026gt; { milk chocolate} 0.01148958 0.5566502 0.02064057 3.072197 113 [13] { crisps and nuts, whole milk} =\u0026gt; { milk chocolate} 0.01372649 0.5487805 0.02501271 3.028763 135 [14] { onions} =\u0026gt; { milk chocolate} 0.01301474 0.5446809 0.02389426 3.006137 128 [15] { butter, whole milk} =\u0026gt; { milk chocolate} 0.01260803 0.5414847 0.02328419 2.988497 124 Support and lift are identical for an item set regardless of the items’ order within a rule (left-hand or right- hand side of the rule). However, confidence reflects direction because it computes the occurrence of the right-hand set conditional on the left-hand side set. A graph display of rules may be useful to seek themes and patterns at a higher level. We chart the top 15 rules byliftwithplot(\u0026hellip; ,method=“graph”):\n{% asset_image final_17.png %}\nThe positioning of items on the resulting graph may differ for your system, but the item clusters should be similar. Each circle represents a rule, with inbound arrows coming from items on the left-hand side of the rule and outbound arrows going to the right-hand side. The size (area) of the circle represents the rule’s support, and shade represents lift (darker indicates higher lift).\n3 Managing Resources Trade-offs 3.1 Selecting Advertising platforms 3.1.1 import and check data 1 2 3 spending.data \u0026lt;- read.csv(\u0026#34;7_advertising.csv\u0026#34;) str(spending.data) plot(spending.data$radio, spending.data$sales) {% asset_image final_18.png %}\n1 plot(spending.data$magazines, spending.data$sales) {% asset_image final_19.png %}\n1 plot(spending.data$social_media, spending.data$sales) {% asset_image final_20.png %}\n1 plot(spending.data$search_ads, spending.data$sales) {% asset_image final_21.png %}\n1 plot(spending.data$tv, spending.data$sales) {% asset_image final_22.png %}\n1 plot(spending.data$newspaper, spending.data$sales) {% asset_image final_23.png %}\n3.1.2 Selecting Advertising platforms 3.1.2.1 line 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 \u0026gt; ## line \u0026gt; regression_1 \u0026lt;- lm(sales ~ radio + magazines + social_media + search_ads + tv + newspaper, data=spending.data) \u0026gt; # 83.14% explained \u0026gt; summary(regression_1) Call: lm(formula = sales ~ radio + magazines + social_media + search_ads + tv + newspaper, data = spending.data) Residuals: Min 1Q Median 3Q Max -1389.11 -73.13 43.17 114.90 835.69 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 323.0262 40.8171 7.914 1.9e-13 *** radio 15.2914 1.0354 14.768 \u0026lt; 2e-16 *** magazines -0.5535 3.5231 -0.157 0.875 social_media 1.8449 1.2796 1.442 0.151 search_ads -1.6499 2.0874 -0.790 0.430 tv 4.6694 0.2177 21.447 \u0026lt; 2e-16 *** newspaper 0.7353 0.9618 0.764 0.446 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 214.3 on 193 degrees of freedom Multiple R-squared: 0.8365,\tAdjusted R-squared: 0.8314 F-statistic: 164.5 on 6 and 193 DF, p-value: \u0026lt; 2.2e-16 3.1.2.2 log 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 \u0026gt; ## log \u0026gt; summary(spending.data$radio) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.04 9.30 22.00 23.23 36.35 79.60 \u0026gt; summary(spending.data$magazines) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.030 1.050 3.420 4.828 6.383 61.160 \u0026gt; summary(spending.data$social_media) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.245 7.050 18.137 22.289 33.523 90.800 \u0026gt; summary(spending.data$search_ads) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.1568 5.6442 11.6133 14.2485 21.4547 50.3014 \u0026gt; summary(spending.data$tv) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.70 74.38 149.75 147.04 218.82 296.40 \u0026gt; summary(spending.data$newspaper) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.30 12.75 25.75 30.55 45.10 114.00 \u0026gt; \u0026gt; regression_2 \u0026lt;- lm(log(sales) ~ log(radio) + log(magazines) + log(social_media) + log(search_ads) + log(tv) + log(newspaper), data=spending.data) \u0026gt; # 90.15% explained \u0026gt; summary(regression_2) Call: lm(formula = log(sales) ~ log(radio) + log(magazines) + log(social_media) + log(search_ads) + log(tv) + log(newspaper), data = spending.data) Residuals: Min 1Q Median 3Q Max -0.53768 -0.08809 -0.01695 0.07090 0.58515 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 5.066700 0.063064 80.342 \u0026lt;2e-16 *** log(radio) 0.143520 0.008449 16.986 \u0026lt;2e-16 *** log(magazines) 0.016958 0.015932 1.064 0.2885 log(social_media) 0.024575 0.015334 1.603 0.1107 log(search_ads) -0.037193 0.016400 -2.268 0.0244 * log(tv) 0.364471 0.013843 26.329 \u0026lt;2e-16 *** log(newspaper) 0.001954 0.017739 0.110 0.9124 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 0.13 on 193 degrees of freedom Multiple R-squared: 0.9045,\tAdjusted R-squared: 0.9015 F-statistic: 304.7 on 6 and 193 DF, p-value: \u0026lt; 2.2e-16 3.1.2.3 log better 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 \u0026gt; regression \u0026lt;- lm(log(sales) ~ log(radio) + log(tv), data=spending.data) \u0026gt; # 89.93% explained \u0026gt; summary(regression) Call: lm(formula = log(sales) ~ log(radio) + log(tv), data = spending.data) Residuals: Min 1Q Median 3Q Max -0.53810 -0.09115 -0.01295 0.06795 0.58926 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 5.098303 0.050058 101.85 \u0026lt;2e-16 *** log(radio) 0.146733 0.008299 17.68 \u0026lt;2e-16 *** log(tv) 0.356862 0.009271 38.49 \u0026lt;2e-16 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 0.1315 on 197 degrees of freedom Multiple R-squared: 0.9003,\tAdjusted R-squared: 0.8993 F-statistic: 889.6 on 2 and 197 DF, p-value: \u0026lt; 2.2e-16 3.1.3 Allocating Marketing Budget Sum elasticity\n1 0.49 = 0.14 + 0.35 Ratio of elasticity\n1 0.2857 = 0.14 / 0.49 TV of elasticity\n1 0.7143 = 0.35 / 0.49 Obtain elasticities from model\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \u0026gt; mean(spending.data$radio) [1] 23.2297 \u0026gt; mean(spending.data$tv) [1] 147.0425 \u0026gt; mean(spending.data$sales) [1] 1402.25 \u0026gt; \u0026gt; # radio \u0026gt; # A 1% increase in radio advertising results in a 0.26% increase in sales. \u0026gt; 0.143520 * (23.2297 / 1402.25) [1] 0.002377555 \u0026gt; \u0026gt; # tv \u0026gt; # A 1% increase in tv advertising results in a 0.26% increase in sales. \u0026gt; 0.364471 * (147.0425 / 1402.25) [1] 0.0382191 3.1.4 Advertising Carryover Effect We are assumed to retain a 10% of your previous advertising stock.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 \u0026gt; adstock \u0026lt;- function(x, rate){ + return(as.numeric(stats::filter(x=x, filter=rate, method=\u0026#34;recursive\u0026#34;))) + } \u0026gt; \u0026gt; spending.data \u0026lt;- spending.data %\u0026gt;% mutate(tv_adstock = adstock(tv,0.1), + magazines_adstock = adstock(magazines, 0.1), + social_media_adstock = adstock(social_media, 0.1), + search_ads_adstock = adstock(search_ads, 0.1), + newspaper_adstock = adstock(newspaper, 0.1), + radio_adstock = adstock(radio, 0.1)) \u0026gt; \u0026gt; regression_with_stock \u0026lt;- lm(log(sales) ~ log(radio_adstock) + log(magazines_adstock) + log(social_media_adstock) + log(search_ads_adstock) + log(tv_adstock) + log(newspaper_adstock), data=spending.data) \u0026gt; # 90.15% explained \u0026gt; summary(regression_with_stock) Call: lm(formula = log(sales) ~ log(radio_adstock) + log(magazines_adstock) + log(social_media_adstock) + log(search_ads_adstock) + log(tv_adstock) + log(newspaper_adstock), data = spending.data) Residuals: Min 1Q Median 3Q Max -1.08867 -0.06993 0.00508 0.06916 0.53508 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 4.253505 0.098083 43.366 \u0026lt;2e-16 *** log(radio_adstock) 0.186030 0.013809 13.472 \u0026lt;2e-16 *** log(magazines_adstock) 0.010671 0.019567 0.545 0.5861 log(social_media_adstock) 0.018066 0.020384 0.886 0.3766 log(search_ads_adstock) -0.040954 0.022445 -1.825 0.0696 . log(tv_adstock) 0.484510 0.019831 24.432 \u0026lt;2e-16 *** log(newspaper_adstock) 0.009142 0.023393 0.391 0.6964 --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 0.1566 on 193 degrees of freedom Multiple R-squared: 0.8615,\tAdjusted R-squared: 0.8572 F-statistic: 200 on 6 and 193 DF, p-value: \u0026lt; 2.2e-16 \u0026gt; spending.data \u0026lt;- spending.data %\u0026gt;% mutate(tv_adstock = adstock(tv,0.1), + radio_adstock = adstock(radio, 0.1)) \u0026gt; \u0026gt; regression_with_stock \u0026lt;- lm(log(sales) ~ log(radio_adstock) + log(tv_adstock), data=spending.data) \u0026gt; # 85.65% explained \u0026gt; summary(regression_with_stock) Call: lm(formula = log(sales) ~ log(radio_adstock) + log(tv_adstock), data = spending.data) Residuals: Min 1Q Median 3Q Max -1.09358 -0.07401 -0.00318 0.07187 0.59975 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 4.30635 0.08387 51.34 \u0026lt;2e-16 *** log(radio_adstock) 0.18899 0.01318 14.34 \u0026lt;2e-16 *** log(tv_adstock) 0.47064 0.01503 31.30 \u0026lt;2e-16 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 0.157 on 197 degrees of freedom Multiple R-squared: 0.858,\tAdjusted R-squared: 0.8565 F-statistic: 594.9 on 2 and 197 DF, p-value: \u0026lt; 2.2e-16 3.1.5 Synergy Effect 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 \u0026gt; # synergy effect \u0026gt; center \u0026lt;- function(x) { scale(x, scale = F)} \u0026gt; \u0026gt; regression \u0026lt;- lm(log(sales) ~ log(radio) + log(magazines) + log(social_media) + log(search_ads) + log(tv) + log(newspaper) + log(radio) * log(tv), data=spending.data) \u0026gt; summary(regression) Call: lm(formula = log(sales) ~ log(radio) + log(magazines) + log(social_media) + log(search_ads) + log(tv) + log(newspaper) + log(radio) * log(tv), data = spending.data) Residuals: Min 1Q Median 3Q Max -0.29003 -0.07859 -0.02022 0.04670 0.54257 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 5.880586 0.111464 52.758 \u0026lt; 2e-16 *** log(radio) -0.117384 0.032064 -3.661 0.000325 *** log(magazines) 0.026573 0.013728 1.936 0.054378 . log(social_media) 0.029733 0.013181 2.256 0.025215 * log(search_ads) -0.034111 0.014086 -2.422 0.016386 * log(tv) 0.182792 0.024784 7.375 4.79e-12 *** log(newspaper) -0.007208 0.015271 -0.472 0.637475 log(radio):log(tv) 0.058409 0.006992 8.354 1.30e-14 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 0.1116 on 192 degrees of freedom Multiple R-squared: 0.93,\tAdjusted R-squared: 0.9274 F-statistic: 364.2 on 7 and 192 DF, p-value: \u0026lt; 2.2e-16 \u0026gt; \u0026gt; regression \u0026lt;- lm(log(sales) ~ log(radio) + log(tv) + log(radio) * log(tv), data=spending.data) \u0026gt; summary(regression) Call: lm(formula = log(sales) ~ log(radio) + log(tv) + log(radio) * log(tv), data = spending.data) Residuals: Min 1Q Median 3Q Max -0.29435 -0.07741 -0.02310 0.05873 0.59461 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 5.870204 0.104963 55.926 \u0026lt; 2e-16 *** log(radio) -0.108507 0.032403 -3.349 0.000974 *** log(tv) 0.184361 0.022818 8.080 6.53e-14 *** log(radio):log(tv) 0.057335 0.007097 8.079 6.54e-14 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 0.1142 on 196 degrees of freedom Multiple R-squared: 0.9252,\tAdjusted R-squared: 0.9241 F-statistic: 808.3 on 3 and 196 DF, p-value: \u0026lt; 2.2e-16 \u0026gt; \u0026gt; spending.data \u0026lt;- spending.data %\u0026gt;% mutate(radio_log_centered = center(log(radio)), + tv_log_centered = center(log(tv)), + newspaper_log_centered = center(log(newspaper)), + magazines_log_centered = center(log(magazines)), + social_media_log_centered = center(log(social_media)), + search_ads_log_centered = center(log(search_ads))) \u0026gt; \u0026gt; regression \u0026lt;- lm(log(sales) ~ radio_log_centered + magazines_log_centered + social_media_log_centered + search_ads_log_centered + tv_log_centered + newspaper_log_centered + radio_log_centered * tv_log_centered, data=spending.data) \u0026gt; summary(regression) Call: lm(formula = log(sales) ~ radio_log_centered + magazines_log_centered + social_media_log_centered + search_ads_log_centered + tv_log_centered + newspaper_log_centered + radio_log_centered * tv_log_centered, data = spending.data) Residuals: Min 1Q Median 3Q Max -0.29003 -0.07859 -0.02022 0.04670 0.54257 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 7.169892 0.007894 908.260 \u0026lt; 2e-16 *** radio_log_centered 0.155506 0.007395 21.027 \u0026lt; 2e-16 *** magazines_log_centered 0.026573 0.013728 1.936 0.0544 . social_media_log_centered 0.029733 0.013181 2.256 0.0252 * search_ads_log_centered -0.034111 0.014086 -2.422 0.0164 * tv_log_centered 0.343443 0.012150 28.267 \u0026lt; 2e-16 *** newspaper_log_centered -0.007208 0.015271 -0.472 0.6375 radio_log_centered:tv_log_centered 0.058409 0.006992 8.354 1.3e-14 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 0.1116 on 192 degrees of freedom Multiple R-squared: 0.93,\tAdjusted R-squared: 0.9274 F-statistic: 364.2 on 7 and 192 DF, p-value: \u0026lt; 2.2e-16 \u0026gt; \u0026gt; regression \u0026lt;- lm(log(sales) ~ radio_log_centered + tv_log_centered + radio_log_centered * tv_log_centered, data=spending.data) \u0026gt; summary(regression) Call: lm(formula = log(sales) ~ radio_log_centered + tv_log_centered + radio_log_centered * tv_log_centered, data = spending.data) Residuals: Min 1Q Median 3Q Max -0.29435 -0.07741 -0.02310 0.05873 0.59461 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 7.169879 0.008074 888.050 \u0026lt; 2e-16 *** radio_log_centered 0.159367 0.007374 21.611 \u0026lt; 2e-16 *** tv_log_centered 0.342059 0.008256 41.429 \u0026lt; 2e-16 *** radio_log_centered:tv_log_centered 0.057335 0.007097 8.079 6.54e-14 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 0.1142 on 196 degrees of freedom Multiple R-squared: 0.9252,\tAdjusted R-squared: 0.9241 F-statistic: 808.3 on 3 and 196 DF, p-value: \u0026lt; 2.2e-16 3.2 Compare groups 3.2.1 import and check data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 \u0026gt; ad.df \u0026lt;- read.csv(\u0026#34;8_clickstream.csv\u0026#34;, stringsAsFactors = TRUE) \u0026gt; summary(ad.df) visit_date condition time_spent_homepage_sec clicked_article 13/04/2020: 1045 quality:15000 Min. :46.10 Min. :0.0000 23/04/2020: 1042 taste :15000 1st Qu.:49.32 1st Qu.:0.0000 02/04/2020: 1038 Median :50.00 Median :1.0000 31/03/2020: 1033 Mean :50.00 Mean :0.6054 15/04/2020: 1031 3rd Qu.:50.67 3rd Qu.:1.0000 27/04/2020: 1029 Max. :54.02 Max. :1.0000 (Other) :23782 clicked_like clicked_share Min. :0.0000 Min. :0.00000 1st Qu.:0.0000 1st Qu.:0.00000 Median :0.0000 Median :0.00000 Mean :0.1177 Mean :0.03143 3rd Qu.:0.0000 3rd Qu.:0.00000 Max. :1.0000 Max. :1.00000 \u0026gt; str(ad.df) \u0026#39;data.frame\u0026#39;:\t30000 obs. of 6 variables: $ visit_date : Factor w/ 30 levels \u0026#34;01/04/2020\u0026#34;,\u0026#34;02/04/2020\u0026#34;,..: 30 30 30 30 30 30 30 30 30 30 ... $ condition : Factor w/ 2 levels \u0026#34;quality\u0026#34;,\u0026#34;taste\u0026#34;: 2 2 2 2 2 2 2 2 2 2 ... $ time_spent_homepage_sec: num 49 48.9 49.1 49.3 50.4 ... $ clicked_article : int 1 1 1 0 0 1 1 1 1 0 ... $ clicked_like : int 0 0 0 1 1 0 0 0 0 0 ... $ clicked_share : int 1 0 0 0 0 0 0 0 0 0 ... \u0026gt; ad.df$clicked_article \u0026lt;- factor(ad.df$clicked_article, ordered = FALSE) \u0026gt; ad.df$clicked_like \u0026lt;- factor(ad.df$clicked_like, ordered = FALSE) \u0026gt; ad.df$clicked_share \u0026lt;- factor(ad.df$clicked_share, ordered = FALSE) 3.2.2 Descriptives by group 3.2.2.1 seconds spent vary for two versions of ads. 1 2 3 4 5 \u0026gt; # seconds spent vary for two versions of ads. \u0026gt; aggregate(time_spent_homepage_sec ~ condition, data = ad.df, mean) condition time_spent_homepage_sec 1 quality 49.99489 2 taste 49.99909 3.2.2.2 the frequency with which different combinations of condition and like occur 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026gt; table(ad.df$condition, ad.df$clicked_article) 0 1 quality 5873 9127 taste 5965 9035 \u0026gt; table(ad.df$condition, ad.df$clicked_like) 0 1 quality 13964 1036 taste 12506 2494 \u0026gt; table(ad.df$condition, ad.df$clicked_share) 0 1 quality 14550 450 taste 14507 493 3.2.3 Visualization by group 1 \u0026gt; histogram(~ clicked_article | condition, data = ad.df) {% asset_image final_24.png %}\n1 \u0026gt; histogram(~ clicked_like | condition, data = ad.df) {% asset_image final_25.png %}\n1 \u0026gt; histogram(~ clicked_share | condition, data = ad.df) {% asset_image final_26.png %}\n1 2 \u0026gt; ad.mean \u0026lt;- aggregate(time_spent_homepage_sec ~ condition, data = ad.df, mean) \u0026gt; barchart(time_spent_homepage_sec ~ condition, data = ad.mean, col = \u0026#34;grey\u0026#34;) {% asset_image final_27.png %}\n3.2.4 Statistical tests 3.2.4.1 chisp.test() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 \u0026gt; # chisp.test() \u0026gt; chisq.test(table(ad.df$clicked_article, ad.df$condition)) Pearson\u0026#39;s Chi-squared test with Yates\u0026#39; continuity correction data: table(ad.df$clicked_article, ad.df$condition) X-squared = 1.1555, df = 1, p-value = 0.2824 \u0026gt; chisq.test(table(ad.df$clicked_like, ad.df$condition)) Pearson\u0026#39;s Chi-squared test with Yates\u0026#39; continuity correction data: table(ad.df$clicked_like, ad.df$condition) X-squared = 681.57, df = 1, p-value \u0026lt; 2.2e-16 \u0026gt; chisq.test(table(ad.df$clicked_share, ad.df$condition)) Pearson\u0026#39;s Chi-squared test with Yates\u0026#39; continuity correction data: table(ad.df$clicked_share, ad.df$condition) X-squared = 1.9313, df = 1, p-value = 0.1646 3.2.4.2 t.test() 1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026gt; # t.test() \u0026gt; t.test(time_spent_homepage_sec ~ condition, data = ad.df) Welch Two Sample t-test data: time_spent_homepage_sec by condition t = -0.36288, df = 29997, p-value = 0.7167 alternative hypothesis: true difference in means between group quality and group taste is not equal to 0 95 percent confidence interval: -0.02691480 0.01850573 sample estimates: mean in group quality mean in group taste 49.99489 49.99909 3.2.4.3 anova 1 2 3 4 5 6 7 8 \u0026gt; ad.aov.con \u0026lt;- aov(time_spent_homepage_sec ~ condition, data = ad.df) \u0026gt; anova(ad.aov.con) Analysis of Variance Table Response: time_spent_homepage_sec Df Sum Sq Mean Sq F value Pr(\u0026gt;F) condition 1 0.1 0.13259 0.1317 0.7167 Residuals 29998 30204.2 1.00687 ","date":"2023-04-05T00:00:00Z","permalink":"https://MyLoveES.github.io/p/r-final/","title":"R final"},{"content":"一、项目生成 1. Spring Initializr 访问 Spring Initializr 的网站：https://start.spring.io/ 选择语言和 Spring Boot 版本。默认是 Java 和最新版本的 Spring Boot。 选择项目元数据，包括 Group、Artifact、Name、Description、Package Name 等信息。 选择项目依赖，比如 Web、JPA、Security 等。根据具体需求选择所需依赖，可以使用搜索框进行搜索。 确认上面的信息都填写正确后，点击 Generate 按钮，即可生成一个基于 Maven 的 Spring Boot 项目的压缩包。 下载该压缩包，并解压到本地磁盘上。 2、Spring Boot CLI 首先，你需要安装 Spring Boot CLI。你可以参考 Spring Boot 官方文档中的安装指南：https://docs.spring.io/spring-boot/docs/current/reference/html/getting-started.html#getting-started.installing.cli 打开命令行终端，输入以下命令，创建一个基于 Maven 的 Spring Boot 项目： 1 spring init --build maven --groupId=com.weasley --artifactId=file-center --name=file-center --description=\u0026#34;file-center\u0026#34; file-center 意外情况 Java - Gradle - Springboot 版本 1 2 3 https://stackoverflow.com/questions/74931848/spring-boot-3-x-upgrade-could-not-resolve-org-springframework-bootspring-boot Go to the settings --\u0026gt; Build, Execution, Deployment --\u0026gt; Build Tools --\u0026gt; Gradle. Click on your gradle project under \u0026#39;Gradle Projects\u0026#39;. Choose your Gradle JVM for the project 二、子模块 1) submodule-common 1 2 3 1) mkdir submodule-common 2) mkdir -p src/main/java/com/weasley/common 3) mkdir -p src/main/resources 2) submodule-domain 1 2 3 1) mkdir submodule-domain 2) mkdir -p src/main/java/com/weasley/domain 3) mkdir -p src/main/resources 3) submodule-application 1 2 3 1) mkdir submodule-application 2) mkdir -p src/main/java/com/weasley/application 3) mkdir -p src/main/resources 4) submodule-interface 1 2 3 1) mkdir submodule-interface 2) mkdir -p src/main/java/com/weasley/interface 3) mkdir -p src/main/resources 5) submodule-infrastructure 1 2 3 1) mkdir submodule-infrastructure 2) mkdir -p src/main/java/com/weasley/infrastructure 3) mkdir -p src/main/resources 2.1 gradle 根目录 setting.gradle\n1 2 3 4 5 include \u0026#39;submodule-common\u0026#39; include \u0026#39;submodule-domain\u0026#39; include \u0026#39;submodule-application\u0026#39; include \u0026#39;submodule-interface\u0026#39; include \u0026#39;submodule-infrastructure\u0026#39; 根目录 build.gradle\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 // 定义项目构建所需的仓库和依赖项 // 定义项目构建所需的仓库和依赖项 buildscript { repositories { mavenCentral() } dependencies { classpath(\u0026#39;org.springframework.boot:spring-boot-gradle-plugin:3.0.4\u0026#39;) } } plugins { id \u0026#39;java\u0026#39; id \u0026#39;groovy\u0026#39; id \u0026#39;java-library\u0026#39; id \u0026#39;org.springframework.boot\u0026#39; version \u0026#39;3.0.4\u0026#39; id \u0026#39;io.spring.dependency-management\u0026#39; version \u0026#39;1.1.0\u0026#39; } allprojects{ group = \u0026#39;com.weasley\u0026#39; version = \u0026#39;0.0.1-SNAPSHOT\u0026#39; sourceCompatibility = \u0026#39;17\u0026#39; repositories { mavenCentral() } dependencyManagement { imports { mavenBom \u0026#39;org.springframework.boot:spring-boot-dependencies:3.0.4\u0026#39; } } } subprojects { apply plugin: \u0026#39;java\u0026#39; apply plugin: \u0026#39;java-library\u0026#39; apply plugin: \u0026#39;groovy\u0026#39; apply plugin: \u0026#39;org.springframework.boot\u0026#39; apply plugin: \u0026#39;io.spring.dependency-management\u0026#39; dependencies {} tasks.named(\u0026#39;test\u0026#39;) { useJUnitPlatform() } } 2.2 maven 1 mvn archetype:generate -DgroupId=com.weasley -DartifactId=sdk-common -Dversion=1.0.0 -DinteractiveMode=false 三、增加通用依赖 1. spring-boot-devtools 1 developmentOnly \u0026#39;org.springframework.boot:spring-boot-devtools\u0026#39; 2. lombok 1 2 compileOnly \u0026#39;org.projectlombok:lombok:1.18.20\u0026#39; annotationProcessor \u0026#39;org.projectlombok:lombok:1.18.20\u0026#39; 3. spring-boot-configuration-processor 1 annotationProcessor \u0026#34;org.springframework.boot:spring-boot-configuration-processor\u0026#34; 4. validation 1 implementation group: \u0026#39;org.springframework.boot\u0026#39;, name: \u0026#39;spring-boot-starter-validation\u0026#39;, version: \u0026#39;3.0.4\u0026#39; 5. mapstruct 1 2 implementation \u0026#39;org.mapstruct:mapstruct:1.5.3.Final\u0026#39; annotationProcessor \u0026#39;org.mapstruct:mapstruct-processor:1.5.3.Final\u0026#39; 6. knife4j 1 implementation group: \u0026#39;com.github.xiaoymin\u0026#39;, name: \u0026#39;knife4j-openapi3-jakarta-spring-boot-starter\u0026#39;, version: \u0026#39;4.0.0\u0026#39; 7. mybatis plus 1 2 3 4 implementation group: \u0026#39;com.baomidou\u0026#39;, name: \u0026#39;mybatis-plus-boot-starter\u0026#39;, version: \u0026#39;3.5.3.1\u0026#39; implementation group: \u0026#39;com.baomidou\u0026#39;, name: \u0026#39;mybatis-plus-generator\u0026#39;, version: \u0026#39;3.5.3.1\u0026#39; implementation group: \u0026#39;org.freemarker\u0026#39;, name: \u0026#39;freemarker\u0026#39;, version: \u0026#39;2.3.31\u0026#39; implementation group: \u0026#39;mysql\u0026#39;, name: \u0026#39;mysql-connector-java\u0026#39;, version: \u0026#39;8.0.32\u0026#39; 8. spock 1 2 testImplementation group: \u0026#39;org.spockframework\u0026#39;, name: \u0026#39;spock-core\u0026#39;, version: \u0026#39;2.3-groovy-4.0\u0026#39; testImplementation group: \u0026#39;org.spockframework\u0026#39;, name: \u0026#39;spock-spring\u0026#39;, version: \u0026#39;2.3-groovy-4.0\u0026#39; 9. h2 1 testImplementation group: \u0026#39;com.h2database\u0026#39;, name: \u0026#39;h2\u0026#39;, version: \u0026#39;2.1.214\u0026#39; 10. embedded-redis 1 2 3 testImplementation (group: \u0026#39;it.ozimov\u0026#39;, name: \u0026#39;embedded-redis\u0026#39;, version: \u0026#39;0.7.3\u0026#39;) { exclude group: \u0026#39;org.slf4j\u0026#39;, module: \u0026#39;slf4j-simple\u0026#39; } 11. actuator 1 implementation group: \u0026#39;org.springframework.boot\u0026#39;, name: \u0026#39;spring-boot-starter-actuator\u0026#39;, version: \u0026#39;3.0.5\u0026#39; 12. redission 1 implementation group: \u0026#39;org.redisson\u0026#39;, name: \u0026#39;redisson-spring-boot-starter\u0026#39;, version: \u0026#39;3.20.0\u0026#39; 13. caffeine 1 implementation group: \u0026#39;com.github.ben-manes.caffeine\u0026#39;, name: \u0026#39;caffeine\u0026#39;, version: \u0026#39;3.1.5\u0026#39; 14. hutool-core 1 2 implementation group: \u0026#39;cn.hutool\u0026#39;, name: \u0026#39;hutool-core\u0026#39;, version: \u0026#39;5.8.15\u0026#39; implementation group: \u0026#39;cn.hutool\u0026#39;, name: \u0026#39;hutool-extra\u0026#39;, version: \u0026#39;5.8.16\u0026#39; 15. minio 1 implementation group: \u0026#39;io.minio\u0026#39;, name:\u0026#39;minio\u0026#39;, version: \u0026#39;8.4.3\u0026#39; 16. vavr 1 implementation group: \u0026#39;io.vavr\u0026#39;, name: \u0026#39;vavr\u0026#39;, version: \u0026#39;0.10.4\u0026#39; 17. apm 1 implementation group: \u0026#39;org.apache.skywalking\u0026#39;, name: \u0026#39;apm-toolkit-logback-1.x\u0026#39;, version: \u0026#39;8.15.0\u0026#39; 18. json-path 1 implementation group: \u0026#39;com.jayway.jsonpath\u0026#39;, name: \u0026#39;json-path\u0026#39;, version: \u0026#39;2.8.0\u0026#39; 添加数据库表 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 import com.baomidou.mybatisplus.generator.FastAutoGenerator; import com.baomidou.mybatisplus.generator.config.OutputFile; import com.baomidou.mybatisplus.generator.engine.FreemarkerTemplateEngine; import org.junit.jupiter.api.Test; import java.util.Collections; public class CodeGenerator { @Test public void generator() { FastAutoGenerator.create(\u0026#34;jdbc:mysql://localhost:3306/file_center?serverTimezone=Asia/Shanghai\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;\u0026#34;) .globalConfig(builder -\u0026gt; { builder.author(\u0026#34;baomidou\u0026#34;) // 设置作者 .enableSwagger() // 开启 swagger 模式 .fileOverride() // 覆盖已生成文件 .outputDir(\u0026#34;/Users/disco/Downloads/outputFileCenter\u0026#34;); // 指定输出目录 }) .packageConfig(builder -\u0026gt; { builder.parent(\u0026#34;com.baomidou.mybatisplus.samples.generator\u0026#34;) // 设置父包名 .moduleName(\u0026#34;system\u0026#34;) // 设置父包模块名 .pathInfo(Collections.singletonMap(OutputFile.xml, \u0026#34;/Users/disco/Downloads/outputFileCenter\u0026#34;)); // 设置mapperXml生成路径 }) .strategyConfig(builder -\u0026gt; { builder.addInclude(Collections.emptyList()); // 设置需要生成的表名 // .addTablePrefix(\u0026#34;t_\u0026#34;, \u0026#34;c_\u0026#34;); // 设置过滤表前缀 }) .templateEngine(new FreemarkerTemplateEngine()) // 使用Freemarker引擎模板，默认的是Velocity引擎模板 .execute(); } } ","date":"2023-03-18T00:00:00Z","permalink":"https://MyLoveES.github.io/p/build-spring-project/","title":"Build spring project"},{"content":"表 file 字段 类型 说明 id long, primary key 自增主键 file_id varchar, business key 业务主键，文件Id fs_key varchar 文件存储key file_md5 varchar 文件md5 create_time datetime 创建时间 update_time datetime 更新时间 file_owner 字段 类型 说明 id long, primary key 自增主键 file_id varchar, business key 业务主键，文件Id file_name varchar 文件名 file_type varchar 文件类型 status integer 状态；0失效，1有效，2排队中，3上传中，4下载中，-3上传失败，-4下载失败 owner_id varchar 拥有者Id create_time datetime 创建时间 update_time datetime 更新时间 file_download 字段 类型 说明 id long, primary key 自增主键 file_download_id varchar, business key 业务主键，文件下载Id file_id varchar, business key 业务主键，文件Id file_origin_path varchar 原始文件路径 download_retry_times integer 重试次数 callback_required integer 是否需要回调 callback_url varchar 回调URL callback_retry_time integer 回调重试次数 callback_status integer 回调状态；0等待回调，1回调成功，-1回调失败 callback_result varchar 回调结果 create_time datetime 创建时间 update_time datetime 更新时间 注意的点 封装SDK 客户端登记文件，直传存储引擎，再次确认状态 流程图 客户端上传 客户端提交 新建项目 1. init 1 spring init --groupId=com.weasley --artifactId=file-center --name=file-center --description=\u0026#34;file-center\u0026#34; file-center 2. module 根目录 setting.gradle\n1 2 3 4 include \u0026#39;file-center-common\u0026#39; include \u0026#39;file-center-domain\u0026#39; include \u0026#39;file-center-application\u0026#39; include \u0026#39;file-center-interface\u0026#39; 根目录 build.gradle\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 // 定义项目构建所需的仓库和依赖项 // 定义项目构建所需的仓库和依赖项 buildscript { repositories { mavenCentral() } dependencies { classpath(\u0026#39;org.springframework.boot:spring-boot-gradle-plugin:3.0.4\u0026#39;) } } plugins { id \u0026#39;java\u0026#39; id \u0026#39;groovy\u0026#39; id \u0026#39;java-library\u0026#39; id \u0026#39;org.springframework.boot\u0026#39; version \u0026#39;3.0.4\u0026#39; id \u0026#39;io.spring.dependency-management\u0026#39; version \u0026#39;1.1.0\u0026#39; } allprojects{ group = \u0026#39;com.weasley\u0026#39; version = \u0026#39;0.0.1-SNAPSHOT\u0026#39; sourceCompatibility = \u0026#39;17\u0026#39; repositories { mavenCentral() } dependencyManagement { imports { mavenBom \u0026#39;org.springframework.boot:spring-boot-dependencies:3.0.4\u0026#39; } } } subprojects { apply plugin: \u0026#39;java\u0026#39; apply plugin: \u0026#39;java-library\u0026#39; apply plugin: \u0026#39;groovy\u0026#39; apply plugin: \u0026#39;org.springframework.boot\u0026#39; apply plugin: \u0026#39;io.spring.dependency-management\u0026#39; dependencies { implementation group: \u0026#39;org.springframework.boot\u0026#39;, name: \u0026#39;spring-boot-starter-web\u0026#39; implementation \u0026#39;org.springframework.boot:spring-boot-starter\u0026#39; testImplementation \u0026#39;org.springframework.boot:spring-boot-starter-test\u0026#39; // devtools compileOnly \u0026#39;org.springframework.boot:spring-boot-devtools\u0026#39; // lombok compileOnly \u0026#39;org.projectlombok:lombok:1.18.26\u0026#39; annotationProcessor \u0026#39;org.projectlombok:lombok:1.18.26\u0026#39; // spring-boot-configuration-processor annotationProcessor \u0026#34;org.springframework.boot:spring-boot-configuration-processor\u0026#34; // spring-boot-starter-validation implementation group: \u0026#39;org.springframework.boot\u0026#39;, name: \u0026#39;spring-boot-starter-validation\u0026#39;, version: \u0026#39;3.0.4\u0026#39; // https://mvnrepository.com/artifact/org.springframework.boot/spring-boot-starter-actuator implementation group: \u0026#39;org.springframework.boot\u0026#39;, name: \u0026#39;spring-boot-starter-actuator\u0026#39;, version: \u0026#39;3.0.5\u0026#39; // mapstruct implementation \u0026#39;org.mapstruct:mapstruct:1.5.3.Final\u0026#39; annotationProcessor \u0026#39;org.mapstruct:mapstruct-processor:1.5.3.Final\u0026#39; // knife4j implementation group: \u0026#39;com.github.xiaoymin\u0026#39;, name: \u0026#39;knife4j-openapi3-jakarta-spring-boot-starter\u0026#39;, version: \u0026#39;4.1.0\u0026#39; // mybatis plus implementation group: \u0026#39;com.baomidou\u0026#39;, name: \u0026#39;mybatis-plus-boot-starter\u0026#39;, version: \u0026#39;3.5.3.1\u0026#39; implementation group: \u0026#39;com.baomidou\u0026#39;, name: \u0026#39;mybatis-plus-generator\u0026#39;, version: \u0026#39;3.5.3.1\u0026#39; implementation group: \u0026#39;org.freemarker\u0026#39;, name: \u0026#39;freemarker\u0026#39;, version: \u0026#39;2.3.31\u0026#39; implementation group: \u0026#39;mysql\u0026#39;, name: \u0026#39;mysql-connector-java\u0026#39;, version: \u0026#39;8.0.32\u0026#39; // spock testImplementation group: \u0026#39;org.spockframework\u0026#39;, name: \u0026#39;spock-core\u0026#39;, version: \u0026#39;2.3-groovy-4.0\u0026#39; testImplementation group: \u0026#39;org.spockframework\u0026#39;, name: \u0026#39;spock-spring\u0026#39;, version: \u0026#39;2.3-groovy-4.0\u0026#39; // h2 testImplementation group: \u0026#39;com.h2database\u0026#39;, name: \u0026#39;h2\u0026#39;, version: \u0026#39;2.1.214\u0026#39; // embedded-redis testImplementation (group: \u0026#39;it.ozimov\u0026#39;, name: \u0026#39;embedded-redis\u0026#39;, version: \u0026#39;0.7.3\u0026#39; ) { exclude group: \u0026#39;org.slf4j\u0026#39;, module: \u0026#39;slf4j-simple\u0026#39; } // redission implementation group: \u0026#39;org.redisson\u0026#39;, name: \u0026#39;redisson-spring-boot-starter\u0026#39;, version: \u0026#39;3.20.0\u0026#39; // https://mvnrepository.com/artifact/com.github.ben-manes.caffeine/caffeine implementation group: \u0026#39;com.github.ben-manes.caffeine\u0026#39;, name: \u0026#39;caffeine\u0026#39;, version: \u0026#39;3.1.5\u0026#39; // hutool-core implementation group: \u0026#39;cn.hutool\u0026#39;, name: \u0026#39;hutool-core\u0026#39;, version: \u0026#39;5.8.15\u0026#39; implementation group: \u0026#39;cn.hutool\u0026#39;, name: \u0026#39;hutool-extra\u0026#39;, version: \u0026#39;5.8.16\u0026#39; // minio implementation group: \u0026#39;io.minio\u0026#39;, name:\u0026#39;minio\u0026#39;, version: \u0026#39;8.4.3\u0026#39; // vavr implementation group: \u0026#39;io.vavr\u0026#39;, name: \u0026#39;vavr\u0026#39;, version: \u0026#39;0.10.4\u0026#39; implementation group: \u0026#39;org.apache.skywalking\u0026#39;, name: \u0026#39;apm-toolkit-logback-1.x\u0026#39;, version: \u0026#39;8.15.0\u0026#39; // https://mvnrepository.com/artifact/com.jayway.jsonpath/json-path implementation group: \u0026#39;com.jayway.jsonpath\u0026#39;, name: \u0026#39;json-path\u0026#39;, version: \u0026#39;2.8.0\u0026#39; } tasks.named(\u0026#39;test\u0026#39;) { useJUnitPlatform() } } 1) file-center-common 1 2 3 1) mkdir file-center-common 2) mkdir -p src/main/java/com/weasley/common 3) mkdir -p src/main/resources 2) file-center-domain 1 2 3 1) mkdir file-center-domain 2) mkdir -p src/main/java/com/weasley/domain 3) mkdir -p src/main/resources 3) file-center-application 1 2 3 1) mkdir file-center-application 2) mkdir -p src/main/java/com/weasley/application 3) mkdir -p src/main/resources 4) file-center-interface 1 2 3 1) mkdir file-center-interface 2) mkdir -p src/main/java/com/weasley/interface 3) mkdir -p src/main/resources 5) file-center-infrastructure 1 2 3 1) mkdir file-center-infrastructure 2) mkdir -p src/main/java/com/weasley/infrastructure 3) mkdir -p src/main/resources 3. boot scripts 4. logback 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;include resource=\u0026#34;org/springframework/boot/logging/logback/defaults.xml\u0026#34;/\u0026gt; \u0026lt;springProperty scope=\u0026#34;context\u0026#34; name=\u0026#34;spring.application.name\u0026#34; source=\u0026#34;spring.application.name\u0026#34;/\u0026gt; \u0026lt;!--定义日志文件的存储地址 勿在 LogBack 的配置中使用相对路径--\u0026gt; \u0026lt;property name=\u0026#34;logging.path\u0026#34; value=\u0026#34;log\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;CONSOLE_LOG_PATTERN\u0026#34; value=\u0026#34;%clr(%d{yyyy-MM-dd HH:mm:ss.SSS}){faint} %clr(${LOG_LEVEL_PATTERN:-%5p}) %clr([${PID:- }]){magenta} %clr([%X{tid}]){magenta} %clr([%X{requestId}]){magenta} %clr([%X{requestPath}]){magenta} %clr([%X{requestIp}]){magenta} %clr(---){faint} %clr([%15.15t]){faint}%clr([%X{chainId}]){faint}%clr([%X{stageId}]){faint} %clr(%-50.50logger{49}){cyan} %clr(:){faint} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;ROLL_FILE_LOG_PATTERN\u0026#34; value=\u0026#34;%d{yyyy-MM-dd HH:mm:ss.SSS} ${LOG_LEVEL_PATTERN:-%5p} [${PID:- }] [%X{tid}] [%X{requestId}] [%X{requestPath}] [%X{requestIp}] --- [%t][%X{chainId}][%X{stageId}] %-50.50logger{49} : %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}}\u0026#34;/\u0026gt; \u0026lt;!-- 控制台输出 --\u0026gt; \u0026lt;appender name=\u0026#34;STDOUT\u0026#34; class=\u0026#34;ch.qos.logback.core.ConsoleAppender\u0026#34;\u0026gt; \u0026lt;!-- 日志输出编码 --\u0026gt; \u0026lt;encoder class=\u0026#34;ch.qos.logback.core.encoder.LayoutWrappingEncoder\u0026#34;\u0026gt; \u0026lt;layout class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.mdc.TraceIdMDCPatternLogbackLayout\u0026#34;\u0026gt; \u0026lt;pattern\u0026gt;${CONSOLE_LOG_PATTERN}\u0026lt;/pattern\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;!-- 控制台异步输出 --\u0026gt; \u0026lt;appender name=\u0026#34;STDOUTASYNC\u0026#34; class=\u0026#34;ch.qos.logback.classic.AsyncAppender\u0026#34;\u0026gt; \u0026lt;!-- 不丢失日志.默认的,如果队列的80%已满,则会丢弃TRACT、DEBUG、INFO级别的日志 --\u0026gt; \u0026lt;discardingThreshold\u0026gt;0\u0026lt;/discardingThreshold\u0026gt; \u0026lt;!-- 更改默认的队列的深度,该值会影响性能.默认值为256 --\u0026gt; \u0026lt;queueSize\u0026gt;512\u0026lt;/queueSize\u0026gt; \u0026lt;!-- 添加附加的appender,最多只能添加一个 --\u0026gt; \u0026lt;appender-ref ref=\u0026#34;STDOUT\u0026#34;/\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;!-- 按照每天生成日志文件 --\u0026gt; \u0026lt;appender name=\u0026#34;FILE\u0026#34; class=\u0026#34;ch.qos.logback.core.rolling.RollingFileAppender\u0026#34;\u0026gt; \u0026lt;file\u0026gt;${logging.path}/${spring.application.name}.log\u0026lt;/file\u0026gt; \u0026lt;rollingPolicy class=\u0026#34;ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy\u0026#34;\u0026gt; \u0026lt;!--日志文件输出的文件名--\u0026gt; \u0026lt;FileNamePattern\u0026gt;${logging.path}/${spring.application.name}.log.%d{yyyy-MM-dd}.%i.log\u0026lt;/FileNamePattern\u0026gt; \u0026lt;maxFileSize\u0026gt;100MB\u0026lt;/maxFileSize\u0026gt; \u0026lt;MaxHistory\u0026gt;7\u0026lt;/MaxHistory\u0026gt; \u0026lt;totalSizeCap\u0026gt;1GB\u0026lt;/totalSizeCap\u0026gt; \u0026lt;/rollingPolicy\u0026gt; \u0026lt;encoder class=\u0026#34;ch.qos.logback.core.encoder.LayoutWrappingEncoder\u0026#34;\u0026gt; \u0026lt;layout class=\u0026#34;org.apache.skywalking.apm.toolkit.log.logback.v1.x.mdc.TraceIdMDCPatternLogbackLayout\u0026#34;\u0026gt; \u0026lt;!--格式化输出：%d表示日期，%thread表示线程名，%msg：日志消息，%n是换行符--\u0026gt; \u0026lt;pattern\u0026gt;${ROLL_FILE_LOG_PATTERN}\u0026lt;/pattern\u0026gt; \u0026lt;/layout\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;!-- 日志文件异步输出 --\u0026gt; \u0026lt;appender name=\u0026#34;FILEASYNC\u0026#34; class=\u0026#34;ch.qos.logback.classic.AsyncAppender\u0026#34;\u0026gt; \u0026lt;!-- 不丢失日志.默认的,如果队列的80%已满,则会丢弃TRACT、DEBUG、INFO级别的日志 --\u0026gt; \u0026lt;discardingThreshold\u0026gt;0\u0026lt;/discardingThreshold\u0026gt; \u0026lt;!-- 更改默认的队列的深度,该值会影响性能.默认值为256 --\u0026gt; \u0026lt;queueSize\u0026gt;512\u0026lt;/queueSize\u0026gt; \u0026lt;!-- 添加附加的appender,最多只能添加一个 --\u0026gt; \u0026lt;appender-ref ref=\u0026#34;FILE\u0026#34;/\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;root level=\u0026#34;INFO\u0026#34;\u0026gt; \u0026lt;appender-ref ref=\u0026#34;STDOUTASYNC\u0026#34;/\u0026gt; \u0026lt;appender-ref ref=\u0026#34;FILEASYNC\u0026#34;/\u0026gt; \u0026lt;/root\u0026gt; \u0026lt;/configuration\u0026gt; 5. docs 6. SQL generate 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 import com.baomidou.mybatisplus.generator.FastAutoGenerator; import com.baomidou.mybatisplus.generator.config.OutputFile; import com.baomidou.mybatisplus.generator.engine.FreemarkerTemplateEngine; import org.junit.jupiter.api.Test; import java.util.Collections; public class CodeGenerator { @Test public void generator() { FastAutoGenerator.create(\u0026#34;jdbc:mysql://localhost:3306/file_center?serverTimezone=Asia/Shanghai\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;\u0026#34;) .globalConfig(builder -\u0026gt; { builder.author(\u0026#34;baomidou\u0026#34;) // 设置作者 .enableSwagger() // 开启 swagger 模式 .fileOverride() // 覆盖已生成文件 .outputDir(\u0026#34;/Users/disco/Downloads/outputFileCenter\u0026#34;); // 指定输出目录 }) .packageConfig(builder -\u0026gt; { builder.parent(\u0026#34;com.baomidou.mybatisplus.samples.generator\u0026#34;) // 设置父包名 .moduleName(\u0026#34;system\u0026#34;) // 设置父包模块名 .pathInfo(Collections.singletonMap(OutputFile.xml, \u0026#34;/Users/disco/Downloads/outputFileCenter\u0026#34;)); // 设置mapperXml生成路径 }) .strategyConfig(builder -\u0026gt; { builder.addInclude(Collections.emptyList()); // 设置需要生成的表名 // .addTablePrefix(\u0026#34;t_\u0026#34;, \u0026#34;c_\u0026#34;); // 设置过滤表前缀 }) .templateEngine(new FreemarkerTemplateEngine()) // 使用Freemarker引擎模板，默认的是Velocity引擎模板 .execute(); } } 7.application.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 spring: application: name: file_center profiles: active: test mvc: pathmatch: matching-strategy: ant_path_matcher datasource: url: jdbc:mysql://localhost:3306/file_center?serverTimezone=Asia/Shanghai username: root password: platform123456 driver-class-name: com.mysql.cj.jdbc.Driver servlet: multipart: enabled: true max-file-size: 100MB max-request-size: 200MB data: redis: timeout: 1000 host: 127.0.0.1 port: 6379 password: YpWcTLkW lettuce: pool: max-active: 100 max-wait: 500 max-idle: 10 min-idle: 10 time-between-eviction-runs: 10s lock: engine: lock-caffeine timeout-seconds: 1 expire-seconds: 120 caffeine: capacity: 10000 maximum-size: 20000 storage: engine: storage-minio local: enabled: true region: asia bucket: file-center minio: enabled: true region: zh-cn-beijing bucket: file-center endpoint: http://127.0.0.1:9000 sslEnable: false accessKey: 4MrQwqp2WyeAh33S secretKey: DCA3sYbWHZlAcwcEHD1foRRVcbiSqqrX custom: executor: configs: stage-download: executor-name: stage-download queue-capacity: 100 core-pool-size: 40 max-pool-size: 100 allow-core-Thread-time-out: false keep-alive-seconds: 20 await-termination: false await-termination-seconds: 5000 abort-policy: block stage-upload: executor-name: stage-upload queue-capacity: 200 core-pool-size: 40 max-pool-size: 100 allow-core-Thread-time-out: false keep-alive-seconds: 20 await-termination: false await-termination-seconds: 5000 abort-policy: block stage-callback: executor-name: stage-callback queue-capacity: 300 core-pool-size: 40 max-pool-size: 100 allow-core-Thread-time-out: false keep-alive-seconds: 20 await-termination: false await-termination-seconds: 5000 abort-policy: block http-callback: executor-name: dispatcher-http-callback queue-capacity: 400 core-pool-size: 40 max-pool-size: 100 allow-core-Thread-time-out: false keep-alive-seconds: 20 await-termination: false await-termination-seconds: 5000 abort-policy: block http: configs: http-callback: client-name: http-callback dispatcher-executor-name: dispatcher-http-callback call-timeout-milliseconds: 1000 connect-timeout-milliseconds: 1000 readTimeoutMilliseconds: 1000 writeTimeoutMilliseconds: 1000 connectionMaxPoolSize: 100 keepAliveSeconds: 20 maxRequests: 300 maxRequestsPerHost: 50 mybatis-plus: mapper-locations: classpath:mappers/*.xml type-aliases-package: com.weasley.infrastructure.db.model configuration: log-impl: org.apache.ibatis.logging.stdout.StdOutImpl swagger: enable: true knife4j: enable: true management: endpoints: web: exposure: include: health,info,metrics,threadpool exclude: shutdown monitor: log: enable: true report: enable: true 8. file-center-common 1) ErrorCode 2) Response 3) Global Exception 4) Response advice 5) Submodule build.gradle 1 2 3 4 # unittest tasks.named(\u0026#39;test\u0026#39;) { useJUnitPlatform() } ","date":"2023-03-18T00:00:00Z","permalink":"https://MyLoveES.github.io/p/self-file/","title":"Self - file"},{"content":"项目结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 . ├── HELP.md ├── README.md ├── mvnw ├── mvnw.cmd ├── pom.xml └── src ├── main │ ├── java │ │ └── com │ │ └── mylovees │ │ └── sweet7 │ │ └── Sweet7Application.java │ └── resources │ └── application.properties └── test └── java └── com └── mylovees └── sweet7 └── Sweet7ApplicationTests.java ","date":"2022-11-02T00:00:00Z","permalink":"https://MyLoveES.github.io/p/%E6%90%AD%E5%BB%BAddd%E8%84%9A%E6%89%8B%E6%9E%B6/","title":"搭建DDD脚手架"},{"content":"maven add module 根目录pom添加 1 \u0026lt;packaging\u0026gt;pom\u0026lt;/packaging\u0026gt; maven 添加module 1 mvn archetype:generate -DgroupId=com.sweet7 -DartifactId=sweet7-domain -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false ","date":"2022-11-01T00:00:00Z","permalink":"https://MyLoveES.github.io/p/maven-related/","title":"Maven related"},{"content":"Q：处理器、进程或者节点各自独立，如何才能够达成共识？ 一般情况下，节点间可以彼此传递消息达成一致。但是有故障节点存在的是时候，故障节点可能会向其他节点发送一个错误值，或者发送一些随即值，甚至不发送，导致每个处理器获取到不同的数据，最终计算出不一致的结果。\n1 比如，处理器间的时钟同步，集群服务器间的数据同步 Reaching Agreement in the Presence of Faults 论文作者提出一种方法来消除错误处理器的影响 - 通过使用循环多轮的信息交换方案来处理;这样的方案可能会迫使有错误的处理器暴露自己有错误，或者至少使其行为与没有错误的处理器保持一致，从而使后者能够达成一致(当然，是在一定的条件下)。\nAssumptions 假设总共有n个独立节点，其中错误节点m个，并且不知道具体是哪些节点出现了问题。节点之间只能双方彼此通信，并且假设数据传输是有保障的并且无延迟。接收方可以识别消息的发出者。\n每个节点n具有私有值Vn（可以理解为当前该节点的状态，比如负载等）。对于给定的m、n，通过彼此间的信息交换，让每个节点持有一个所有节点私有值的向量。最终达成：\n非故障节点能够计算得到完全相同的向量; 该向量中，非错误节点的所对应的元素元素，是该节点的私有值 举个例子：\n{% asset_img ReachAggrement-finger1.png finger1 %}\n交互一致性 虽然我们不需要最终知道哪些节点是有问题的，与错误节点对应的向量元素也可以是任意的;但是正确节点对于错误节点的向量元素必须是一致的。\n比如下面的这种情况是不被接受的： {% asset_img ReachAggrement-finger2.png finger2 %}\n正确节点对所有节点(包括有故障的节点)持有的值达成共识，最终得到（交互式）一致性向量。这样，每个节点就能够通过对该向量的计算，继续得到业务上的所需要的值。\n单节点错误 n=4, m=1 首先可以假设 n=4, m=1。进行两轮信息交换：\n各个节点先把自己的私有值发给其他节点 各个节点彼此交换第一轮收到的信息 在信息交换过程中，错误节点可能会发出错误的值，或者不发出任何值，来干扰其他正常节点的计算。对于一个正常节点，如果没有收到节点N的消息，会将其置为默认值（假设为NULL）。\nSTEP1:\n{% asset_img ReachAggrement-finger3.png finger3 %} STEP2:\n{% asset_img ReachAggrement-finger4.png finger4 %}\n在两轮信息交换完成后，每个节点都会持有“一组”向量值元素。节点可以选取“多数”作为认可的元素值，形成最终的向量。 {% asset_img ReachAggrement-finger5.png finger5 %}\n多节点错误 仅仅两轮信息交换不足以达成共识：\nSTEP1:\n{% asset_img ReachAggrement-finger6.png finger6 %}\nSTEP2:\n{% asset_img ReachAggrement-finger7.png finger7 %}\nFinally:\n{% asset_img ReachAggrement-finger8.png finger8 %}\n继续下一轮交换信息\u0026hellip;\u0026hellip;..\nm+1 轮后：\nP: 节点集合 V: 值的集合 定义：\n1）w=p1p2p3\u0026hellip;.pr, σ(w) 意为 pr -\u0026gt; p(r-1) -\u0026gt; p(r-2) -\u0026gt; ··· -\u0026gt; p2 -\u0026gt; p1，Vpr最终流转到p1的结果。\n2）对于一个单节点，σ(p) = Vp\n3）如果一个节点q是正常的，那么他一定满足：对于任意的集合组成的字串w和任意节点p，\nσ(pqw) = σ(qw) 同理如果一个集合全部是正常节点，那么集合所组成的字串 w=p1p2p3...pr，和一个任意节点p', 一定能够满足 σ(pwp') = σ(wp') 那么，通过如下方法帮助p节点得到q值（总节点n，错误节点m，n\u0026gt;=3m+1）：\n对于集合P的某个大小超过(n+m)/2的子集Q，σp(pwq) = v 对于每个长度不大于m的字串w（取自Q）都成立，那么p记录下v； 否则，算法将递归应用m-1，n-1，使P-{q}来替代P，并且对于每个长度不大于m的字串w（取自于P-{q}） σp'(pw) = σp(pwq) 如果在这向量n-1个元素里有至少(n+m)/2个元素值相同，p记录下该值，否则记录NIL值。 step1：（目的是确定源节点q正确与否）一定能够找到一个全部是正常节点的集合Q(size \u0026lt;= m)，使得正常的源节点q的值，经过Q处理后，依然不变。 {% asset_img ReachAggrement-finger9.png finger9 %}\nstep2：（目的是对错误节点的值达成共识）如果源节点q没能满足step1，说明q在乱发值，q是一个问题节点。\nq向d发送X： {% asset_img ReachAggrement-finger10.png finger10 %}\nq向e发送Y： {% asset_img ReachAggrement-finger11.png finger11 %}\n所以q的值是多少不重要了，重要的是其他节点要对q的值达成共识。做法，把问题节点q踢出去，询问其他节点，在他们眼里，q是多少。如果某个值Vq\u0026rsquo;超过半数认可，那么就以Vq\u0026rsquo;作为q的值，否则，记默认值NIL。即：\nσp'(pw) = σp'(pwq') = σp(pwq'q) = σp(pwq) p 问q\u0026rsquo;（中间也经过了step1的处理），你眼里q是多少？如果获得了不一致的答案，说明q\u0026rsquo;也有问题，踢了，再问其他节点，最终得到一个正常节点认可的值Vq'1\n最终，p得到了其他所有正常节点眼里的q值: Vq'1 Vq'2 \u0026hellip;.. Vq\u0026rsquo;k，如果在这中间，某个值Vq\u0026rsquo;m超过了半数，那么以Vq\u0026rsquo;m作为q的值，否值取默认值NIL。\n这个过程就像是询问“认可值”，只要获得了足够多的“认可”，就可视Vq为q的值。\nn=7, m=2 以A为主视角\nstep1: 每个节点把自己的值发送给其他节点 {% asset_img ReachAggrement-finger12.png finger12 %}\nstep2: 每个节点分享上一轮接收到的值 step3: 每个节点再次分享上一轮接收到的值 {% asset_img ReachAggrement-finger13.png finger13 %}\n对于一个正常节点，经过子集Q，抵达A的值不会变 {% asset_img ReachAggrement-finger14.png finger14 %}\n对于一个非正常节点，经过子集Q，抵达A的值可能会变。此时需要踢出错误节点，来达成值的一致 {% asset_img ReachAggrement-finger15.png finger15 %}\n{% asset_img ReachAggrement-finger16.png finger16 %}\n拜占庭国王放下手中的 Reaching_agreement_in_the_presence_of_faults.pdf，陷入沉思。\n{% asset_img ReachAggrement-finger17.png finger17 %}\n最近他的军队正在攻打敌方同样强大的城池，需要将领们协同一致才可制胜。而他也知道，将军们中间有叛徒，正因此进攻才耽搁许久。忽然他眉头一皱，计上心来！\n{% asset_img ReachAggrement-finger18.png finger18 %}\n国王究竟想到了什么办法呢？请看下回：\n{% asset_img ReachAggrement-finger19.png finger19 %}\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 回想一下，上一节给出的过程需要两轮信息交换，第一轮“我的私有值是”，第二轮“节点x告诉我他的私有值是....”。在m个节点故障的一般情况下，需要m + 1轮通信。为了描述该算法，可以以更通用的方式描述这种消息交换。 \u0026lt;center\u0026gt;P: 节点集合\u0026lt;/center\u0026gt; \u0026lt;center\u0026gt;V: 值的集合\u0026lt;/center\u0026gt; 对于 k\u0026gt;=1. 定义k-scenario为从非空字符串（可能含有重复）P(length \u0026lt;= k+1) 映射到 V。对于一个给出的k-level scenario σ 和字符串 w= p1p2...pr, 2\u0026lt;=r\u0026lt;=k+1, σ(w) 意为 pr-\\\u0026gt;p(r-1)-\\\u0026gt;p(r-2)-\u0026gt;···-\\\u0026gt;p2-\\\u0026gt;p1, 【pr 的私有值】。对于一个单一元素的字符串p，σp指代p的私有值Vp。一个k-level scenario 总结了k轮信息交换的结果。(请注意，如果一个错误的节点伪造其他节点给它的信息，这就相当于对给它的值造假。)对于一个正常节点子集，只有可能是确定的映射：尤其是，一个正常节点在传递消息时总是诚实的，所以对于一个正常节点q，任意节点p，以及字符串w来说，一个scenario一定满足：（也就是传递到q的字符串会原封不动地传达给p） \u0026lt;center\u0026gt;σ(pqw) = σ(qw)\u0026lt;/center\u0026gt; 节点p在scenario σ接收的消息由σp对以p开头的字符串的限制操作给出(这句话好难理解，我感觉是在表达，secnario σ 是给来源字串加了一个p头)。现在我们要描述，对于任意m\u0026gt;=0,n\u0026gt;=3m+1, 计算p的过程。对于一个给定的σp, 其中交互一致性向量的元素对应着每个节点p。计算过程如下： 1. 对于P的某个大小\u0026gt;(n+m)/2的子集Q，σp(pwq) = v 对于每个长度\u0026lt;=m的字串w（取自Q）都成立，那么p记录下v； 2. 否则，算法将递归应用m-1，n-1，使P-{q}来替代P，并且对于每个长度不大于m的字串w（取自于P-{q}） \u0026lt;center\u0026gt;σp\u0026#39;(pw) = σp(pwq)\u0026lt;/center\u0026gt; 如果在这向量n-1个元素里有至少(n+m)/2个元素值相同，p记录下该值，否则记录NIL值。 σp\u0026#39;反应了m-level σ的子场景，其中q被排除在外，并且在σp\u0026#39;每个节点的私有值是它直接从σ中的q获得的值。 通过对m进行约简，证明上述算法确实保证了交互一致性: Basis m=0。此时没有节点是错误的，算法总是在第一步结束。p记录Vq为q的私有值； Induction Step m\u0026gt;0. 如果q是正常节点，对于来自于正常节点集合的，长度不大于m的字串w（包括空串），σp(pwq)=Vq。这个集合包含了n-m个成员，多于(n+m)/2，满足了条件1. 此外，满足这些要求的任何其他集合必然包含一个正常节点, 因为集合数量大于(n+m)/2，而n\u0026gt;=3m+1. 因此也必然得到Vq作为可用值。因此算法在第(1)步终止，p按要求记录Vq为q的私有值。 现在假设q是错误的。我们必须表明p记录下的q私有值Vq和其他正常节点一致。 首先考虑这种情况：p和其他节点p\u0026#39;都在step1结束，他们都找到了一个合适的集合Q。由于每个集合都包含有(n+m)/2个成员，并且由于P总共只有n个成员，两个集合必须有超过2((n + m)/2) - n = m个公共成员。因为其中至少有一个必须是正常的节点，所以这两个集合必须产生相同的值v。 接下来我们假设p\u0026#39;在step1退出了，找到了合适的集合Q以及值v，并且p执行step2. ","date":"2022-08-11T00:00:00Z","permalink":"https://MyLoveES.github.io/p/reaching-agreement-in-the-presence-of-faults/","title":"Reaching agreement in the presence of faults"},{"content":"Byzantine Generals Problem 一组拜占庭将军分别各率领一支军队共同围困一座城市。为了简化问题，将各支军队的行动策略限定为进攻或撤离两种。因为部分军队进攻部分军队撤离可能会造成灾难性后果，因此各位将军必须通过投票来达成一致策略，即所有军队一起进攻或所有军队一起撤离。因为各位将军分处城市不同方向，他们只能通过信使互相联系。在投票过程中每位将军都将自己投票给进攻还是撤退的信息通过信使分别通知其他所有将军，这样一来每位将军根据自己的投票和其他所有将军送来的信息就可以知道共同的投票结果而决定行动策略。\n系统的问题在于，可能将军中出现叛徒，他们不仅可能向较为糟糕的策略投票，还可能选择性地发送投票信息。假设有9位将军投票，其中1名叛徒。8名忠诚的将军中出现了4人投进攻，4人投撤离的情况。这时候叛徒可能故意给4名投进攻的将领送信表示投票进攻，而给4名投撤离的将领送信表示投撤离。这样一来在4名投进攻的将领看来，投票结果是5人投进攻，从而发起进攻；而在4名投撤离的将军看来则是5人投撤离。这样各支军队的一致协同就遭到了破坏。\n由于将军之间需要通过信使通讯，叛变将军可能通过伪造信件来以其他将军的身份发送假投票。而即使在保证所有将军忠诚的情况下，也不能排除信使被敌人截杀，甚至被敌人间谍替换等情况。因此很难通过保证人员可靠性及通讯可靠性来解决问题。\n假使那些忠诚（或是没有出错）的将军仍然能通过多数决定来决定他们的战略，便称达到了拜占庭容错。在此，票都会有一个预设值，若讯息（票）没有被收到，则使用此预设值来投票。\n上述的故事对映到计算机系统里，将军便成了计算机，而信差就是通讯系统。虽然上述的问题涉及了电子化的决策支援与资讯安全，却没办法单纯的用密码学与数位签章来解决。因为电路错误仍可能影响整个加密过程，这不是密码学与数位签章演算法在解决的问题。因此计算机就有可能将错误的结果送出去，亦可能导致错误的决策。\n","date":"2022-07-11T00:00:00Z","permalink":"https://MyLoveES.github.io/p/%E6%8B%9C%E5%8D%A0%E5%BA%AD%E5%B0%86%E5%86%9B-%E5%8C%85%E5%90%AB%E6%81%B6%E6%84%8F%E8%8A%82%E7%82%B9%E7%9A%84%E5%85%B1%E8%AF%86%E9%97%AE%E9%A2%98/","title":"拜占庭将军 - 包含恶意节点的共识问题"},{"content":"INTRODUCTION A reliable computer system must be able to cope with the failure of one or more of its components. A failed component may exhibit a type of behavior that is often overlooked\u0026ndash;namely, sending conflicting information to different parts of the system. The problem of coping with this type of failure is expressed abstractly as the Byzantine Generals Problem. We devote the major part of the paper to a discussion of this abstract problem and conclude by indicating how our solutions can be used in implementing a reliable computer system. We imagine that several divisions of the Byzantine army are camped outside an enemy city, each division commanded by its own general. The generals can communicate with one another only by messenger. After observing the enemy, they must decide upon a common plan of action. However, some of the generals may be traitors, trying to prevent the loyal generals from reaching agreement. The generals must have an algorithm to guarantee that\n一个可靠的计算机系统必须能够应付它的一个或多个部件的故障。一个失败的组件可能会表现出一种经常被忽略的行为——即，向系统的不同部分发送冲突的信息。应对这种类型的失败的问题被抽象地表达为拜占庭将军问题。我们将论文的主要部分用于讨论这一抽象问题，并通过表明我们的解决方案可以用于实现一个可靠的计算机系统。我们想象一下，拜占庭军队的几个师在敌人的城市外扎营，每个师由自己的将军指挥。将军们只能通过信使相互联系。在观察敌人之后，他们必须决定一个共同的行动计划。然而，有些将军可能是叛徒，试图阻止忠诚的将军达成协议。将军们肯定有算法来保证:\nA. All loyal generals decide upon the same plan of action. The loyal generals will all do what the algorithm says they should, but the traitors may do anything they wish. The algorithm must guarantee condition A regardless of what the traitors do. The loyal generals should not only reach agreement, but should agree upon a reasonable plan. We therefore also want to insure that.\nA. 所有忠诚的将军都决定相同的行动计划。 忠诚的将军都会按照算法的要求行事，而叛徒则可以随心所欲。算法必须保证不管叛徒做了什么, 所有忠诚的将军都决定相同的行动计划。忠诚的将军们不仅要达成一致，而且要商定一个合理的计划。因此，我们也希望确保这一点\nB. A small number of traitors cannot cause the loyal generals to adopt a bad plan. B. 少数叛徒不能使忠诚的将军采取错误的计划。\nCondition B is hard to formalize, since it requires saying precisely what a bad plan is, and we do not attempt to do so. Instead, we consider how the generals reach a decision. Each general observes the enemy and communicates his observations to the others. Let v(i) be the information communicated by the ith general. Each general uses some method for combining the values v (1) \u0026hellip;.. v(n) into a single plan of action, where n is the number of generals. Condition A is achieved by having all generals use the same method for combining the information, and Condition B is achieved by using a robust method. For example, if the only decision to be made is whether to attack or retreat, then v(i) con be General i\u0026rsquo;s opinion of which option is best, and the final decision can be based upon a majority vote among them. A small number of traitors can affect the decision only if the loyal generals were almost equally divided between the two possibilities, in which case neither decision could be called bad. While this approach may not be the only way to satisfy conditions A and B, it is the only one we know of. It assumes a method by which the generals communicate their values v (i) to one another. The obvious method is for the ith general to send v (i) by messenger to each other general. However, this does not work, because satisfying condition A requires that every loyal general obtain the same values v(1) \u0026hellip;.. v(n), and a traitorous general may send different values to different generals. For condition A to be satisfied, the following must be true:\n条件B很难去具象化，因为它需要准确地说清楚错误的计划是什么，我们不打算去具体描述它。相反，我们考虑将军们如何达成这个共识。每个将军观察敌人并且和其他人沟通他的观察结果。设v(i)是将军i所传达的信息，每个将军使用一些方法来让v(1)~v(n)形成一个具体的行动。条件A是通过让所有的将军使用相同的方法来组合信息来实现的，条件B是通过使用鲁棒的方法来实现的。比如，如果决策是去进攻或者撤退，v(i)是将军i认为最佳的选项，最终结论可以通过他们的多数投票决定。只有当忠诚的将领几乎平分两种可能性时，少数叛徒才能影响决策，在这种情况下，任何一个决策都不能被称为错误的决定。虽然这种方法可能不是满足条件A和B的唯一方法，但它是我们所知道的唯一方法。它假定了一种方法，通过这种方法，将军们互相传达他们的价值观v (i)。最明显的方法是第i个将军通过信使发送v (i)给其他将军。然而，这是行不通的，因为满足条件A要求每个忠诚的将军都获得相同的值v(1) \u0026hellip;..V (n)，一个叛变的将军可能会向不同的将军传达不同的价值观。要满足条件A，必须满足以下条件:\n1) Every loyal general must obtain the same information v (1) \u0026hellip;. , v (n). Condition 1 implies that a general cannot necessarily use a value of v(i) obtained directly from the ith general, since a traitorous ith general may send different values to different generals. This means that unless we are careful, in meeting condition 1 we might introduce the possibility that the generals use a value of v (i) different from the one sent by the ith general\u0026ndash;even though the ith general is loyal. We must not allow this to happen if condition B is to be met. For example, we cannot permit a few traitors to cause the loya generals to base their decision upon the values \u0026ldquo;retreat\u0026rdquo;,\u0026hellip;, \u0026ldquo;retreat\u0026rdquo; if every loyal general sent the value \u0026ldquo;attack\u0026rdquo;. We therefore have the following requirement for each i:\n每个忠诚的将军必须获得相同的信息v (1) \u0026hellip;.， v(n)。 条件1表明，将军不能使用直接从第i个将军处获得的v(i)的值，因为叛变的第i个将军可能会向不同的将军发送不同的值。这意味着，除非我们很小心，在满足条件1时，我们可能会引入这样一种可能性:将军使用的v (i)值与第i个将军发送的值不同——即使第i个将军是忠诚的。如果要满足条件B，我们决不能允许这种情况发生。例如，我们不能允许少数叛徒使忠诚将军们根据 “retreat”\u0026hellip;..\u0026ldquo;retreat\u0026rdquo; 的价值观作出决定，如果每一个忠诚的将军都发出“进攻”的指令。因此，我们对每个i有以下要求:\n2) If the ith general is loyal, then the value that he sends must be used by every loyal general as the value of v (i). 如果第i个将军是忠诚的，那么他发送的值必须被每个忠诚的将军用作v (i)的值。 We can rewrite condition I as the condition that for every i (whether or not the ith general is loyal):\n1\u0026rsquo;. Any two loyal generals use the same value of v(i).\nConditions 1\u0026rsquo; and 2 are both conditions on the single value sent by the ith general. We can therefore restrict our consideration to the problem of how a single general sends his value to the others. We phrase this in terms of a commanding general sending an order to his lieutenants, obtaining the following problem. Byzantine Generals Problem. A commanding general must send an order to his n - 1 lieutenant generals such that\n条件1\u0026rsquo;和2都是第i个将军发送的单个值的条件。因此，我们可以把我们的考虑限制在一个将军如何把他的值传递给其他人的问题上。我们用一个将军向他的副手们发出命令的方式来表述这个问题，得到了下面的问题。拜占庭将军的问题。一位指挥官必须向他的n - 1名中将发出这样的命令:\nIC1. All loyal lieutenants obey the same order. IC2. If the commanding general is loyal, then every loyal lieutenant obeys the order he sends. IC1。所有忠诚的中尉都服从同一条命令。 IC2。如果指挥官是忠诚的，那么每个忠诚的中尉都会服从他的命令。\nConditions IC1 and IC2 are called the interactive consistency conditions. Note that if the commander is loyal, then IC1 follows from IC2. However, the commander need not be loyal. To solve our original problem, the ith general sends his value of v(i) by using a solution to the Byzantine Generals Problem to send the order \u0026ldquo;use v (i) as my value\u0026rdquo;, with the other generals acting as the lieutenants.\nIC1和IC2称为交互一致性条件。注意，如果指挥官是忠诚的，IC1包含于IC2。但是指挥官不一定是忠诚的。为了解决我们最初的问题，第i个将军发出他的值v(i)，通过使用拜占庭将军问题的一个解决方案，发送命令“使用v(i)作为我的值”，其他将军充当中尉。\n2. IMPOSSIBILITY RESULTS 不可能的结果 The Byzantine Generals Problem seems deceptively simple. Its difficulty is indicated by the surprising fact that if the generals can send only oral messages, then no solution will work unless more than two-thirds of the generals are loyal. In particular, with only three generals, no solution can work in the presence of a single traitor. An oral message is one whose contents are completely under the control of the sender, so a traitorous sender can transmit any possible message. Such a message corresponds to the type of message that computers normally send to one another. In Section 4 we consider signed, written messages, for which this is not true.\nWe now show that with oral messages no solution for three generals can handle a single traitor. For simplicity, we consider the case in which the only possible decisions are \u0026ldquo;attack\u0026rdquo; or \u0026ldquo;retreat\u0026rdquo;. Let us first examine the scenario pictured in Figure 1 in which the commander is loyal and sends an \u0026ldquo;attack\u0026rdquo; order, but Lieutenant 2 is a traitor and reports to Lieutenant 1 that he received a \u0026ldquo;retreat\u0026rdquo; order. For IC2 to be satisfied, Lieutenant 1 must obey the order to attack.\nNow consider another scenario, shown in Figure 2, in which the commander is a traitor and sends an \u0026ldquo;attack\u0026rdquo; order to Lieutenant 1 and a \u0026ldquo;retreat\u0026rdquo; order to Lieutenant 2. Lieutenant 1 does not know who the traitor is, and he cannot tell what message the commander actually sent to Lieutenant 2. Hence, the scenarios in these two pictures appear exactly the same to Lieutenant 1. If the traitor lies consistently, then there is no way for Lieutenant 1 to distinguish between these two situations, so he must obey the \u0026ldquo;attack\u0026rdquo; order in both of them. Hence, whenever Lieutenant 1 receives an \u0026ldquo;attack\u0026rdquo; order from the commander, he must obey it.\n拜占庭将军问题看似简单。他的困难点在于，如果将军们只能口头传递信息，那么除非三分之二的将军是忠诚的，否则不会有解决方案。特别是，只有三个将军，任何解决方案在一个叛徒面前都无法奏效。口头信息的内容完全在发送者的控制之下，所以一个叛国的发送者可以传递任何可能的信息。这种消息对应于计算机通常相互发送的消息类型。在第4节中，我们考虑了签名的书面消息, 对于这样的消息，这是做不到的（指的是签名的书面消息，不会让叛徒随意传递消息）。\n我们现在证明，通过口头消息，三个将军的解决方案对付不了一个叛徒。让我们先来看看图1所示的场景:指挥官很忠诚，发出了“攻击”命令，但中尉2是叛徒，他向中尉1报告他收到了“撤退”命令。为了使IC2满意，中尉1必须服从攻击命令。\n现在考虑另一种场景，如图2所示，其中指挥官是叛徒，向中尉1发送“攻击”命令，向中尉2发送“撤退”命令。中尉1不知道叛徒是谁，他也不知道指挥官到底给中尉2发了什么信息。因此，在中尉1看来，这两幅图中的场景是完全相同的。如果叛徒一直说谎，那么中尉1就没有办法区分这两种情况，所以他必须在两种情况下都服从“攻击”命令。因此，每当中尉收到指挥官的“攻击”命令时，他必须遵守。\nHowever, a similar argument shows that if Lieutenant 2 receives a \u0026ldquo;retreat\u0026rdquo; order from the commander then he must obey it even if Lieutenant 1 tells him that the commander said \u0026ldquo;attack\u0026rdquo;. Therefore, in the scenario of Figure 2, Lieutenant 2 must obey the \u0026ldquo;retreat\u0026rdquo; order while Lieutenant 1 obeys the \u0026ldquo;attack\u0026rdquo; order, thereby violating condition IC1. Hence, no solution exists for three generals that works in the presence of a single traitor.\nThis argument may appear convincing, but we strongly advise the reader to be very suspicious of such nonrigorous reasoning. Although this result is indeed correct, we have seen equally plausible \u0026ldquo;proofs\u0026rdquo; of invalid results. We know of no area in computer science or mathematics in which informal reasoning is more likely to lead to errors than in the study of this type of algorithm. For a rigorous proof of the impossibility of a three-general solution that can handle a single traitor, we refer the reader to [3].\n然而，一个类似的论点表明，如果中尉2收到指挥官的“撤退”命令，那么他必须遵守，即使中尉1告诉他指挥官说的是“攻击”。因此，在图2场景中，中尉2必须服从“撤退”命令，而中尉1必须服从“进攻”命令，因此违反了条件IC1。因此，在一个叛徒在场的情况下，三个将军是不存在解决方案的。\nUsing this result, we can show that no solution with fewer than 3m + 1 generals can cope with m traitors. The proof is by contradiction\u0026ndash;we assume such a solution for a group of 3m or fewer and use it to construct a three-general solution to the Byzantine Generals Problem that works with one traitor, which we know to be impossible. To avoid confusion between the two algorithms, we call the generals of the assumed solution Albanian generals, and those of the constructed solution Byzantine generals. Thus, starting from an algorithm that allows 3m or fewer Albanian generals to cope with m traitors, we construct a solution that allows three Byzantine generals to handle a single traitor.\nThe three-general solution is obtained by having each of the Byzantine generals simulate approximately one-third of the Albanian generals, so that each Byzantine general is simulating at most m Albanian generals. The Byzantine commander simulates the Albanian commander plus at most m - 1 Albanian lieutenants, and each of the two Byzantine lieutenants simulates at most m Albanian lieutenants. Since only one Byzantine general can be a traitor, and he simulates at most m Albanians, at most m of the Albanian generals are traitors. Hence, the assumed solution guarantees that IC1 and IC2 hold for the Albanian generals. By IC1, all the Albanian lieutenants being simulated by a loyal Byzantine lieutenant obey the same order, which is the order he is to obey. It is easy to check that conditions IC1 and IC2 of the Albanian generals solution imply the corresponding conditions for the Byzantine generals, so we have constructed the required impossible solution.\n利用这个结果，我们可以证明，少于3m + 1个将军的解无法应对m个叛徒。证明方法是矛盾的——我们假设这样的解对于3m或更少的一群人来说，并使用它来构建一个适用于三个将军，一个叛徒的拜占庭将军问题，我们知道这是不可能的。为了避免两种算法之间的混淆，我们称假设解的将军为阿尔巴尼亚将军，而称构造解的将军为拜占庭将军。因此，从一个允许3m或更少的阿尔巴尼亚将军对付m个叛徒的算法开始，我们构建了一个允许3个拜占庭将军对付一个叛徒的解决方案。\n三个将军的解决方案是由每个拜占庭将军模拟大约三分之一的阿尔巴尼亚将军得到的，这样每个拜占庭将军最多模拟m个阿尔巴尼亚将军。拜占庭指挥官模拟阿尔巴尼亚指挥官加上最多m - 1个阿尔巴尼亚中尉，两个拜占庭中尉分别模拟最多m个阿尔巴尼亚中尉。因为只有一个拜占庭将军可以是叛徒，而且他最多模拟了m个阿尔巴尼亚人，所以最多m个阿尔巴尼亚将军是叛徒。因此，假定的解决方案保证了IC1和IC2适用于阿尔巴尼亚将军。在IC1中，所有的阿尔巴尼亚中尉都被模拟成一个忠诚的拜占庭中尉，服从同一条命令，这是他必须服从的命令。很容易检查阿尔巴尼亚将军解的条件IC1和IC2意味着对应的拜占庭将军解的条件，因此我们已经构造了所需的不可能解。\nOne might think that the difficulty in solving the Byzantine Generals Problem stems from the requirement of reaching exact agreement. We now demonstrate that this is not the case by showing that reaching approximate agreement is just as hard as reaching exact agreement. Let us assume that instead of trying to agree on a precise battle plan, the generals must agree only upon an approximate time of attack. More precisely, we assume that the commander orders the time of the attack, and we require the following two conditions to hold:\n有人可能会认为，解决拜占庭将军问题的困难源于达成确切协议的要求。我们现在通过表明达成近似一致和达成精确一致一样困难来证明，情况并非如此。让我们假设，将领们不必就精确的作战计划达成一致，而只需就大致的进攻时间达成一致。更准确地说，我们假设指挥官下令攻击的时间，我们需要以下两个条件才能维持下去:\nIC1 \u0026lsquo;. All loyal lieutenants attack within 10 minutes of one another. IC2\u0026rsquo;. If the commanding general is loyal, then every loyal lieutenant attacks within 10 minutes of the time given in the commander\u0026rsquo;s order.\nIC1”。所有忠诚的中尉都在十分钟内互相攻击。 IC2”。如果指挥官是忠诚的，那么每个忠诚的中尉在指挥官命令的10分钟内攻击。 (We assume that the orders are given and processed the day before the attack and that the time at which an order is received is irrelevant\u0026ndash;only the attack time given in the order matters.}\n(我们假设命令是在攻击发生的前一天发出并处理的，而接收到命令的时间是无关紧要的——只有命令中给出的攻击时间是重要的。)\nLike the Byzantine Generals Problem, this problem is unsolvable unless more than two-thirds of the generals are loyal. We prove this by first showing that if there were a solution for three generals that coped with one traitor, then we could construct a three-general solution to the Byzantine Generals Problem that also worked in the presence of one traitor. Suppose the commander wishes to send an \u0026ldquo;attack\u0026rdquo; or \u0026ldquo;retreat\u0026rdquo; order. He orders an attack by sending an attack time of 1:00 and orders a retreat by sending an attack time of 2:00, using the assumed algorithm. Each lieutenant uses the following procedure to obtain his order.\n就像拜占庭将军问题一样，这个问题是无法解决的，除非超过三分之二的将军是忠诚的。我们首先证明，如果有一个解决方案，可以解决三个将军同时对付一个叛徒，那么我们就可以构建一个解决三将军一叛徒的拜占庭将军问题。假设指挥官想要发出“进攻”或“撤退”的命令。他通过发送1点的攻击时间来命令攻击，通过发送2点的攻击时间来命令撤退，使用假设的算法。每个中尉都使用以下程序来获得命令。\n(1) After receiving the attack time from the commander, a lieutenant does one of the following: (a) If the time is 1:10 or earlier, then attack. (b) If the time is 1:50 or later, then retreat. (c) Otherwise, continue to step (2). (2) Ask the other lieutenant what decision he reached in step (1). (a) If the other lieutenant reached a decision, then make the same decision he did. (b) Otherwise, retreat. It follows from IC2\u0026rsquo; that if the commander is loyal, then a loyal lieutenant will obtain the correct order in step (1), so IC2 is satisfied. If the commander is loyal, then IC1 follows from IC2, so we need only prove IC1 under the assumption that the commander is a traitor. Since there is at most one traitor, this means that both lieutenants are loyal. It follows from ICI\u0026rsquo; that if one lieutenant decides to attack in step (1), then the other cannot decide to retreat in step (1). Hence, either they will both come to the same decision in step (1) or at least one of them will defer his decision until step (2). In this case, it is easy to see that they both arrive at the same decision, so IC1 is satisfied. We have therefore constructed a three-general solution to the Byzantine Generals Problem that handles one traitor, which is impossible. Hence, we cannot have a three-general algorithm that maintains ICI\u0026rsquo; and IC2\u0026rsquo; in the presence of a traitor. The method of having one general simulate m others can now be used to prove that no solution with fewer than 3rn + 1 generals can cope with m traitors. The proof is similar to the one for the original Byzantine Generals Problem and is left to the reader.\n由IC2’可知，如果指挥官是忠诚的，那么忠诚的中尉将在步骤(1)中获得正确的命令，因此IC2是满足的。如果指挥官是忠诚的，那么IC1从IC2继承而来，所以我们只需要在指挥官是叛徒的假设下证明IC1。因为最多有一个叛徒，这意味着两个中尉都是忠诚的。根据ICI\u0026rsquo;可知，如果一个中尉在步骤(1)中决定进攻，那么另一个中尉在步骤(1)中就不能决定撤退。因此，他们要么在步骤(1)中都做出相同的决定，要么至少有一个会推迟到步骤(2)。在这种情况下，很容易看到他们都做出了相同的决定，所以IC1是满意的。因此，我们建立了一个解决三将军，一叛徒的办法，但这是不可能的。因此，我们不能有一个三将军算法，在叛徒存在的情况下保持ICI\u0026rsquo;和IC2\u0026rsquo;。用一个将军模拟m个其他将军模拟的方法现在可以用来证明，没有一个小于3rn + 1个将军的解可以对付m个叛徒。这个证明类似于原来的拜占庭将军问题，留给读者。\n3. A SOLUTION WITH ORAL MESSAGES 3.一个口头信息的解决方案\nWe showed above that for a solution to the Byzantine Generals Problem using oral messages to cope with rn traitors, there must be at least 3m + 1 generals. We now give a solution that works for 3m + 1 or more generals. However, we first specify exactly what we mean by \u0026ldquo;oral messages\u0026rdquo;. Each general is supposed to execute some algorithm that involves sending messages to the other generals, and we assume that a loyal general correctly executes his algorithm. The definition of an oral message is embodied in the following assumptions which we make for the generals\u0026rsquo; message system:\n我们在上面展示过，要想解决拜占庭将军问题，用口头信息来对付rn叛徒，至少需要300m + 1名将军。我们现在给出的解决方案适用于3m + 1或更多的将军。然而，我们首先具体说明我们所说的“口头信息”是什么意思。每个将军都应该执行一些算法包括向其他将军发送消息，我们假设一个忠诚的将军正确地执行他的算法。口头信息的定义体现在我们对将军信息系统做出的以下假设:\nA1. Every message that is sent is delivered correctly.\nA2. The receiver of a message knows who sent it.\nA3. The absence of a message can be detected.\nA1. 发送的每个消息都被正确地传递。 A2. 消息的接收者知道是谁发送的。\nA3. 可以检测到消息的缺失。\nAssumptions A1 and A2 prevent a traitor from interfering with the communication between two other generals, since by A1 he cannot interfere with the messages they do send, and by A2 he cannot confuse their intercourse by introducing spurious messages. Assumption A3 will foil a traitor who tries to prevent a decision by simply not sending messages. The practical implementation of these assumptions is discussed in Section 6.\n假设A1和A2可以防止叛徒干扰另外两位将军之间的通信，因为A1不能干扰他们实际发送的信息，而A2不能通过引入虚假信息来混淆他们的通信。假设A3将挫败试图通过不发送消息来阻止决策的叛徒。第6节将讨论这些假设的实际实现。\nThe algorithms in this section and in the following one require that each general be able to send messages directly to every other general. In Section 5, we describe algorithms which do not have this requirement.\nA traitorous commander may decide not to send any order. Since the lieuten- ants must obey some order, they need some default order to obey in this case. We let RETREAT be this default order.\n本节和下一节中的算法要求每个将军能够直接向其他将军发送消息。在第5节中，我们描述的算法没有这个要求。\n叛国的指挥官可以决定不发出任何命令。由于中尉必须服从某些命令，在这种情况下，他们需要一些默认的命令来服从。我们让撤退成为这个默认的命令。\nWe inductively define the Oral Message algorithms OM(m), for all nonnegative integers m, by which a commander sends an order to n - 1 lieutenants. We show that OM(m) solves the Byzantine Generals Problem for 3m + 1 or more generals in the presence of at most m traitors. We find it more convenient to describe this algorithm in terms of the lieutenants \u0026ldquo;obtaining a value\u0026rdquo; rather than \u0026ldquo;obeying an order\u0026rdquo;.\n我们归纳地定义了口头消息算法OM(m)，对于所有非负整数m，指挥官通过它向n - 1个中尉发送命令。我们证明OM(m)在最多m个叛徒存在的情况下，解决了3m + 1或更多将军的拜占庭将军问题。我们发现用中尉“获取一个值”来描述这个算法比用“服从一个命令”来描述更方便。\nThe algorithm assumes a function majority with the property that if a majority of the values vi equal v, then majority (V1,···, vn-1 equals v. (Actually, it assumes a sequence of such functions\u0026ndash;one for each n.) There are two natural choices for the value of majority(v1, \u0026hellip;, vn-1):\n该算法假设函数具有如下属性:如果大多数值vi等于v，那么大多数(V1,···，vn-1)等于v(实际上，它假设有一个这样的函数序列——每个n对应一个函数)。多数的值有两个自然的选择(v1，…, vn-1):\nThe majority value among the vi if it exists, otherwise the value RETREAT;\nThe median of the vi, assuming that they come from an ordered set.\n如果存在，则为vi中的多数值，否则为RETREAT值；\nvi的中位数，假设它们来自一个有序集合。\nThe following algorithm requires only the aforementioned property of majority.\n下面的算法只需要上述的多数属性。\nAlgorithm OM(0).\n(1) The commander sends his value to every lieutenant.\n(2) Each lieutenant uses the value he receives from the commander, or uses the value RETREAT if he receives no value.\n(1) 指挥官将他的值发送给每个中尉。\n(2) 每个中尉使用他从指挥官那里收到的值，如果他没有收到值，则使用RETREAT。\nAlgorithm OM(m), m \u0026gt; O.\n(1) The commander sends his value to every lieutenant.\n(2) For each i, let vi be the value Lieutenant i receives from the commander, or else be RETREAT if he receives no value. Lieutenant i acts as the commander in Algorithm OM(m - 1) to send the value vi to each of the n - 2 other lieutenants.\n(3) For each i, and each j ~ i, let vj be the value Lieutenant i received from Lieutenant j in step (2) (using Algorithm OM(m - 1)), or else RETREAT if he received no such value. Lieutenant i uses the value majority (vl \u0026hellip;.. vn-1).\n(1) 司令员向每一个中尉报信。\n(2) 对于每一个i, vi为中尉i从指挥官那里得到的值，否则为撤退，如果他没有得到值。中尉i作为算法OM(m - 1)中的指挥官，将值vi发送给n - 2个其他中尉。\n(3) 对于每个i和每个j ~ i，让vj为步骤(2)中中尉i从中尉j处得到的值(使用算法OM(m - 1))，如果没有得到该值则撤退。中尉i使用的值多数(vl \u0026hellip;..vn-1)。\nTo understand how this algorithm works, we consider the case m = 1, n = 4. Figure 3 illustrates the messages received by Lieutenant 2 when the commander sends the value v and Lieutenant 3 is a traitor. In the first step of OM(1), the commander sends v to all three lieutenants. In the second step, Lieutenant 1 sends the value v to Lieutenant 2, using the trivial algorithm OM(0). Also in the second step, the traitorous Lieutenant 3 sends Lieutenant 2 some other value x. In step 3, Lieutenant 2 then has v1 = v2 = v and v3 = x, so he obtains the correct value v = majority(v, v, x).\n为了理解这个算法是如何工作的，我们假定 m=1, n=4。图三阐释了，当指挥官发送值v并且当副官3是一名叛徒的时候，副官2接收到的值。第一步OM(1)中，指挥官发送v给所有的副官。第二步，副官1发送v给副官2，使用OM(0)。在第二步，叛徒副官3发送给副官2一些其他值，比如x。第三步，副官2有了v1=v2=v and v3=x，所以它维持了正确的v=majority(v,v,x)。\nNext, we see what happens if the commander is a traitor. Figure 4 shows the values received by the lieutenants if a traitorous commander sends three arbitrary values x, y, and z to the three lieutenants. Each lieutenant obtains v1 = x, v2 = y, and v3 = z, so they all obtain the same value majority(x, y, z) in step (3), regardless of whether or not any of the three values x, y, and z are equal.\n然后，我们再来看如果指挥官是一名叛徒的话，会发生什么。图4展示了副官们接收到的值，当指挥官是叛徒时，他分别发送三个值给三个副官：x,y,z。每个副官保持着v1 = x, v2 = y, and v3 = z，所以他们都保持着相同的majority(x,y,z).\nThe recursive algorithm OM(m) invokes n - 1 separate executions of the algorithm OM(m - 1), each of which invokes n - 2 executions of OM(m - 2), etc. This means that, for m \u0026gt; 1, a lieutenant sends many separate messages to each other lieutenant. There must be some way to distinguish among these different messages. The reader can verify that all ambiguity is removed if each lieutenant i prefixes the number i to the value vi that he sends in step (2). As the recursion \u0026ldquo;unfolds,\u0026rdquo; the algorithm OM(m - k) will be called (n - 1) \u0026hellip; (n - k) times to send a value prefixed by a sequence of k lieutenants\u0026rsquo; numbers.\n递归算法OM(m)调用算法OM(m - 1)的n - 1次单独执行，其每次调用OM(m - 2)的n - 2次执行，以此类推。这意味着，对于m\u0026gt;1，一个中尉向其他每个中尉发送了许多单独的信息。必须有某种方法来区分这些不同的信息。读者可以验证，如果每个中尉i在他在步骤(2)中发送的值vi前加上数字i，那么所有的模糊性都会被消除。随着递归“展开”，算法OM(m-k)将被调用 (n - 1) \u0026hellip; (n - k) 次去发送一个前缀是k个副官序号的序列值的value。\nTo prove the correctness of the algorithm OM{m) for arbitrary m, we first prove the following lemma.\n为了证明任意m的算法OM(m)的正确性，我们首先要证明以下定理。\nLEMMA 1. For any m and k, Algorithm OM (m ) satisfies IC2 if there are more than 2k + m generals and at most k traitors.\nLEMMA 1. 对于任何m和k，如果有超过2k+m的将军和最多k的叛徒，算法OM（m）满足IC2。\nPROOF. The proof is by induction on m. IC2 only specifies what must happen if the commander is loyal. Using A1, it is easy to see that the trivial algorithm OM(0) works if the commander is loyal, so the lemma is true for m = 0. We now assume it is true for m-1(m\u0026gt;0), and prove it for m.\nIn step (1), the loyal commander sends a value v to all n - 1 lieutenants. In step (2), each loyal lieutenant applies OM(m - 1) with n - 1 generals. Since by hypothesis n\u0026gt;2k+m, we have n-1\u0026gt;2k+(m- 1),so we can apply the induction hypothesis to conclude that every loyal lieutenant gets vj = v for each loyal Lieutenant j. Since there are at most k traitors, and n - 1 \u0026gt; 2k + (m - 1) \u0026gt; 2k, a majority of the n - 1 lieutenants are loyal. Hence, each loyal lieutenant has vi = v for a majority of the n - 1 values i, so he obtains majority(v1 \u0026hellip;. , vn-1) = v in step (3), proving IC2.\n证明。证明是通过对m的归纳。IC2只规定了如果指挥官是忠诚的，必须发生什么。利用A1，我们很容易看到，如果指挥官是忠诚的，算法OM(0)是有效的，所以该定理对于m=0来说是真的。\n在步骤（1）中，忠诚的指挥官向所有n-1名中尉发送一个值v。在步骤（2）中，每个忠诚的中尉向n-1个将军应用OM（m - 1）。由于假设n\u0026gt;2k+m，我们有n-1\u0026gt;2k+(m-1),所以我们可以应用归纳假设得出结论：每个忠诚的中尉j得到vj=v。由于最多只有k个叛徒，而n-1\u0026gt;2k+(m-1)\u0026gt;2k，n-1个中尉的大多数是忠诚的。因此，每个忠诚的中尉在n - 1的大多数值i中都有vi = v，所以他在步骤(3)中得到major(v1 \u0026hellip;. , vn-1) = v，证明了IC2。\nThe following theorem asserts that Algorithm OM(m) solves the Byzantine Generals Problem.\n以下定理断言，算法OM(m)解决了拜占庭将军问题。\nTHEOREM 1. For any m, Algorithm OM (m ) satisfies conditions IC1 and IC2 if there are more than 3m generals and at most m traitors.\n定理1. 对于任何m，如果有超过3m个将军和最多m个叛徒，则算法OM（m ）满足条件IC1和IC2。\nPROOF. The proof is by induction on m. If there are no traitors, then it is easy to see that OM(0) satisfies IC1 and IC2. We therefore assume that the theorem is true for OM(m - 1) and prove it for OM(m), m \u0026gt; 0.\nWe first consider the case in which the commander is loyal. By taking k equal to m in Lemma 1, we see that OM(m) satisfies IC2. IC1 follows from IC2 if the commander is loyal, so we need only verify IC1 in the case that the commander is a traitor.\nThere are at most m traitors, and the commander is one of them, so at most m - 1 of the lieutenants are traitors. Since there are more than 3m generals, there are more than 3m - 1 lieutenants, and 3m - 1 \u0026gt; 3(m - 1). We may therefore apply the induction hypothesis to conclude that OM(m - 1) satisfies conditions IC1 and IC2. Hence, for each j, any two loyal lieutenants get the same value for vj in step (3). (This follows from IC2 if one of the two lieutenants is Lieutenant j, and from IC1 Otherwise.) Hence, any two loyal lieutenants get the same vector of values vl \u0026hellip;.. Vn-1, and therefore obtain the same value majority(vl \u0026hellip;.. Vn-1) in step (3), proving IC1.\n证明。如果没有叛徒，那么很容易看出OM(0)满足IC1和IC2。因此，我们假设该定理对OM(m - 1)是真的，并对OM(m)，m\u0026gt;0进行证明。\n我们首先考虑司令员是忠诚的情况。通过将k等同于结论1中的m，我们看到OM(m)满足IC2。如果指挥官是忠诚的，IC1由IC2得出，所以我们只需要在指挥官是叛徒的情况下验证IC1。\n最多有m个叛徒，而指挥官是其中之一，所以最多有m-1个中尉是叛徒。由于有超过3米的将军，所以有超过3米-1的中尉，而且3米-1\u0026gt;3（米-1）。因此，我们可以运用归纳假设得出结论：OM(m - 1)满足条件IC1和IC2。因此，对于每个j，任何两个忠诚的副手在步骤（3）中得到的vj值都是一样的。(如果两个中尉中的一个是中尉j，这由IC2得出，否则由IC1得出）。因此，任何两个忠诚的中尉都会得到相同的价值向量vl \u0026hellip;.. Vn-1，因此在步骤(3)中获得相同的值 majority(vl \u0026hellip;.. Vn-1)，证明了IC1。\n4. A SOLUTION WITH SIGNED MESSAGES 签名消息方案\nAs we saw from the scenario of Figures 1 and 2, it is the traitors\u0026rsquo; ability to lie that makes the Byzantine Generals Problem so difficult. The problem becomes easier to solve if we can restrict that ability. One way to do this is to allow the generals to send unforgeable signed messages. More precisely, we add to A1-A3 the A4\n(a) A loyal general\u0026rsquo;s signature cannot be forged, and any alteration of the contents of his signed messages can be detected.\n(b) Anyone can verify the authenticity of a general\u0026rsquo;s signature.\n正如我们从图1和图2的情景中看到的那样，正是叛徒的撒谎能力使拜占庭将军问题变得如此困难。如果我们能限制这种能力，问题就会变得更容易解决。做到这一点的一个方法是允许将军们发送不可伪造的签名信息。更确切地说，我们在A1-A3中加入A4\n(a) 忠诚的将军的签名不能被伪造，任何对其签名信息内容的篡改都能被发现。\n(b) 任何人都可以验证一个将军的签名的真实性。\nNote that we make no assumptions about a traitorous general\u0026rsquo;s signature. In particular, we allow his signature to be forged by another traitor, thereby permitting collusion among the traitors.\nNow that we have introduced signed messages, our previous argument that four generals are required to cope with one traitor no longer holds. In fact, a three-general solution does exist. We now give an algorithm that copes with m traitors for any number of generals. (The problem is vacuous if there are fewer than m + 2 generals.)\n请注意，我们对叛国将军的签名不做任何假设。特别是，我们允许他的签名被另一个叛徒伪造，从而允许叛徒之间的勾结。\n既然我们已经引入了签名信息，那么我们之前的论点，即需要四名将军来应对一个叛徒就不再成立了。事实上，一个三将军的解决方案确实存在。我们现在给出一个算法，可以应对任何数量的将军的m个叛徒。(如果少于m+2个将军，这个问题就是没有意义的）。\nIn our algorithm, the commander sends a signed order to each of his lieutenants. Each lieutenant then adds his signature to that order and sends it to the other lieutenants, who add their signatures and send it to others, and so on. This means that a lieutenant must effectivelyreceive one signed message, make several copies of it, and sign and send those copies. It does not matter how these copies are obtained; a single message might be photocopied, or else each message might consist of a stack of identical messages which are signed and distributed as required.\n在我们的算法中，指挥官向他的每个中尉发送一个签名的命令。然后每个中尉在该命令上加上自己的签名，并将其发送给其他中尉，其他中尉再加上自己的签名，并将其发送给其他人，如此反复。这意味着，一个中尉必须有效地接收一份已签署的信息，将其复制几份，并签署和发送这些副本。如何获得这些副本并不重要；一份电文可能是复印的，否则每份电文可能由一叠相同的电文组成，这些电文按要求进行签署和分发。\nOur algorithm assumes a function choice which is applied to a set of orders to /obtain a single one. The only requirements we make for this function are\nIf the set V consists of the single element v, then choice(V) = v. choice(Q) = RETREAT, where ø is the empty set. 我们的算法假定有一个函数选择，它被应用于一组命令，以获得一个单一的命令。我们对这个函数的唯一要求是\n如果集合V由单一元素v组成，那么选择(V)=v。 choice(Q) = RETREAT，其中是ø空集。 Note that one possible definition is to let choice(V) be the median element of V \u0026ndash; assuming that there is an ordering of the elements.\nIn the following algorithm, we let x:i denote the value x signed by General i. Thus, v:j:i denotes the value v signed by j, and then that value v:j signed by i. We let General 0 be the commander. In this algorithm, each lieutenant i maintains a set Vi, containng the set of properly signed orders he has received so far. (If the commander is loyal, then this set should never contain more than a single element.) Do not confuse Vi, the set of orders he has received, with the set of messages that he has received. There may be many different messages with the same order.\n请注意，一个可能的定义是让choice(V)是V的中位数元素\u0026ndash;假设元素是有序的。\n在下面的算法中，我们让x:i表示由i将军签署的值x。因此，v:j:i表示由j签署的值v，然后是由i签署的那个值v:j。我们让0将军做指挥官。在这个算法中，每个中尉i维护一个集合Vi，包含他迄今为止收到的正确签署的命令集合。(如果指挥官是忠诚的，那么这个集合就不应该包含超过一个元素）。不要把Vi，即他所收到的命令集，与他所收到的信息集混淆起来。同一个命令可能有许多不同的信息。\nAlgorithm SM (m).\nInitially Vi = 0. (1) The commander signs and sends his value to every lieutenant.\n(2) For each i:\n(A) If Lieutenant i receives a message of the form v:0 from the commander and he has not yet received any order, then\n(i) he lets V equal (v); (ii) he sends the message v:0:i to everyother lieutenant. (B) If Lieutenant i receives a message of the form v:0:j1:···:jk and v is not in the set Vi, then\n(i) he adds v to Vi;\n(ii) if k \u0026lt; m, then he sends the message v:0:j1:···:jk:i to every lieutenant other than j1\u0026hellip;..,jk.\n(3) For each i: When Lieutenant i will receive no more messages, he obeys the order choice(Vi).\n最初，Vi = 0。\n(1) 指挥官签署并将他的命令发送给每个中尉。\n(2) 对于每个i：\n(A) 如果i中尉从指挥官那里收到形式为v:0的信息，并且他还没有收到任何命令，那么\n(i) 他让V等于（v）。\n(ii)他将信息v:0:i发送给其他所有的中尉。\n(B) 如果i中尉收到一个形式为v:0:j1:\u0026ndash;:jk的信息，并且v不在集合V中，那么\n(i) 他将v加入Vi。\n(ii) 如果k \u0026lt; m，那么他将信息v:0:j1:\u0026mdash;:jk:i发送给除j1\u0026hellip;..,jk之外的每个中尉。\n(3) 对于每个i。当中尉i不会再收到信息时，他就会服从choice(Vi)的命令。\nNote that in step (2), Lieutenant i ignores any message containing an order v that is already in the set Vi.\n请注意，在步骤（2）中，中尉i会忽略任何包含已经在集合Vi中的命令v的消息。\nWe have not specified how a lieutenant determines in step (3) that he will receive no more messages. By induction on k, one easily shows that for each sequence of lieutenants j1, ···, jk with k\u0026lt;=m, a lieutenant can receive at most one message of the form v:0:j1:···:jk in step (2). If we require that Lieutenant jk either send such a message or else send a message reporting that he will not send such a message, then it is easy to decide when all messages have been received. (By assumption A3, a lieutenant can determine if a traitorous lieutenant jk sends neither of those two messages.) Alternatively, time-out can be used to determine when no more messages will arrive. The use of time-out is discussed in Section 6.\n我们没有说明中尉如何在步骤(3)中确定他不会再收到信息。通过对k的归纳，我们很容易发现，对于每一个k\u0026lt;=m的中尉序列j1, \u0026mdash;, jk，一个中尉在步骤(2)中最多可以收到一条形式为v:0:j1:\u0026mdash;:jk的信息。如果我们要求中尉jk要么发送这样的信息，要么发送一个报告他不会发送这样的信息的信息，那么很容易决定何时所有信息都被收到了。(根据假设A3，中尉可以判断出叛徒中尉jk是否不发送这两条信息)。另外，也可以用超时来决定何时不再有信息到达。第6节将讨论超时的使用。\nNote that in step (2), Lieutenant i ignores any messages that do not have the proper form of a value followed by a string of signatures. If packets of identical messages are used to avoid having to copy messages, this means that he throws away any packet that does not consist of a sufficient number of identical, properly signed messages.(There should be(n-k-2)(n-k-3)···(n-m-2)copies of the message if it has been signed by k lieutenants.)\n请注意，在步骤(2)中，中尉i忽略了任何没有正确形式的信息，即一个值后面有一串签名的信息。如果使用相同的信息包来避免复制信息，这意味着他扔掉任何不包含足够数量的相同的、正确签名的信息包。（如果信息已经被k个中尉签名，应该有(n-k-2)(n-k-3)\u0026mdash;(n-m-2)份。）\nFigure 5 illustrates Algorithm SM(1) for the case of three generals when the commander is a traitor. The commander sends an \u0026ldquo;attack\u0026rdquo; order to one lieutenant and a \u0026ldquo;retreat\u0026rdquo; order to the other. Both lieutenants receive the two orders in step (2), so after step (2) V1 = V2 = {\u0026ldquo;attack\u0026rdquo;, \u0026ldquo;retreat\u0026rdquo;}, and they both obey the order choice( {\u0026ldquo;attack\u0026rdquo;, \u0026ldquo;retreat\u0026rdquo;} ). Observe that here, unlike the situation in Figure 2, the lieutenants know the commander is a traitor because his signature appears on two different orders, and A4 states that only he could have generated those signatures.\nIn Algorithm SM(m), a lieutenant signs his name to acknowledge his receipt of an order. If he is the mth lieutenant to add his signature to the order, then that signature is not relayed to anyone else by its recipient, so it is superfluous. (More precisely, assumption A2 makes it unnecessary.) In particular, the lieutenants need not sign their messages in SM(1).\n图5说明了当指挥官是叛徒时三个将军的情况下的算法SM(1)。指挥官向一名中尉发出了 \u0026ldquo;攻击 \u0026ldquo;命令，向另一名中尉发出了 \u0026ldquo;撤退 \u0026ldquo;命令。两个中尉在步骤（2）中都收到了这两个命令，所以在步骤（2）之后，V1=V2={\u0026ldquo;进攻\u0026rdquo;，\u0026ldquo;撤退\u0026rdquo;}，他们都服从命令选择（{\u0026ldquo;进攻\u0026rdquo;，\u0026ldquo;撤退\u0026rdquo;}）。请注意，这里与图2的情况不同，中尉们知道指挥官是个叛徒，因为他的签名出现在两个不同的命令上，而且A4说只有他才能产生这些签名。\n在算法SM(m)中，一名中尉签署了他的名字，以确认他收到了一份命令。如果他是第m个在命令上签名的中尉，那么这个签名就不会被其接收者转达给其他人，所以它是多余的。(更确切地说，假设A2使其成为不必要的。)特别是，中尉们不需要在SM(1)中签署他们的信息。\nWe now prove the correctness of our algorithm.\n证明算法\nTHEOREM 2. For any m, Algorithm SM(m) solves the Byzantine Generals Problem if there are at most m traitors.\n定理2. 对于任何m，如果有最多m个叛徒，算法SM(m)可以解决拜占庭将军问题。\nPROOF. We first prove IC2. If the commander is loyal, then he sends his signed order v:0 to every lieutenant in step (1). Every loyal lieutenant will therefore receive the order v in step (2)(A). Moreover, since no traitorous lieutenant can forge any other message of the form v\u0026rsquo;:0, a loyal lieutenant can receive no additional order in step (2)(B). Hence, for each loyal Lieutenant i, the set Vi obtained in step (2) consists of the single order v, which he will obey in step (3) by property 1 of the choice function. This proves IC2.\nSince IC1 follows from IC2 if the commander is loyal, to prove IC1 we need only consider the case in which the commander is a traitor. Two loyal lieutenants i and j obey the same order in step (3) if the sets of orders Vi and Vj that they receive in step (2) are the same. Therefore, to prove IC1 it suffices to prove that, if i puts an order v into Vi in step (2), thenj must put the same order v into V1in step (2). To do this, we must show that j receives a properly signed message containing that order. If i receives the order v in step (2)(A), then he sends it to j in step (2)(A)(ii); so j receives it (by A1). If i adds the order to Vi in step (2)(B), then he must receive a first message of the form v:0 :j1:···:jk. If j is one of the jr, then by A4 he must already have received the order v. If not, we consider two cases:\nk \u0026lt; m. In this case, i sends the message v:0:j1: \u0026hellip; :jk:i to j; soj must receive the order v. k = m. Since the commander is a traitor, at most m - 1 of the lieutenants are traitors. Hence, at least one of the lieutenants j1, \u0026hellip;. , jm is loyal. This loyal lieutenant must have sent j the value v when he first received it, so j must therefore receive that value. 证明。我们首先证明IC2。如果指挥官是忠诚的，那么他在步骤(1)中向每个中尉发送了他签署的命令v:0。因此，每个忠诚的中尉都会在步骤（2）（A）中收到命令v。此外，由于没有一个叛徒中尉可以伪造任何其他形式的v\u0026rsquo;:0的信息，忠诚的中尉在步骤(2)(B)中无法收到任何额外的命令。因此，对于每个忠诚的中尉i来说，在步骤(2)中得到的集合Vi包括单一的命令v，根据选择函数的属性1，他将在步骤(3)中服从这个命令。这就证明了IC2。\n由于如果指挥官是忠诚的，则IC1由IC2得出，为了证明IC1，我们只需要考虑指挥官是叛徒的情况。如果两个忠诚的中尉i和j在步骤(3)中接受的命令集Vi和Vj是相同的，那么他们在步骤(2)中就会服从同一个命令。因此，要证明IC1，只需证明，如果i在步骤(2)中将一个命令v放入Vi中，那么j在步骤(2)中必须将同样的命令v放入V1中。要做到这一点，我们必须证明j收到一个包含该命令的正确签名信息。如果i在步骤(2)(A)中收到了命令v，那么他就把它发送给了j, 在步骤(2)(A)(ii)中；所以j收到了它（通过A1）。如果i在步骤(2)(B)中将订单添加到Vi中，那么他必须收到形式为v:0 :j1:\u0026mdash;:jk的第一个信息。如果j是jr之一，那么根据A4，他一定已经收到了订单v。 如果不是，我们考虑两种情况:\n1.在这种情况下，i向j发送了v:0:j1:\u0026hellip;:jk:i的信息；所以j必须接受命令v。\n2.k=m。由于指挥官是叛徒，所以最多只有m-1名中尉是叛徒。因此，至少有一个中尉j1, \u0026hellip;. , jm是忠诚的。这个忠诚的中尉在第一次收到价值v的时候，一定给j发送了这个命令，因此j一定会收到这个命令。\n5. MISSING COMMUNICATION PATHS Thus far, we have assumed that a general can send messages directly to every other general. We now remove this assumption. Instead, we suppose that physical barriers place some restrictions on who can send messages to whom. We consider the generals to form the nodes of a simple, finite undirected graph G, where an arc between two nodes indicates that those two generals can send messages directly to one another. We now extend Algorithms OM(m) and SM(m), which assumed G to be completely connected, to more general graphs.\nTo extend our oral message algorithm OM(m), we need the following definition, where two generals are said to be neighbors if they are joined by an arc.\n到目前为止，我们假设一个将军可以直接向其他每个将军发送信息。现在我们取消这一假设。相反，我们假设物理屏障对谁能向谁发送信息有一些限制。我们认为将军们构成了一个简单的、有限的无向图G的节点，其中两个节点之间的弧表示这两个将军可以直接向对方发送消息。现在我们将假设G是完全连接的算法OM(m)和SM(m)扩展到更一般的图。\n为了扩展我们的口头信息算法OM(m)，我们需要以下定义，如果两个将军被一条弧连接，就说它们是邻居。\nDefinition 1.\n(a) A set of nodes (il, \u0026hellip;, ip} is said to be a regular set of neighbors of a node if\n(i) each ij is a neighbor of i, and (ii) for any general k different from i, there exist paths yj,kfrom ijto k not passing through i such that any two different paths Yi,khave no node in common other than k.\n定义1.\n(a) 一组节点（il，\u0026hellip;，ip}被称为是一个节点的常规邻居集，如果\n(i) 每个ij都是i的邻居，并且\n(ii) 对于任何不同于i的k，存在从ij到k的不经过i的路径yj,k，使得任何两个不同的路径Yi,k除了k之外没有共同的节点。\n(b) The graph G is said to be p-regular if every node has a regular set of neighbors consisting of p distinct nodes.\n(b) 如果每个节点都有一个由p个不同节点组成的规则的邻居集，则称图G为p-规则。\nFigure 6 shows an example of a simple 3-regular graph. Figure 7 shows an example of a graph that is not 3-regular because the central node has no regular set of neighbors containing three nodes.\n图6是一个简单的3-规则图的例子。图7显示了一个非3规则图的例子，因为中心节点没有包含三个节点的规则邻居集。\nWe extend OM(m) to an algorithm that solves the Byzantine Generals Problem in the presence of m traitors if the graph G of generals is 3m-regular. (Note that a 3m-regular graph must contain at least 3m + 1 nodes.} For all positive integers m and p, we define the algorithm OM(m, p) as follows when the graph G of generals is p-regular. (OM(m,p) is not defined if G is not p-regular.) The definition uses induction on m.\n我们将OM(m)扩展为一种算法，如果图G是3m规则的，那么在有m个叛徒的情况下，可以解决拜占庭将军问题。(请注意，一个3m规则的图必须至少包含3m+1个节点)。对于所有正整数m和p，当将军图G是p-regular时，我们定义算法OM(m, p)如下。(如果G不是p-regular，OM(m,p)就没有定义) 。该定义使用了对m的归纳法。\nAlgorithm OM(m,p).\n(0) Choose a regular set N of neighbors of the commander consisting ofp lieutenants.\n(1) The commander sends his value to every lieutenant in N.\n(2) For each i in N, let vi be the value Lieutenant i receives from the commander, or else RETREAT if he receives no value. Lieutenant i sends vi to every other lieutenant k as follows:\n(A) If m = 1, then by sending the value along the path yi,k whose existence is guaranteed by part (a)(ii) of Definition 1.\n(B) If rn \u0026gt; 1, then by acting as the commander in the algorithm OM(m - 1, p - 1), with the graph of generals obtained by removing the original commander from G.\n(3) For each k, and each i in N with i ~ k, let vi be the value Lieutenant k received from Lieutenant i in step (2), or RETREAT if he received no value. Lieutenant k uses the value majority(vi\u0026hellip;\u0026hellip; vi,), where N = {il\u0026hellip;.. ip}.\n(0) 选择一个由p个中尉组成的指挥官的常规邻居集N。\n(1) 指挥官向N中的每个中尉发送他的命令。\n(2) 对于N中的每一个i，让vi成为i中尉从指挥官那里收到的命令，如果他没有收到任何价值，则RETREAT。中尉i将vi发送给其他每个中尉k，如下所示。\n(A) 如果m=1，那么通过沿路径yi,k发送命令，其存在由定义1的(a)(ii)部分保证。\n(B) 如果m\u0026gt;1，那么通过在算法OM(m - 1, p - 1)中充当指挥官，通过从G中删除原指挥官得到将军图。\n(3) 对于每个k，以及N中的每个i，i ~ k，让vi为k中尉在步骤(2)中从i中尉那里收到的命令，如果他没有收到命令，则RETREAT。中尉k使用价值 majority(vi\u0026hellip;\u0026hellip;vi)其中N = {i1\u0026hellip;.. ip}。\nNote that removing a single node from a p-regular graph leaves a (p - 1) regular graph. Hence, one can apply the algorithm OM(m - 1, p - 1) in step (2)(B).\n请注意，从一个p-regular图中移除一个节点，会留下一个（p - 1）regular图。因此，我们可以在步骤(2)(B)中应用算法OM(m - 1, p - 1)。\nWe now prove that OM(m, 3m) solves the Byzantine Generals Problem if there are at most m traitors. The proof is similar to the proof for the algorithm OM(m) and will just be sketched. It begins with the following extension of Lemma 1.\n我们现在证明OM(m, 3m)解决了拜占庭将军问题，如果有最多m个叛徒的话。该证明与OM(m)算法的证明类似，将只是略加说明。它从以下对结论1的扩展开始。\nLEMMA 2. For any m \u0026gt; 0 and any p \u0026gt; 2k + m, Algorithm OM (m, p) satisfies IC2 if there are at most k traitors.\nLEMMA 2. 对于任何m\u0026gt;0和任何p\u0026gt;2k+m，如果最多有k个叛徒，那么算法OM（m，p）满足IC2。\nPROOF. For m=1, observe that a lieutenant obtains the value majority(v1, \u0026hellip;, vp), where each vi is a value sent to him by the commander along a path disjoint from the path used to send the other values to him. Since there are at most k traitors and p = 2k + 1, more than half of those paths are composed entirely of loyal lieutenants. Hence, if the commander is loyal, then a majority of the values vi will equal the value he sent, which implies that IC2 is satisfied.\nNow assume the lemma for m - 1, m \u0026gt; 1. If the commander is loyal, then each of the p lieutenants in N gets the correct value. Since p \u0026gt; 2k, a majority of them are loyal, and by the induction hypothesis each of them sends the correct value to every loyal lieutenant. Hence, each loyal lieutenant gets a majority of correct values, thereby obtaining the correct value in step (3).\n证明。对于m=1，观察一下，一个中尉获得的值是 majority(v1, \u0026hellip;, vp)，其中每个vi是由指挥官沿着与用来发送其他值的路径不相交的路径发送给他的一个值。由于最多只有k个叛徒，p=2k+1，这些路径中有一半以上完全由忠诚的中尉组成。因此，如果指挥官是忠诚的，那么大多数的值将等于他发送的值，这意味着IC2被满足。\n现在假设m-1，m\u0026gt;1的定理。如果指挥官是忠诚的，那么N中的p个中尉都会得到正确的价值。由于p\u0026gt;2k，他们中的大多数人都是忠诚的，根据归纳假设，他们中的每一个人都向每个忠诚的中尉发送了正确的价值。因此，每个忠诚的中尉都得到了大多数的正确值，从而得到了步骤（3）中的正确值。\nThe correctness of Algorithm OM(m, 3m) is an immediate consequence of the following result.\n算法OM(m, 3m)的正确性是以下结果的直接结果。\nTHEOREM 3. For any m \u0026gt; 0 and any p \u0026gt; 3m, Algorithm OM(m, p) solves the Byzantine Generals Problem if there are at most m traitors.\nPROOF. By Lemma 2, letting k = m, we see that OM(m, p) satisfies IC2. If the commander is loyal, then IC1 follows from IC2, so we need only prove IC1 under the assumption that the commander is a traitor. To do this, we prove that every loyal lieutenant gets the same set of values vi in step (3). If m = 1, then this follows because all the lieutenants, including those in N, are loyal and the paths yi,k do not pass through the commander. For m \u0026gt; 1, a simple induction argument can be applied, since p-1\u0026gt;=3m implies that p-1\u0026gt;=3(m - 1).\n定理3. 对于任何m\u0026gt;0和任何p\u0026gt;3m，如果有最多m个叛徒，算法OM(m, p)就能解决拜占庭将军问题。\n证明。根据定理2，让k=m，我们看到OM(m, p)满足IC2。如果指挥官是忠诚的，那么IC1由IC2得出，所以我们只需要在指挥官是叛徒的假设下证明IC1。要做到这一点，我们要证明每个忠诚的中尉在步骤（3）中得到相同的价值集。如果m=1，那么这就意味着，所有的中尉，包括N中的中尉，都是忠诚的，而且路径yi,k不经过司令官。对于m\u0026gt;1，可以应用一个简单的归纳论证，因为p-1\u0026gt;=3m意味着p-1\u0026gt;=3（m - 1）。\nOur extension of Algorithm OM(m) requires that the graph G be 3m-regular, which is a rather strong connectivity hypothesis. 3 In fact, if there are only 3m + 1 generals (the minimum number required), then 3m-regularity means complete connectivity, and Algorithm OM(m, 3m) reduces to Algorithm OM(m). In contrast, Algorithm SM(m) is easily extended to allow the weakest possible connectivity hypothesis. Let us first consider how much connectivity is needed for the Byzantine Generals Problem to be solvable. IC2 requires that a loyal lieutenant obey a loyal commander. This is clearly impossible if the commander cannot communicate with the lieutenant. In particular, if every message from the commander to the lieutenant must be relayed by traitors, then there is no way to guarantee that the lieutenant gets the commander\u0026rsquo;s order. Similarly, IC1 cannot be guaranteed if there are two lieutenants who can only communicate with one another via traitorous intermediaries.\nThe weakest connectivity hypothesis for which the Byzantine Generals Problem is solvable is that the subgraph formed by the loyal generals be connected. We show that under this hypothesis, the algorithm SM(n - 2) is a solution, where n is the number of generals\u0026ndash;regardless of the number of traitors. Of course, we must modify the algorithm so that generals only send messages to where they can be sent. More precisely, in step (1), the commander sends his signed order only to his neighboring lieutenants; and in step (2)(B), Lieutenant i only sends the message to every neighboring lieutenant not among the jr.\n我们对算法OM(m)的扩展要求图G是3m-规则的，这是一个相当强的连接性假设。3 事实上，如果只有3m+1个将军（所需的最小数量），那么3m-规则性就意味着完全连通性，而算法OM(m, 3m)就简化为算法OM(m)。相比之下，算法SM(m)很容易扩展到允许最弱的连接性假设。让我们首先考虑拜占庭将军问题需要多少连通性才能被解决。IC2要求一个忠诚的中尉服从一个忠诚的指挥官。如果指挥官不能与中尉沟通，这显然是不可能的。特别是，如果从指挥官到中尉的每条信息都必须由叛徒转达，那么就没有办法保证中尉得到指挥官的命令。同样，如果有两个中尉只能通过叛徒的中间人相互沟通，那么IC1也无法保证。\n拜占庭将军问题可解的最弱连接性假设是，由忠诚的将军们形成的子图是连接的。我们表明，在这个假设下，算法SM(n - 2)是一个解决方案，其中n是将军的数量\u0026ndash;无论叛徒的数量如何。当然，我们必须修改算法，使将军们只把信息发送到可以发送的地方。更确切地说，在步骤(1)中，指挥官只将他签署的命令发送给他邻近的中尉；而在步骤(2)(B)中，中尉i只将信息发送给每一个不在jr的邻近中尉。\nWe prove the following more general result, where the diameter of a graph is the smallest number d such that any two nodes are connected by a path containing at most d arcs.\n我们证明以下更一般的结果，图的直径是最小的数字d，即任何两个节点都由最多包含d个弧的路径连接。\nTHEOREM 4. For any m and d, if there are at most m traitors and the subgraph of loyal generals has diameter d, then Algorithm SM(m + d - 1) (with the above modification) solves the Byzantine Generals Problem.\n定理4. 对于任何m和d，如果最多存在m个叛徒，并且忠诚将军的子图的直径为d，那么算法SM(m + d - 1)（经过上述修改）可以解决拜占庭将军问题。\nPROOF. The proof is quite similar to that of Theorem 2 and is just sketched here. To prove IC2, observe that by hypothesis there is a path from the loyal commander to a lieutenant i going through d - 1 or fewer loyal lieutenants. Those lieutenants will correctly relay the order until it reaches i. As before, assumption A4 prevents a traitor from forging a different order.\nTo prove IC1, we assume the commander is a traitor and must show that any order received by a loyal lieutenant i is also received by a loyal lieutenant j. Suppose i receives an order v:0:j1:···:jk not signed by j. If k \u0026lt; m, then i will send it to every neighbor who has not already received that order, and it will be relayed to j within d - 1 more steps. If k \u0026gt; m, then one of the first m signers must be loyal and must have sent it to all of his neighbors, whereupon it will be relayed by loyal generals and will reach j within d - 1 steps.\n证明。该证明与定理2非常相似，在此仅作简要说明。为了证明IC2，请注意，根据假设，有一条从忠诚的指挥官到中尉i的路径要经过d-1或更少的忠诚中尉。这些中尉将正确地转达命令，直到它到达i。如前所述，假设A4防止叛徒伪造不同的命令。\n为了证明IC1，我们假设指挥官是个叛徒，并且必须证明忠心的中尉i收到的任何命令也会被忠心的中尉j收到。假设i收到一个没有j签名的命令v:0:j1:\u0026ndash;:jk。如果k\u0026gt;m，那么前m个签名者中的一个一定是忠诚的，而且一定是把它发送给了他的所有邻居，这时它将被忠诚的将军们转发，并在d-1步内到达j。\nCOROLLARY. If the graph of loyal generals is connected, then SM(n - 2) (as modified above) solves the Byzantine Generals Problem for n generals.\n推论。如果忠诚将军的图是连接的，那么SM(n - 2)(如上文所修改)解决了n个将军的拜占庭将军问题。\nPROOF. Let d be the diameter of the graph of loyal generals. Since the diameter of a connected graph is less than the number of nodes, there must be more than d loyal generals and fewer than n - d traitors. The result follows from the theorem by letting m=n - d - 1.\n证明。设d为忠诚将军图的直径。由于连通图的直径小于节点数，所以忠诚的将军一定多于d，而叛徒一定少于n-d。由定理可知，结果是让m=n - d - 1。\nTheorem 4 assumes that the subgraph of loyal generals is connected. Its proof is easily extended to show that even if this is not the case, if there are at most m traitors, then the algorithm SM(m + d - 1) has the following two properties:\nAny two loyal generals connected by a path of length at most d passing through only loyal generals will obey the same order. If the commander is loyal, then any loyal lieutenant connected to him by a path of length at most m + d passing only through loyal generals will obey his order. 定理4 假设忠诚将军的子图是连接的。它的证明很容易扩展到表明，即使不是这样，如果有最多m个叛徒，那么算法SM(m + d - 1)有以下两个特性。\n任何两个忠诚的将军被一条最多通过忠诚将军的长度为d的路径所连接，都将服从相同的命令。 如果指挥官是忠诚的，那么任何与他相连的忠诚的中尉，通过一条长度最多为m+d的路径，只经过忠诚的将军，都会服从他的命令。 ","date":"2022-07-11T00:00:00Z","permalink":"https://MyLoveES.github.io/p/%E6%8B%9C%E5%8D%A0%E5%BA%AD%E5%B0%86%E5%86%9B-%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/","title":"拜占庭将军 - 论文翻译"},{"content":"(没有查到根本原因)\n背景 服务运行中，发现服务器频繁不定时重启。发现是kernel crash造成的。\n查看 /var/crash/ 下的log，发现\nBUG: unable to handle kernel paging request at xxxxxxx\n分析vmcore 环境 crash工具 崩溃转储文件(vmcore) 发生崩溃的内核映像文件(vmlinux)，包含调试内核所需调试信息 需要安装：kernel-debuginfo kernel-debuginfo-common\n1 2 3 4 yum install crash wget http://debuginfo.centos.org/7/x86_64/kernel-debuginfo-common-x86_64-`uname -r`.rpm wget http://debuginfo.centos.org/7/x86_64/kernel-debuginfo-`uname -r`.rpm rpm -ivh *.rpm 安装完成后，在/lib/debug/lib/modules找到vmlinux内核映像文件\nvmcore-dmesg.txt vmcore 1 crash /lib/debug/lib/modules/3.10.0-957.el7.x86_64/vmlinux /var/crash/127.0.0.1-2020-04-04-14\\:10\\:45/vmcore 执行 bt\n执行 sym\nnvidia-smi 查看显卡信息 nvidia-smi gpu fan 显示 error\nnvidia-smi -q\n","date":"2022-07-10T00:00:00Z","permalink":"https://MyLoveES.github.io/p/linux-nvidia-%E9%80%A0%E6%88%90-kernel-crash/","title":"Linux - NVIDIA 造成 kernel crash"},{"content":"","date":"2022-05-06T00:00:00Z","permalink":"https://MyLoveES.github.io/p/callback-or-messagequeue/","title":"Callback Or MessageQueue"},{"content":" 1 pandoc --pdf-engine=xelatex --highlight-style zenburn --toc -V geometry:margin=1in -V urlcolor=NavyBlue -V CJKmainfont=\u0026#34;STKaitiSC-Regular\u0026#34; ${input} ${output} ","date":"2022-05-06T00:00:00Z","permalink":"https://MyLoveES.github.io/p/pandoc-mardown-to-pdf/","title":"Pandoc mardown to pdf"},{"content":"ssh-keygen -t rsa ","date":"2022-05-06T00:00:00Z","permalink":"https://MyLoveES.github.io/p/ssh-key-%E7%94%9F%E6%88%90/","title":"SSH key 生成"},{"content":"只列出了我常用的操作\nsurround 操作 快捷键 效果 插入 ysiw' abc -\u0026gt; \u0026lsquo;abc\u0026rsquo; ysiwt abc -\u0026gt; abc ys3w) print a,b -\u0026gt; print (a,b) ys$) print a,b -\u0026gt; print (a,b) vwwS\u0026quot; print a,b -\u0026gt; print \u0026ldquo;a,b\u0026rdquo; 替换 cs\u0026quot;' \u0026ldquo;abc\u0026rdquo; -\u0026gt; \u0026lsquo;abc\u0026rsquo; csw' abc def! -\u0026gt; abc \u0026lsquo;def\u0026rsquo;! csW' abc def! -\u0026gt; abc \u0026lsquo;def!\u0026rsquo; cs)] (abc) -\u0026gt; [abc] cs){ (abc) -\u0026gt; { abc } 删除 ds' \u0026lsquo;abc\u0026rsquo; -\u0026gt; abc ds( (abc)def -\u0026gt; abcdef dst abc -\u0026gt; abc commentary 操作 快捷键 效果 注释 gcc abc -\u0026gt; //abc gcap abc -\u0026gt; //abc 取消注释 gcu //abc -\u0026gt; abc argtextobj 操作 快捷键 效果 删除 daa function(arg1, arg2) -\u0026gt; function(arg1) 更改 cia function(arg1, arg2) -\u0026gt; function(arg1, ) cia function(arg1, arg2, arg3) -\u0026gt; function(arg1, arg2, arg3) cia function(arg1, func(a1, a2)) -\u0026gt; function(arg1, ) exchange 操作 快捷键 效果 替换 cx{motion} abcdef123 -\u0026gt; abc123def 替换line cxx abc -\u0026gt; 123 visual mode X cx 清除缓存 cxc textobj-entire 操作 快捷键 效果 删除 dae/die easymotion 操作 快捷键 效果 搜索 f 搜索 搜索 s 搜索 replace with register 操作 快捷键 效果 粘贴 gr{motion} abcdef -\u0026gt; abcabc NERDTree 操作 快捷键 效果 Tree打开关闭 ctrl+n 光标在目录树与文件间切换 ctrl+w+w 切换到前一个tab g+T 切换到后一个tab g+t 在新 Tab 中打开选中文件/书签，并跳到新 Tab t 关闭当前的 tab :tabc 关闭所有其他的 tab :tabo paragraph-motion 操作 快捷键 效果 shift + {/} 段落移动 即使空格行也算 ","date":"2022-04-16T00:00:00Z","permalink":"https://MyLoveES.github.io/p/vim-%E6%8F%92%E4%BB%B6/","title":"Vim 插件"},{"content":".vimrc 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 if filereadable(expand(\u0026#34;~/.vimrc.bundles\u0026#34;)) source ~/.vimrc.bundles endif syntax enable \u0026#34;set background=light \u0026#34;colorscheme solarized let mapleader=\u0026#34;,\u0026#34; \u0026#34;\u0026#34;\u0026#34; Plugins -------------------------------- \u0026#34; ys, cs, ds, S \u0026#34; set surround \u0026#34; gcc, gc + motion, v_gc \u0026#34; set commentary \u0026#34; argument text objects: aa, ia \u0026#34; set argtextobj \u0026#34; cx{motion} to select, again to exchange \u0026#34; set exchange \u0026#34; entire buffer text object: ae \u0026#34; set textobj-entire \u0026#34; easymotion \u0026lt;Leader\u0026gt; s / f \u0026#34; set easymotion \u0026#34; 寄存器替换 \u0026#34; set ReplaceWithRegister \u0026#34; 文件树展示 \u0026#34; set NERDTree \u0026#34; 复制时高亮内容 \u0026#34; set highlightedyank \u0026#34; 空格行也能够跳转 \u0026#34; set vim-paragraph-motion \u0026#34; autocmd VimEnter * NERDTree \u0026#34;\u0026#34;\u0026#34; Common settings ------------------------- \u0026#34; 显示当前mode set showmode \u0026#34; 光标移动时保留5行 set so=5 \u0026#34; 实时查找 set incsearch \u0026#34; 显示行号 \u0026#34; set nu \u0026#34; 忽略大小写 set ignorecase \u0026#34; 允许光标到行末 set virtualedit=onemore \u0026#34; 搜索内容高亮显示 \u0026#34; set hlsearch set tabstop=4 set shiftwidth=4 set expandtab \u0026#34;\u0026#34;\u0026#34; Plugin settings ------------------------- let g:argtextobj_pairs=\u0026#34;[:],(:),\u0026lt;:\u0026gt;\u0026#34; let g:highlightedyank_highlight_duration = \u0026#34;1000\u0026#34; autocmd StdinReadPre * let s:std_in=1 autocmd VimEnter * if argc() == 1 \u0026amp;\u0026amp; isdirectory(argv()[0]) \u0026amp;\u0026amp; !exists(\u0026#34;s:std_in\u0026#34;) | exe \u0026#39;NERDTree\u0026#39; argv()[0] | wincmd p | ene | endif autocmd bufenter * if (winnr(\u0026#34;$\u0026#34;) == 1 \u0026amp;\u0026amp; exists(\u0026#34;b:NERDTree\u0026#34;) \u0026amp;\u0026amp; b:NERDTree.isTabTree()) | q | endif \u0026#34;\u0026#34;\u0026#34; My Mappings ----------------------------- map \u0026lt;leader\u0026gt;f \u0026lt;Plug\u0026gt;(easymotion-sn) map \u0026lt;leader\u0026gt;e \u0026lt;Plug\u0026gt;(easymotion-fn) map \u0026lt;C-n\u0026gt; :NERDTreeToggle\u0026lt;CR\u0026gt; .vimrc.bundles 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 set nocompatible \u0026#34; be iMproved, required filetype off \u0026#34; required \u0026#34; set the runtime path to include Vundle and initialize set rtp+=~/.vim/bundle/Vundle.vim call vundle#begin() \u0026#34; alternatively, pass a path where Vundle should install plugins \u0026#34;call vundle#begin(\u0026#39;~/some/path/here\u0026#39;) \u0026#34; let Vundle manage Vundle, required Plugin \u0026#39;VundleVim/Vundle.vim\u0026#39; Plugin \u0026#39;tpope/vim-surround\u0026#39; Plugin \u0026#39;tpope/vim-commentary\u0026#39; Plugin \u0026#39;vim-scripts/argtextobj.vim\u0026#39; Plugin \u0026#39;tommcdo/vim-exchange\u0026#39; Plugin \u0026#39;kana/vim-textobj-user\u0026#39; Plugin \u0026#39;kana/vim-textobj-entire\u0026#39; Plugin \u0026#39;easymotion/vim-easymotion\u0026#39; Plugin \u0026#39;vim-scripts/ReplaceWithRegister\u0026#39; Plugin \u0026#39;preservim/nerdtree\u0026#39; Plugin \u0026#39;machakann/vim-highlightedyank\u0026#39; Plugin \u0026#39;dbakker/vim-paragraph-motion\u0026#39; call vundle#end() \u0026#34; required filetype plugin indent on \u0026#34; required \u0026#34; Brief help \u0026#34; :PluginList - lists configured plugins \u0026#34; :PluginInstall - installs plugins; append `!` to update or just :PluginUpdate \u0026#34; :PluginSearch foo - searches for foo; append `!` to refresh local cache \u0026#34; :PluginClean - confirms removal of unused plugins; append `!` to auto-approve removal .ideavimrc 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 \u0026#34;\u0026#34; Source your .vimrc source ~/.vimrc \u0026#34;\u0026#34; -- Suggested options -- \u0026#34; Show a few lines of context around the cursor. Note that this makes the \u0026#34; text scroll if you mouse-click near the start or end of the window. set scrolloff=5 \u0026#34; Do incremental searching. set incsearch \u0026#34; Don\u0026#39;t use Ex mode, use Q for formatting. map Q gq \u0026#34;\u0026#34; -- Map IDE actions to IdeaVim -- https://jb.gg/abva4t \u0026#34;\u0026#34; Map \\r to the Reformat Code action \u0026#34;map \\r \u0026lt;Action\u0026gt;(ReformatCode) \u0026#34;\u0026#34; Map \u0026lt;leader\u0026gt;d to start debug \u0026#34;map \u0026lt;leader\u0026gt;d \u0026lt;Action\u0026gt;(Debug) \u0026#34;\u0026#34; Map \\b to toggle the breakpoint on the current line \u0026#34;map \\b \u0026lt;Action\u0026gt;(ToggleLineBreakpoint) let mapleader=\u0026#39;,\u0026#39; \u0026#34; Find more examples here: https://jb.gg/share-ideavimrc \u0026#34;\u0026#34;\u0026#34; Plugins -------------------------------- \u0026#34; ys, cs, ds, S set surround \u0026#34; gcc, gc + motion, v_gc set commentary \u0026#34; argument text objects: aa, ia set argtextobj \u0026#34; cx{motion} to select, again to exchange set exchange \u0026#34; entire buffer text object: ae set textobj-entire \u0026#34; easymotion \u0026lt;Leader\u0026gt; s / f set easymotion \u0026#34; 寄存器替换 set ReplaceWithRegister \u0026#34; 文件树展示 set NERDTree \u0026#34; 复制时高亮内容 set highlightedyank \u0026#34; 空格行也能够跳转 set vim-paragraph-motion \u0026#34;\u0026#34;\u0026#34; Common settings ------------------------- \u0026#34; 显示当前mode set showmode \u0026#34; 光标移动时保留5行 set so=5 \u0026#34; 实时查找 set incsearch \u0026#34; 显示行号 set nu \u0026#34; 忽略大小写 set ignorecase \u0026#34; 允许光标到行末 set virtualedit=onemore \u0026#34; 搜索内容高亮显示 set hlsearch \u0026#34;\u0026#34;\u0026#34; Idea specific settings ------------------ \u0026#34; 多行合并 J set ideajoin \u0026#34; icon展示 set ideastatusicon=gray \u0026#34; 在normal mode默认eng set keep-english-in-normal-and-restore-in-insert \u0026#34;\u0026#34;\u0026#34; Plugin settings ------------------------- let g:argtextobj_pairs=\u0026#34;[:],(:),\u0026lt;:\u0026gt;\u0026#34; let g:highlightedyank_highlight_duration = \u0026#34;1000\u0026#34; \u0026#34;\u0026#34;\u0026#34; My Mappings ----------------------------- map \u0026lt;leader\u0026gt;f \u0026lt;Plug\u0026gt;(easymotion-s) map \u0026lt;leader\u0026gt;e \u0026lt;Plug\u0026gt;(easymotion-f) map \u0026lt;leader\u0026gt;d \u0026lt;Action\u0026gt;(Debug) map \u0026lt;leader\u0026gt;r \u0026lt;Action\u0026gt;(RenameElement) map \u0026lt;leader\u0026gt;c \u0026lt;Action\u0026gt;(Stop) map \u0026lt;leader\u0026gt;z \u0026lt;Action\u0026gt;(ToggleDistractionFreeMode) map \u0026lt;leader\u0026gt;s \u0026lt;Action\u0026gt;(SelectInProjectView) map \u0026lt;leader\u0026gt;a \u0026lt;Action\u0026gt;(Annotate) map \u0026lt;leader\u0026gt;h \u0026lt;Action\u0026gt;(Vcs.ShowTabbedFileHistory) map \u0026lt;S-Space\u0026gt; \u0026lt;Action\u0026gt;(GotoNextError) map \u0026lt;leader\u0026gt;= \u0026lt;Action\u0026gt;(ReformatCode) nnoremap \u0026lt;Tab\u0026gt; \u0026gt;\u0026gt;_ nnoremap \u0026lt;S-Tab\u0026gt; \u0026lt;\u0026lt;_ inoremap \u0026lt;S-Tab\u0026gt; \u0026lt;C-D\u0026gt; vnoremap \u0026lt;Tab\u0026gt; \u0026gt;gv vnoremap \u0026lt;S-Tab\u0026gt; \u0026lt;gv set ideastrictmode ","date":"2022-04-16T00:00:00Z","permalink":"https://MyLoveES.github.io/p/vim-%E9%85%8D%E7%BD%AE/","title":"Vim 配置"},{"content":"Name ack - grep-like text finder 类grep的文本搜索\nSYNOPSIS ack [options] PATTERN [FILE\u0026hellip;] // 查文本 ack -f [options] [DIRECTORY\u0026hellip;] // 搜文件 DESCRIPTION ack 被设计为程序员的 grep 的替代品。\nack 在输入文件或目录中搜索与给定模式匹配的行。默认情况下， ack 打印匹配的行。如果没有给出 FILE 或 DIRECTORY，则将搜索当前目录。\nAck 还可以列出将要搜索的文件，而无需实际搜索它们，以让您利用 ack 的文件类型过滤功能。\nFILE SELECTION 如果没有指定文件进行搜索，无论是在命令行上还是使用 -x 选项通过管道输入，ack 都会深入到子目录中选择文件进行搜索。\n-x Read the list of files to search from STDIN.\nack 对它搜索的文件很智能。它基于文件的扩展名以及在某些情况下文件的内容了解某些文件类型。可以使用 \u0026ndash;type 选项进行这些选择。\nexample: ack \u0026ndash;type=python\n在没有文件选择的情况下，ack 搜索没有被 \u0026ndash;ignore-dir 和 \u0026ndash;ignore-file 选项明确排除的常规文件，这些文件要么存在于 ackrc 文件中，要么存在于命令行中。\nack 的默认选项忽略某些文件和目录。这些包括：\n备份文件：匹配 #*# 或以 ~ 结尾的文件。 Coredumps：与核心匹配的文件。\\d+ 版本控制目录，如 .svn 和 .git。 使用 \u0026ndash;dump 选项运行 ack 以查看设置了哪些设置。\n但是，ack 总是搜索命令行中给出的文件，不管是什么类型。如果你告诉 ack 在 coredump 中搜索，它会在 coredump 中搜索。\nDIRECTORY SELECTION Ack通过指定的起始目录的目录树递归。如果没有指定目录，则使用当前工作目录。但是，它将忽略许多版本控制系统使用的影子目录，以及Perl MakeMaker系统使用的构建目录。你可以使用——[no]ignore-dir选项从这个列表中添加或删除一个目录。该选项可以重复添加/删除忽略列表中的多个目录。\n有关未搜索的目录的完整列表，请运行 ack \u0026ndash;dump。\nMATCHING IN A RANGE OF LINES \u0026ndash;range-start 和 \u0026ndash;range-end 选项允许您指定要在每个文件中搜索的行范围。\n假设您有以下文件，称为 testfile：\n1 2 3 4 5 6 7 8 9 # This function calls print on \u0026#34;foo\u0026#34;. sub foo { print \u0026#39;foo\u0026#39;; } my $print = 1; sub bar { print \u0026#39;bar\u0026#39;; } my $task = \u0026#39;print\u0026#39;; 调用 ack print 会给我们五个匹配项：\n1 2 3 4 5 6 $ ack print testfile # This function calls print on \u0026#34;foo\u0026#34;. print \u0026#39;foo\u0026#39;; my $print = 1; print \u0026#39;bar\u0026#39;; my $task = \u0026#39;print\u0026#39;;\\ 如果我们只想在子程序中搜索 print 怎么办？我们可以指定要 ack 搜索的行的范围。范围以匹配模式 ^sub \\w+ 的行开始，并以匹配 ^} 的行结束。\n1 2 3 $ ack --range-start=\u0026#39;^sub \\w+\u0026#39; --range-end=\u0026#39;^}\u0026#39; print testfile print \u0026#39;foo\u0026#39;; print \u0026#39;bar\u0026#39;; 请注意， ack 搜索了两个范围的行。下面的清单显示了哪些行在范围内，哪些行不在范围内。\n1 2 3 4 5 6 7 8 9 Out # This function calls print on \u0026#34;foo\u0026#34;. In sub foo { In print \u0026#39;foo\u0026#39;; In } Out my $print = 1; In sub bar { In print \u0026#39;bar\u0026#39;; In } Out my $task = \u0026#39;print\u0026#39;; 不必同时指定 \u0026ndash;range-start 和 \u0026ndash;range-end。 如果省略\u0026ndash;range-start，则范围从文件中的第一行开始，即匹配\u0026ndash;range-end的第一行。类似地，如果省略 \u0026ndash;range-end，则范围从匹配 \u0026ndash;range-start 的第一行到文件末尾。 例如，如果你想搜索所有 HTML 文件直到 的第一个实例，你可以这样做\n1 ack foo --range-end=\u0026#39;\u0026lt;body\u0026gt;\u0026#39; 或者搜索 Perl 的 __DATA__ 或 __END__ 标记，你可以这样做\n1 ack pattern --range-end=\u0026#39;^__(END|DATA)__\u0026#39; 范围可以在同一行开始和停止。例如:\n1 --range-start=\u0026#39;\u0026lt;title\u0026gt;\u0026#39; --range-end=\u0026#39;\u0026lt;/title\u0026gt;\u0026#39; 将匹配该行作为范围的开始和结束，形成单行范围。\n1 \u0026lt;title\u0026gt;Page title\u0026lt;/title\u0026gt; 请注意，\u0026ndash;range-start 和 \u0026ndash;range-end 中的模式不受 -i、-w 和 -Q 等选项的影响，这些选项会修改被匹配的主模式的行为。\n同样，范围仅影响查找匹配项的位置。 ack 中的其他所有内容都以相同的方式工作。对范围使用 -c 选项将计算出现在这些范围内的所有匹配项。 -l 显示在某个范围内匹配的文件，-L 选项显示在某个范围内没有匹配的文件。\n用于否定匹配的 -v 选项也适用于该范围内。要在 HTML 文件的“”部分中查看与“google”不匹配的行，您可以执行以下操作：\n1 ack google -v --html --range-start=\u0026#39;\u0026lt;head\u0026#39; --range-end=\u0026#39;\u0026lt;/head\u0026gt;\u0026#39; 指定要搜索的范围不会影响匹配项的显示方式。匹配的上下文仍然是相同的，并且使用上下文选项的工作方式相同，即使上下文行超出范围，也会显示匹配的上下文行。同样， \u0026ndash;passthru 将显示文件中的所有行，但仅显示范围内的行的匹配项。\nOPTIONS `### \u0026ndash;ackrc 指定要在所有其他文件之后加载的 ackrc 文件；请参阅 \u0026ldquo;ACKRC LOCATION SEMANTICS\u0026rdquo;.\n-A NUM, \u0026ndash;after-context=NUM 在匹配行之后打印 NUM 行。\n-B NUM, \u0026ndash;before-context=NUM 在匹配行之前打印 NUM 行。\n\u0026ndash;[no]break 打印来自不同文件的结果之间的中断。交互使用时默认开启。\n-C [NUM], \u0026ndash;context[=NUM] 在匹配行周围打印 NUM 行（默认 2）上下文。您可以指定零行上下文来覆盖 ackrc 中指定的另一个上下文。\n-c, \u0026ndash;count 抑制正常输出； 而是打印每个输入文件的匹配行数。 如果 -l 生效，它只会显示每个文件的行数匹配的行数。 如果没有 -l，某些行数可能为零。 如果结合 -h (\u0026ndash;no-filename) ack 只输出一个总数。\n\u0026ndash;[no]color, \u0026ndash;[no]colour \u0026ndash;color 突出显示匹配的文本。 \u0026ndash;nocolor 抑制颜色。 除非输出被重定向，否则默认情况下这是打开的。 在 Windows 上，此选项默认关闭，除非安装了 Win32::Console::ANSI 模块或使用了 ACK_PAGER_COLOR 环境变量。\n\u0026ndash;color-filename=color 设置用于文件名的颜色。\n\u0026ndash;color-match=color 设置用于匹配的颜色。\n\u0026ndash;color-colno=color 设置用于列号的颜色。\n\u0026ndash;color-lineno=color 设置用于行号的颜色。\n\u0026ndash;[no]column 显示第一个匹配的列号。这对于可以将光标放在给定位置的编辑器很有帮助。\n\u0026ndash;create-ackrc 将默认 ack 选项转储到标准输出。当您想要自定义默认值时，这很有用。\n\u0026ndash;dump 将加载的选项列表及其来源写入标准输出。方便调试。\n\u0026ndash;[no]env \u0026ndash;noenv 禁用所有环境处理。不读取 .ackrc 并忽略所有环境变量。默认情况下，ack 会考虑环境中的 .ackrc 和设置。\n\u0026ndash;flush \u0026ndash;flush 立即刷新输出。默认情况下这是关闭的，除非 ack 以交互方式运行（当输出到管道或文件时）。\n-f 打印将要搜索的文件，而不实际进行任何搜索。不得指定 PATTERN，否则将被视为搜索路径。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 --ackrc Specifies an ackrc file to load after all others; see \u0026#34;ACKRC LOCATION SEMANTICS\u0026#34;. -A NUM, --after-context=NUM Print NUM lines of trailing context after matching lines. -B NUM, --before-context=NUM Print NUM lines of leading context before matching lines. --[no]break Print a break between results from different files. On by default when used interactively. -C [NUM], --context[=NUM] Print NUM lines (default 2) of context around matching lines. You can specify zero lines of context to override another context specified in an ackrc. -c, --count Suppress normal output; instead print a count of matching lines for each input file. If -l is in effect, it will only show the number of lines for each file that has lines matching. Without -l, some line counts may be zeroes. If combined with -h (--no-filename) ack outputs only one total count. --[no]color, --[no]colour --color highlights the matching text. --nocolor suppresses the color. This is on by default unless the output is redirected. On Windows, this option is off by default unless the Win32::Console::ANSI module is installed or the \u0026#34;ACK_PAGER_COLOR\u0026#34; environment variable is used. --color-filename=color Sets the color to be used for filenames. --color-match=color Sets the color to be used for matches. --color-colno=color Sets the color to be used for column numbers. --color-lineno=color Sets the color to be used for line numbers. --[no]column Show the column number of the first match. This is helpful for editors that can place your cursor at a given position. --create-ackrc Dumps the default ack options to standard output. This is useful for when you want to customize the defaults. --dump Writes the list of options loaded and where they came from to standard output. Handy for debugging. --[no]env --noenv disables all environment processing. No .ackrc is read and all environment variables are ignored. By default, ack considers .ackrc and settings in the environment. --flush --flush flushes output immediately. This is off by default unless ack is running interactively (when output goes to a pipe or file). -f Only print the files that would be searched, without actually doing any searching. PATTERN must not be specified, or it will be taken as a path to search. --files-from=FILE The list of files to be searched is specified in FILE. The list of files are separated by newlines. If FILE is \u0026#34;-\u0026#34;, the list is loaded from standard input. Note that the list of files is not filtered in any way. If you add \u0026#34;--type=html\u0026#34; in addition to \u0026#34;--files-from\u0026#34;, the \u0026#34;--type\u0026#34; will be ignored. --[no]filter Forces ack to act as if it were receiving input via a pipe. --[no]follow Follow or don\u0026#39;t follow symlinks, other than whatever starting files or directories were specified on the command line. This is off by default. -g PATTERN Print searchable files where the relative path + filename matches PATTERN. Note that ack -g foo is exactly the same as ack -f | ack foo This means that just as ack will not search, for example, .jpg files, \u0026#34;-g\u0026#34; will not list .jpg files either. ack is not intended to be a general-purpose file finder. Note also that if you have \u0026#34;-i\u0026#34; in your .ackrc that the filenames to be matched will be case-insensitive as well. This option can be combined with --color to make it easier to spot the match. --[no]group --group groups matches by file name. This is the default when used interactively. --nogroup prints one result per line, like grep. This is the default when output is redirected. -H, --with-filename Print the filename for each match. This is the default unless searching a single explicitly specified file. -h, --no-filename Suppress the prefixing of filenames on output when multiple files are searched. --[no]heading Print a filename heading above each file\u0026#39;s results. This is the default when used interactively. --help Print a short help statement. --help-types Print all known types. --help-colors Print a chart of various color combinations. --help-rgb-colors Like --help-colors but with more precise RGB colors. -i, --ignore-case Ignore case distinctions in PATTERN. Overrides --smart-case and -I. -I, --no-ignore-case Turns on case distinctions in PATTERN. Overrides --smart-case and -i. --ignore-ack-defaults Tells ack to completely ignore the default definitions provided with ack. This is useful in combination with --create-ackrc if you really want to customize ack. --[no]ignore-dir=DIRNAME, --[no]ignore-directory=DIRNAME Ignore directory (as CVS, .svn, etc are ignored). May be used multiple times to ignore multiple directories. For example, mason users may wish to include --ignore-dir=data. The --noignore-dir option allows users to search directories which would normally be ignored (perhaps to research the contents of .svn/props directories). The DIRNAME must always be a simple directory name. Nested directories like foo/bar are NOT supported. You would need to specify --ignore-dir=foo and then no files from any foo directory are taken into account by ack unless given explicitly on the command line. --ignore-file=FILTER:ARGS Ignore files matching FILTER:ARGS. The filters are specified identically to file type filters as seen in \u0026#34;Defining your own types\u0026#34;. -k, --known-types Limit selected files to those with types that ack knows about. -l, --files-with-matches Only print the filenames of matching files, instead of the matching text. -L, --files-without-matches Only print the filenames of files that do NOT match. --match PATTERN Specify the PATTERN explicitly. This is helpful if you don\u0026#39;t want to put the regex as your first argument, e.g. when executing multiple searches over the same set of files. # search for foo and bar in given files ack file1 t/file* --match foo ack file1 t/file* --match bar -m=NUM, --max-count=NUM Print only NUM matches out of each file. If you want to stop ack after printing the first match of any kind, use the -1 options. --man Print this manual page. -n, --no-recurse No descending into subdirectories. -o Show only the part of each line matching PATTERN (turns off text highlighting). This is exactly the same as \u0026#34;--output=$\u0026amp;\u0026#34;. --output=expr Output the evaluation of expr for each line (turns off text highlighting). If PATTERN matches more than once then a line is output for each non-overlapping match. expr may contain the strings \u0026#34;\\n\u0026#34;, \u0026#34;\\r\u0026#34; and \u0026#34;\\t\u0026#34;, which will be expanded to their corresponding characters line feed, carriage return and tab, respectively. expr may also contain the following Perl special variables: $1 through $9 The subpattern from the corresponding set of capturing parentheses. If your pattern is \u0026#34;(.+) and (.+)\u0026#34;, and the string is \u0026#34;this and that\u0026#39;, then $1 is \u0026#34;this\u0026#34; and $2 is \u0026#34;that\u0026#34;. $_ The contents of the line in the file. $. The number of the line in the file. $\u0026amp;, \u0026#34;$`\u0026#34; and \u0026#34;$\u0026#39;\u0026#34; $\u0026amp; is the the string matched by the pattern, \u0026#34;$`\u0026#34; is what precedes the match, and \u0026#34;$\u0026#39;\u0026#34; is what follows it. If the pattern is \u0026#34;gra(ph|nd)\u0026#34; and the string is \u0026#34;lexicographic\u0026#34;, then $\u0026amp; is \u0026#34;graph\u0026#34;, \u0026#34;$`\u0026#34; is \u0026#34;lexico\u0026#34; and \u0026#34;$\u0026#39;\u0026#34; is \u0026#34;ic\u0026#34;. Use of these variables in your output will slow down the pattern matching.rocess Scheduling In Linux $+ The match made by the last parentheses that matched in the pattern. For example, if your pattern is \u0026#34;Version: (.+)|Revision: (.+)\u0026#34;, then $+ will contain whichever set of parentheses matched. $f $f is available, in \u0026#34;--output\u0026#34; only, to insert the filename. This is a stand- in for the discovered $filename usage in old \u0026#34;ack2 --output\u0026#34;, which is disallowed with \u0026#34;ack3\u0026#34; improved security. The intended usage is to provide the grep or compile-error syntax needed for editor/IDE go-to-line integration, e.g. \u0026#34;--output=$f:$.:$_\u0026#34; or \u0026#34;--output=$f\\t$.\\t$\u0026amp;\u0026#34; --pager=program, --nopager --pager directs ack\u0026#39;s output through program. This can also be specified via the \u0026#34;ACK_PAGER\u0026#34; and \u0026#34;ACK_PAGER_COLOR\u0026#34; environment variables. Using --pager does not suppress grouping and coloring like piping output on the command-line does. --nopager cancels any setting in ~/.ackrc, \u0026#34;ACK_PAGER\u0026#34; or \u0026#34;ACK_PAGER_COLOR\u0026#34;. No output will be sent through a pager. --passthru Prints all lines, whether or not they match the expression. Highlighting will still work, though, so it can be used to highlight matches while still seeing the entire file, as in: # Watch a log file, and highlight a certain IP address. $ tail -f ~/access.log | ack --passthru 123.45.67.89 --print0 Only works in conjunction with -f, -g, -l or -c, options that only list filenames. The filenames are output separated with a null byte instead of the usual newline. This is helpful when dealing with filenames that contain whitespace, e.g. # Remove all files of type HTML. ack -f --html --print0 | xargs -0 rm -f -p[N], --proximate[=N] Groups together match lines that are within N lines of each other. This is useful for visually picking out matches that appear close to other matches. For example, if you got these results without the \u0026#34;--proximate\u0026#34; option, 15: First match 18: Second match 19: Third match 37: Fourth match they would look like this with \u0026#34;--proximate=1\u0026#34; 15: First match 18: Second match 19: Third match 37: Fourth match and this with \u0026#34;--proximate=3\u0026#34;. 15: First match 18: Second match 19: Third match 37: Fourth match If N is omitted, N is set to 1. -P Negates the effect of the --proximate option. Shortcut for --proximate=0. -Q, --literal Quote all metacharacters in PATTERN, it is treated as a literal. -r, -R, --recurse Recurse into sub-directories. This is the default and just here for compatibility with grep. You can also use it for turning --no-recurse off. --range-start=PATTERN, --range-end=PATTERN Specifies patterns that mark the start and end of a range. See \u0026#34;MATCHING IN A RANGE OF LINES\u0026#34; for details. -s Suppress error messages about nonexistent or unreadable files. This is taken from fgrep. -S, --[no]smart-case, --no-smart-case Ignore case in the search strings if PATTERN contains no uppercase characters. This is similar to \u0026#34;smartcase\u0026#34; in the vim text editor. The options overrides -i and -I. -S is a synonym for --smart-case. -i always overrides this option. --sort-files Sorts the found files lexicographically. Use this if you want your file listings to be deterministic between runs of ack. --show-types Outputs the filetypes that ack associates with each file. Works with -f and -g options. -t TYPE, --type=TYPE, --TYPE Specify the types of files to include in the search. TYPE is a filetype, like perl or xml. --type=perl can also be specified as --perl, although this is deprecated. Type inclusions can be repeated and are ORed together. See ack --help-types for a list of valid types. -T TYPE, --type=noTYPE, --noTYPE Specifies the type of files to exclude from the search. --type=noperl can be done as --noperl, although this is deprecated. If a file is of both type \u0026#34;foo\u0026#34; and \u0026#34;bar\u0026#34;, specifying both --type=foo and --type=nobar will exclude the file, because an exclusion takes precedence over an inclusion. --type-add TYPE:FILTER:ARGS Files with the given ARGS applied to the given FILTER are recognized as being of (the existing) type TYPE. See also \u0026#34;Defining your own types\u0026#34;. --type-set TYPE:FILTER:ARGS Files with the given ARGS applied to the given FILTER are recognized as being of type TYPE. This replaces an existing definition for type TYPE. See also \u0026#34;Defining your own types\u0026#34;. --type-del TYPE The filters associated with TYPE are removed from Ack, and are no longer considered for searches. --[no]underline Turns on underlining of matches, where \u0026#34;underlining\u0026#34; is printing a line of carets under the match. $ ack -u foo peanuts.txt 17: Come kick the football you fool ^^^ ^^^ 623: Price per square foot ^^^ This is useful if you\u0026#39;re dumping the results of an ack run into a text file or printer that doesn\u0026#39;t support ANSI color codes. The setting of underline does not affect highlighting of matches. -v, --invert-match Invert match: select non-matching lines. --version Display version and copyright information. -w, --word-regexp Force PATTERN to match only whole words. -x An abbreviation for --files-from=-. The list of files to search are read from standard input, with one line per file. Note that the list of files is not filtered in any way. If you add \u0026#34;--type=html\u0026#34; in addition to \u0026#34;-x\u0026#34;, the \u0026#34;--type\u0026#34; will be ignored. -1 Stops after reporting first match of any kind. This is different from --max-count=1 or -m1, where only one match per file is shown. Also, -1 works with -f and -g, where -m does not. --thpppt Display the all-important Bill The Cat logo. Note that the exact spelling of --thpppppt is not important. It\u0026#39;s checked against a regular expression. --bar Check with the admiral for traps. --cathy Chocolate, Chocolate, Chocolate! ACKRC LOCATION SEMANTICS Ack 可以从许多来源加载其配置。 以下列表指定了 Ack 查找配置文件的来源； 找到的每一个都按此处指定的顺序加载，并且每一个都覆盖在它之前的任何源中设置的选项。 （例如，如果我在我的用户 ackrc 中设置了 \u0026ndash;sort-files，在命令行中设置了 \u0026ndash;nosort-files，则命令行优先）。\n默认值从 App::Ack::ConfigDefaults 加载。这可以使用 \u0026ndash;ignore-ack-defaults 省略。\nGlobal ackrc\n然后从全局 ackrc 加载选项。它位于类 Unix 系统上的 /etc/ackrc 中。 在 Windows XP 及更早版本下，全局 ackrc 位于 C:\\Documents and Settings\\All Users\\Application Data\\ackrc 在 Windows Vista/7 下，全局 ackrc 位于 C:\\ProgramData\\ackrc\n\u0026ndash;noenv 选项可防止加载所有 ackrc 文件。\nUser ackrc 然后从用户的 ackrc 加载选项。 它位于类 Unix 系统上的 $HOME/.ackrc 中。 在 Windows XP 和更早版本下，用户的 ackrc 位于 C:\\Documents and Settings$USER\\Application Data\\ackrc。 在 Windows Vista/7 下，用户的 ackrc 位于 C:\\Users$USER\\AppData\\Roaming\\ackrc。\n如果要加载不同的用户级 ackrc，可以使用 $ACKRC 环境变量指定。\n\u0026ndash;noenv 选项可防止加载所有 ackrc 文件。\nProject ackrc\n然后从项目 ackrc 加载选项。 项目ackrc是第一个名为.ackrc或_ackrc的ackrc文件，先在当前目录搜索，然后是父目录，然后是祖父目录，等等。这个可以用\u0026ndash;noenv省略。\n\u0026ndash;ackrc \u0026ndash;ackrc 选项可以包含在命令行中以指定可以覆盖所有其他文件的 ackrc 文件。即使存在 \u0026ndash;noenv 也会参考它。\nCommand line 然后从命令行加载选项。\n附录A - ack dump 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 Defaults ======== --ignore-directory=is:.bzr --ignore-directory=is:.cabal-sandbox --ignore-directory=is:.cdv --ignore-directory=is:.git --ignore-directory=is:.hg --ignore-directory=is:.metadata --ignore-directory=is:.pc --ignore-directory=is:.pytest_cache --ignore-directory=is:.svn --ignore-directory=is:CMakeFiles --ignore-directory=is:CVS --ignore-directory=is:RCS --ignore-directory=is:SCCS --ignore-directory=is:_MTN --ignore-directory=is:__MACOSX --ignore-directory=is:__pycache__ --ignore-directory=is:_build --ignore-directory=is:_darcs --ignore-directory=is:_sgbak --ignore-directory=is:autom4te.cache --ignore-directory=is:blib --ignore-directory=is:cover_db --ignore-directory=is:node_modules --ignore-directory=is:~.dep --ignore-directory=is:~.dot --ignore-directory=is:~.nib --ignore-directory=is:~.plst --ignore-file=ext:bak --ignore-file=ext:gif,jpg,jpeg,png --ignore-file=ext:gz,tar,tgz,zip --ignore-file=ext:mo --ignore-file=ext:pdf --ignore-file=ext:pyc,pyd,pyo --ignore-file=ext:so --ignore-file=is:.DS_Store --ignore-file=is:.git --ignore-file=match:/[.-]min[.]js$/ --ignore-file=match:/[.]css[.]map$/ --ignore-file=match:/[.]css[.]min$/ --ignore-file=match:/[.]js[.]map$/ --ignore-file=match:/[.]js[.]min$/ --ignore-file=match:/[.]min[.]css$/ --ignore-file=match:/[._].*[.]swp$/ --ignore-file=match:/^#.+#$/ --ignore-file=match:/core[.]\\d+$/ --ignore-file=match:/~$/ --type-add=actionscript:ext:as,mxml --type-add=ada:ext:ada,adb,ads --type-add=asm:ext:asm,s --type-add=asp:ext:asp --type-add=aspx:ext:master,ascx,asmx,aspx,svc --type-add=batch:ext:bat,cmd --type-add=bazel:ext:bazelrc --type-add=bazel:ext:bzl --type-add=bazel:is:BUILD --type-add=bazel:is:WORKSPACE --type-add=cc:ext:c,h,xs --type-add=cfmx:ext:cfc,cfm,cfml --type-add=clojure:ext:clj,cljs,edn,cljc --type-add=cmake:ext:cmake --type-add=cmake:is:CMakeLists.txt --type-add=coffeescript:ext:coffee --type-add=cpp:ext:cpp,cc,cxx,m,hpp,hh,h,hxx --type-add=csharp:ext:cs --type-add=css:ext:css --type-add=dart:ext:dart --type-add=delphi:ext:pas,int,dfm,nfm,dof,dpk,dproj,groupproj,bdsgroup,bdsproj --type-add=elisp:ext:el --type-add=elixir:ext:ex,exs --type-add=elm:ext:elm --type-add=erlang:ext:erl,hrl --type-add=fortran:ext:f,f77,f90,f95,f03,for,ftn,fpp --type-add=go:ext:go --type-add=groovy:ext:groovy,gtmpl,gpp,grunit,gradle --type-add=gsp:ext:gsp --type-add=haskell:ext:hs,lhs --type-add=hh:ext:h --type-add=hpp:ext:hpp,hh,h,hxx --type-add=html:ext:htm,html,xhtml --type-add=jade:ext:jade --type-add=java:ext:java,properties --type-add=js:ext:js --type-add=json:ext:json --type-add=jsp:ext:jsp,jspx,jspf,jhtm,jhtml --type-add=kotlin:ext:kt,kts --type-add=less:ext:less --type-add=lisp:ext:lisp,lsp --type-add=lua:ext:lua --type-add=lua:firstlinematch:/^#!.*\\blua(jit)?/ --type-add=make:ext:mak --type-add=make:ext:mk --type-add=make:is:GNUmakefile --type-add=make:is:Makefile --type-add=make:is:Makefile.Debug --type-add=make:is:Makefile.Release --type-add=make:is:makefile --type-add=markdown:ext:md,markdown --type-add=matlab:ext:m --type-add=objc:ext:m,h --type-add=objcpp:ext:mm,h --type-add=ocaml:ext:ml,mli,mll,mly --type-add=perl:ext:pl,pm,pod,t,psgi --type-add=perl:firstlinematch:/^#!.*\\bperl/ --type-add=perltest:ext:t --type-add=php:ext:php,phpt,php3,php4,php5,phtml --type-add=php:firstlinematch:/^#!.*\\bphp/ --type-add=plone:ext:pt,cpt,metadata,cpy,py --type-add=pod:ext:pod --type-add=purescript:ext:purs --type-add=python:ext:py --type-add=python:firstlinematch:/^#!.*\\bpython/ --type-add=rake:is:Rakefile --type-add=rr:ext:R --type-add=rst:ext:rst --type-add=ruby:ext:rb,rhtml,rjs,rxml,erb,rake,spec --type-add=ruby:firstlinematch:/^#!.*\\bruby/ --type-add=ruby:is:Rakefile --type-add=rust:ext:rs --type-add=sass:ext:sass,scss --type-add=scala:ext:scala --type-add=scheme:ext:scm,ss --type-add=shell:ext:sh,bash,csh,tcsh,ksh,zsh,fish --type-add=shell:firstlinematch:/^#!.*\\b(?:ba|t?c|k|z|fi)?sh\\b/ --type-add=smalltalk:ext:st --type-add=smarty:ext:tpl --type-add=sql:ext:sql,ctl --type-add=stylus:ext:styl --type-add=svg:ext:svg --type-add=swift:ext:swift --type-add=swift:firstlinematch:/^#!.*\\bswift/ --type-add=tcl:ext:tcl,itcl,itk --type-add=tex:ext:tex,cls,sty --type-add=toml:ext:toml --type-add=ts:ext:ts,tsx --type-add=ttml:ext:tt,tt2,ttml --type-add=vb:ext:bas,cls,frm,ctl,vb,resx --type-add=verilog:ext:v,vh,sv --type-add=vhdl:ext:vhd,vhdl --type-add=vim:ext:vim --type-add=xml:ext:xml,dtd,xsd,xsl,xslt,ent,wsdl --type-add=xml:firstlinematch:/\u0026lt;[?]xml/ --type-add=yaml:ext:yaml,yml ","date":"2022-04-14T00:00:00Z","permalink":"https://MyLoveES.github.io/p/ack-command/","title":"Ack command"},{"content":"\n调度 Scheduling 调度是进行资源分配的一种行为。在单线程、多线程之间进行调度任务。\n目标 Target 最大吞吐量(单位时间内完成的任务量) 最小的等待时间(进程准备就绪到开始执行经过的时间) 最小的响应时间(进程准备就绪到执行完成经过的时间) 最大的公平性(为每个进程公平地分配资源) Linux 中的两种进程类型 实时进程 常规进程 实时进程 实时进程需要服从个响应时间的限制，而不去考虑系统负载。换句话说，实时进程是紧急的，任何情况下都不能延迟。\n举个例子来说，Linux中的负载均衡进程，负责跨CPU内核分配进程。\n常规进程 传统进程没有严格的响应时间的限制，如果系统忙碌，响应会出现延迟。\n比如浏览器的进程，忙碌时加载页面会出现延迟。\n每种进程都有不同的调度算法，只要有准备运行的实时进程，它们就会运行并使常规进程等待。\n实时调度 实时调度有两种调度策略，SCHED_RR 和 SCHED_FIFO。\n该策略会影响进程将获得多少运行时间以及运行队列的运行方式。\n准备运行的进程存储在一个名为 runqueue 的队列中。调度程序正在根据策略从该运行队列中选择要运行的进程。\nSCHED_FIFO 在此策略中，调度程序将根据到达时间（FIFO = 先进先出）选择一个进程。\n具有 SCHED_FIFO 调度策略的进程可以在以下几种情况下“放弃”CPU：\n进程正在等待，例如等待 IO 操作。 当进程回到“就绪”状态时，它将回到运行队列的末尾。 进程通过系统调用 sched_yield 让出 CPU。 该过程将立即返回到运行队列的末尾。 SCHED_RR RR = Round Robin\n在此调度策略中，运行队列中的每个进程都获得一个时间片（单位量）并以循环方式轮流执行（基于优先级）。 为了让我们对循环有更好的理解，让我们考虑一个例子，我们的运行队列中有 3 个进程 A B C，它们都具有 SCHED_RR 的策略。\n如下图所示，每个进程都得到一个时间片并轮流执行。当所有进程运行 1 次时，它们会重复相同的执行顺序。\n实时调度总结 实时进程可以在两种不同的策略 SCHED_FIFO 和 SCHED_RR 中实现调度。该策略会影响运行队列的工作方式以及每个进程执行的时间。\n常规调度 CFS — Completely Fair Scheduler 是 Linux 2.6.23 版本以来常规进程的调度算法。\nCFS 主要关注一个指标——它希望尽可能公平，这意味着他让每个进程都获得 CPU 的平均时间片。\n请注意，具有更高优先级的进程可能仍会获得更大的时间片。 为了让我们了解 CFS 的工作原理，我们必须熟悉一个新术语——虚拟运行时（vruntime）。\nVirtual Runtime 进程的虚拟运行时间是实际执行所花费的时间，不包括任何形式的等待。\nCFS 力求尽可能公平。 为此，CFS 将以准备运行的最短虚拟时间安排进程。\nCFS 维护保持最大和最小虚拟运行时间的变量，原因我们很快就会明白。\nCFS — 完全公平的调度器 在讨论算法是如何工作的之前，让我们先了解一下这个算法使用的是什么数据结构。\nCFS 使用红黑树，它是一种平衡的二叉搜索树——这意味着插入、删除和查找在 O(logN) 中执行，其中 N 是进程数。\n这棵树中的关键是进程的虚拟运行时。\n新进程或从等待中恢复到就绪状态的进程插入到树中，键为 vruntime=min_vruntime。这对于防止树中旧进程的饥饿非常重要。\n继续介绍算法，首先，算法为自己设置了一个时间限制 —— sched_latency。 在这个时间限制内，它将尝试执行所有准备好的进程 —— N。\n这意味着每个进程将获得时间限制除以进程数的时间片 —— Qᵢ = sched_latency/N。\n当一个进程完成其时间片 (Qᵢ) 时，算法会选择树中虚拟运行时间最少的进程来执行下一个。\n让我们解决一个我迄今为止描述算法的方式可能存在问题的情况。\n假设算法选择了 48ms（毫秒）的时间限制，我们有 6 个进程——在这种情况下，每个进程都有 8ms 的时间轮流执行。\n但是当系统中的进程过载时会发生什么？ 假设时间限制仍然是 48 毫秒，但现在我们有 32 个进程，现在每个进程有 1.5 毫秒的执行时间——这将导致我们的系统严重减速。\nWhy? What’s the difference? 上下文切换。 上下文切换是一个存储进程或线程状态的过程，以便它可以在以后恢复并恢复执行。\n每次进程完成其执行时间并安排新进程时，都会发生上下文切换，这也需要时间。\n假设上下文切换花费了我们 1 毫秒，在第一个示例中，每个进程有 6 毫秒，我们可以允许这样做，我们在上下文切换上浪费了 1 毫秒，在实际执行进程上浪费了 5 毫秒。但是在第二个例子中，我们只有 0.5ms 来执行这个过程——我们浪费了大部分时间片来进行上下文切换，这就是它根本无法工作的原因。\n为了克服这种情况，我们引入了一个新变量，它将确定允许的时间片有多小——min_granularity。\n假设 min_granularity=6ms 并回到我们的示例。 我们的时间限制是 48，我们有 32 个进程。 根据我们之前的计算，每个进程都会得到 1.5ms，但现在根本不允许，因为 min_granularity 指定了每个进程应该得到的最小时间片。 在这种情况下，当 Qᵢ \u0026lt; min_granularity 我们将 min_granularity 作为我们的 Qᵢ 并根据它更改时间限制。\n在我们的示例中，Qᵢ 将等于 6ms，因为 1.5ms \u0026lt; 6ms，这意味着新的时间限制将是 Qᵢ ⋅ N = 6ms ⋅ 32 = 192ms。\ndiff between cfs and rr 在这一点上，可能不清楚 CFS 和 RR 之间的区别是什么，因为它们都定义了一些时间片并使进程以某种顺序执行。\n为了总结和更好地理解这些算法之间的差异，这里有一个简短的表格 Ref Process Scheduling In Linux\n","date":"2022-04-11T00:00:00Z","permalink":"https://MyLoveES.github.io/p/process-scheduling-in-linux/","title":"Process Scheduling In Linux"},{"content":"兼二者之长\nSeda 背景 SEDA主要汇聚了两类模型：使用基于线程的并发模型来简化编程，使用基于事件的模型来实现广泛的并发。\n直观地说，如果一个服务的行为像一个简单的管道，那么它就是good的。管道的承载量是由网络的路径和服务自身的处理阶段决定的。随着提供负载的增加，交付的吞吐量按比例增加，直到管道满，吞吐量饱和;附加负载不应该降低吞吐量。同样，在轻负荷下，服务的响应时间大致是恒定的，因为它受管道深度的影响。当负载接近饱和时，排队延迟占主导地位。在许多服务的典型的闭环场景中，每个客户端在交付下一个请求之前等待一个响应，响应时间应该随客户端数量线性增加。\n一个good的服务的关键特性是平稳的降级:当提供的负载超过容量时，服务保持高吞吐量，并以线性的响应时间惩罚影响所有的客户端，或者至少根据某些特定策略进行预测。请注意，典型的Web体验却是相反的，随着负载的增加，吞吐量下降，响应时间急剧增加，造成服务崩溃的印象。\nThread-based 特点是编程容易，每个请求一个线程彼此独立处理。但是和线程相关的开销在线程数量很大的时候可能会造成性能的严重下降，例如缓存或者TLB的未命中、调度开销或者锁的竞争。而且，现在的操作系统（或者这里可以指代为编程语言）在努力以一种对应用程序透明的方式，虚拟化硬件资源。在这种背景下，应用程序很少有机会能够参与系统范围内的资源管理、决策，或者了解到当前系统的资源可用情况，来适应、调整自己的调度分配。操作系统资源的虚拟化，实际上是隐藏了资源有限和资源共享的事实。\nBounded thread pools(Without a Queue) 在这种情况下，有一种解决方案 - 有限的线程池大小。在超出固定限制的时候，新链接不会被接受。通过限制最大并发量的方式，来保证系统维持在高性能的状态下。但是这种情况会造成客户端等待时间任意长（不可控。理想情况下是等待时间成线性增长）。\n同时，这种情况下很难识别一个流中的瓶颈在哪里，因为从宏观层面上只能看到“池”被用完了。因此，需要对于资源的纵向切分。\nEvent-driven 线程驱动的情况下，服务器仅由少量线程构成，线程们循环处理队列中的事件。事件驱动将长链路的任务处理，转换成FSM，状态转换通过事件触发，不再依赖于线程上下文。\n事件驱动在负载方面健壮，当负载逐渐逼近并超过最大负载量的时候，吞吐量几乎不会下降，并且后续请求会堆积在队列中，延迟线性增加。\n但需要注意的是，应尽量避免事件处理过程的阻塞发生（其实在单线程情况下亦如此）\nStructured event queues(common now) 针对事件驱动，人们提出改进方案（感觉可以理解为去中心化）。使用一组事件队列以完成模块化，简化设计。 例如：解耦两个组件的执行，从而提高了模块化和鲁棒性。聚合多个类似事件的执行，增强局部性，从而提高性能。\nThe Staged Event-Driven Architecture: 高并发 \u0026amp; 负载调节 目标 防止过载造成的性能下降 事件驱动，灵活扩展 模块化 资源调度控制器 队列分析（根据优先级、负载排序） Stages as robust building blocks Stage: self-contained\nEvent Handler: 事件处理程序/逻辑\nEvent Queue: 事件队列\nThread pool: 线程池\nController: 控制调度和资源分配\n取出一批事件 -\u0026gt; 处理 -\u0026gt; 放入下一阶段的队列。其中，取出的批量大小和处理的线程池大小通过controller进行控制。\n阶段线程的操作方式是将一批事件从传入的事件队列中取出，并调用应用程序提供的事件处理程序。事件处理程序处理每一批事件，并通过将它们排队到其他阶段的事件队列中来调度零个或多个事件。\n线程是SEDA中的基本并发机制，但是它们的使用被限制在每个阶段的少量线程，而不是系统中的每个任务一个线程。此外，使用动态控制可以根据需要自动调整分配给每个阶段的线程数量。根据线程系统和调度器的特点，这种设计允许阶段按顺序或并行运行，或两者的结合。在本文中，我们假设SMP环境中有优先的、os支持的线程，尽管这种选择不是SEDA设计的基础。例如，可以设计一个线程系统，它可以识别应用程序的阶段结构，并相应地调度线程。\n每个阶段的核心逻辑都由事件处理程序提供，事件处理程序的输入是一批多个事件。事件处理程序不能直接控制队列操作或线程。通过将核心应用程序逻辑与线程管理和调度分离，阶段能够控制事件处理程序的执行，以实现各种资源管理策略。例如，传递给事件处理程序的事件的数量和顺序可以由运行时环境在外部控制。但是，应用程序也可以通过过滤或重新排序传递给它的事件批处理来实现自己的调度策略。\nDetails 队列满了怎么办 背压\n报错\n降级\n两个stage之间应该通过队列通信，还是直接使用子程序调用 通过队列的好处是，可以提供两个stage之间的隔离，模块化和负载管理。代价是增加延迟。\nDynamic resource controllers 目的：控制器根据观察到的，每个Stage的性能和需求自动调整Stage的资源使用情况。控制器观察阶段的运行他正，调整分配和调度参数以满足性能目标。控制器既可以了解某个特定阶段的局部情况，也可以基于全局状态协同工作。 文章提供了几种控制器的实现形式\n线程池数量控制器。保证有足够的线程处理Stage，防止过多或者过少分配。周期性采样输入队列，当队列长度超过某个阈值的时候添加线程；当线程空闲时间超过阈值的时候，去掉线程。 批处理控制器，它调整每个阶段(批处理因子)中事件处理程序的每次调用所处理的事件数量。比如，如果某些任务可以进行组合、聚合的情况下，大批量的事件读入可以提高处理效率。不过这个就需要根据实际情况看了，见仁见智。控制器尝试寻找高吞吐的批量因子。它可以观察stage的输出速率，逐步减少批量因子，直到速率出现下降（然后稍微增多）。在输出速度突然下降时，可以将batch size提升到最大值。 除此之外，控制器还可以根据优先级调整线程池大小，或者给一个全局阈值。还可以根据Stage的进度调整。\nCompare with Procedure-oriented Procedure-oriented 优点：\n架构简单明了，易于实现 各环节在在同一线程/进程内完成，易于追踪 实时性强，任务从一而终 缺点：\n组件结耦不好，边界不清晰 由于资源竞争、锁等待等原因，会发生性能下降 Seda 优点(其实也是事件驱动的优点)：\n解耦：Stage之间通过事件通信 异步：各个环节通过事件关联，所以服务间不会block。带来执行的可调度性，资源分配的动态性 灵活：对Stage施加自定义操作（如监控、日志、限流、降级、熔断等）相当容易 独立：服务间的吞吐也被拆解了，各个服务可以专注于自己的实现，按照自己的处理速度处理 缓冲：保持吞吐 缺点：\n对于一个任务，可能会增加延时（出入队列） 开发不便 ","date":"2022-03-23T19:00:00Z","permalink":"https://MyLoveES.github.io/p/seda-the-staged-event-driven-architecture/","title":"Seda - The Staged Event-Driven Architecture"},{"content":"Why Events Are A Bad Idea 论点：线程可以实现事件的所有优点，包括支持高并发性、低开销和简单的并发模型。此外，线程允许更简单和更自然的编程风格。 原文 Specifically, we believe that threads can achieve all of the strengths of events, including support for high concurrency, low overhead, and a simple concurrency model. Moreover, we argue that threads allow a simpler and more natural programming style.\n过去的一段时间（论文发表前），人们认为Event-oriented是高并发程序中实现高性能的最佳方法。 主要原因有：\n由于协作多任务处理，同步的成本很低; 管理状态的开销更低(没有栈); 基于应用级的信息，更好的调度和局部性; 更灵活的控制流(不仅仅是调用/返回); 但是作者认为：\n线程为高并发服务器提供了一个更自然的抽象; 对编译器和线程运行时系统的小改进可以消除使用事件的历史原因; 线程更易于接受基于编译器的增强; 原文 1 2 3 4 5 6 7 • Inexpensive synchronization due to cooperative multitasking; • Lower overhead for managing state (no stacks); • Better scheduling and locality, based on application-level information; and • More flexible control flow (not just call/return). We believe that (1) threads provide a more natural abstraction for high-concurrency servers, and that (2) small improvements to compilers and thread runtime systems can eliminate the historical reasons to use events. Additionally, threads are more amenable to compiler-based enhancements; we believe the right paradigm for highly concurrent applications is a thread package with better compiler support. Duality Revisited event handlers monitors events accepted by a handler functions exported by a module SendMessage / AwaitReply procedure call, or fork/join SendReply return from procedure waiting for messages waiting on condition variables First, Lauer and Needham ignore the cooperative scheduling used by events for synchronization. Second, most event systems use shared memory and global data structures, which are described as atypical for Lauer and Needham’s message- passing systems. “Problems” with Threads Performance Criticism: 许多尝试使用线程实现的高并发模型并不是很棒\n开销的主要来源是，存在和线程数相关的O(n)的操作的存在；并且上下文切换开销大（由于需要保存寄存器和其他状态，以及kernel crossings）。但这些是线程工具包实现的问题，而非线程自身的问题。 // TODO: 什么是上下文切换，并且为什么开销大\nControl flow Criticism: 线程是有限制性的控制流。它鼓励程序员对控制流进行过于线性的思考，可能会排除使用更有效的控制流模式。\n复杂的控制流模式在实践中是很少见的，并且在不同类型的系统中都需要得到处理。\nSynchronization Criticism: 线程同步机制很重 // TODO: 什么是线程同步？我的理解应该是说，加锁保证同步\n事件系统认为合作多任务模式\u0026quot;免费\u0026quot;地为它们提供了同步机制，运行时系统无需处理互斥、等待队列等等。但是仅仅是在单处理器上实现，而现在大多是多处理器。\nState Management Criticism: 线程堆栈不是管理实时状态的一种有效方式。线程系统通常面临着堆栈溢出的风险和在大堆栈上浪费虚拟地址空间之间的权衡\n作者手动解决\nScheduling Criticism: 线程提供的虚拟处理器模型使运行时系统过于泛化，使其无法做出最佳调度决策 // TODO: 啥意思？\n事件系统能够在应用层面上调度事件的交付。因此，应用程序可以执行最短的剩余完成时间调度，有利于某些请求流，或执行其他优化。也有一些证据表明，通过连续运行几个相同类型的事件，事件允许更好的代码定位[9]。然而，Lauer-Needham二元性表明，我们可以将同样的调度技巧应用于合作调度的线程。\nConclusion 上述论点表明，在高并发性方面，线程的性能至少与事件一样好，而且事件在质量上没有实质性的优势。\n可扩展的用户级线程的缺失为事件风格提供了最大的推动力，但我们已经表明，这一缺陷是现有实现的一个伪命题，而不是线程抽象的一个基本属性。\n（作者认为，上面列举的问题都不是线程本身的锅！都是实现方式造成的！EventDriven和Threads能达到同样的效果！）\nThe Case for Threads 对于高并发服务器，线程是更合理的抽象。原因有二：\n并发请求之间，很大程度上是彼此独立的。 处理请求的代码是顺序的。 Control flow 基于事件的系统，往往会混淆应用程序的事件流，因为不同环节之间实际是通过“事件”连接的。Coder需要记住并且匹配event和环节，并且不便于调试。\n而线程模型表达控制流的方式更自然（线性处理）了，并且线程堆栈封装了足够的信息来进行调试。\nException Handling and State Lifetime 在线程系统中，在异常和正常终止后清理任务状态更简单，因为线程堆栈自然会跟踪该任务的活动状态。\n在事件系统中，任务状态通常是堆分配的。在正确的时间释放这个状态是非常困难的，因为应用程序控制流中的分支(特别是在错误条件下)可能会导致错过释放步骤。 // ?\nCompiler Support for Threads 编译器和系统紧密结合, 实现更好的安全性和性能!\n动态堆栈增长 运行时调整堆栈大小，避免固定大小堆带来的溢出风险和内存浪费。通过编译器分析来得到所需堆栈空间的上限及增长点。\n实时状态管理 清除不必要的状态(state)，检测阻塞调用中持有大量实例的情况等等。\n同步 编译器警告数据竞争处来减少错误的发生。\nConclusion 线程模型能够达到和事件驱动几乎一样的性能效果，并且模型更简单，更易于编译器分析，因此 Threads 是一个更好的编程模型。\nRef ref: Why events are a bad idea\n","date":"2022-03-23T18:00:00Z","permalink":"https://MyLoveES.github.io/p/why-events-are-a-bad-idea/","title":"Why events are a bad idea"},{"content":"// 发现本文很多概念在现在已经不适用了，不做参考\nThreads 诞生于操作系统的发展 演变为用户级别的工具 作为各种问题的解决方案 每个程序员都应该是线程程序员？ 什么是线程 用于管理并发的通用解决方案 执行流彼此独立 共享状态 抢占式调度 同步（例如锁、条件） 问题 线程编程很难用\n为啥这么难用 必须对共享资源加锁来控制访问 由锁产生的死锁问题 难以 debug. 依赖数据和时间 难以抽象 modules 回调在加锁的场景下很难用 很难取得好的性能： 简单的锁(例如监视器)产生低并发性 细粒度锁增加了复杂度，降低了性能 操作系统限制性能(调度、上下文切换) 对线程支持的不好 线程代码难以移植 标准库非线程安全的 内核调用，窗口系统不是多线程 调试工具匮乏 通常不需要并发 替代品 Events\n主张 大多数情况下，事件更好 只有在真正需要CPU并发的时候，线程才需要 Event-Driven Programming 1个执行流:无CPU并发。 注册事件回调 事件循环等待事件，调用处理程序 不抢占事件处理程序 handler通常生命周期很短 难用的地方 长时间运行的环节使应用程序无响应 fork 子进程，用事件在完成时唤醒 拆分handlers 在处理程序中定期调用事件循环（重入性增加了复杂性）。 不能跨事件维持本地状态（处理程序必须return） 没有CPU并发（不适合科学应用） 事件驱动的I/O并不总是得到很好的支持（例如差劲的写缓冲） Events vs. Threads 事件尽可能避免并发，线程相反： 事件易于上手：无并发、无抢占、无同步、无死锁。 仅在不寻常的情况下使用复杂的技术。 使用线程，即使是最简单的应用程序也面临着很大的复杂性。 使用事件更容易调试 时序依赖只与事件相关，与事件内部调度无关 更容易追踪的问题：按钮响应缓慢 vs 损坏的内存 单CPU上事件驱动更快 没有锁的开销 没有上下文切换 事件的可以执行更强 线程提供真正的并发 可以使用长时间运行的有状态处理程序而不会冻结 可扩展的多cpu性能 应该用线程吗：NO！ 尽可能避免线程:\n对于gui、分布式系统、低端服务器，应该使用事件，而不是线程 只使用真正需要CPU并发的线程 在需要线程的地方，在线程应用程序内核中隔离使用:保持大多数代码是单线程的 Conclusions 并发从根本上来说很难;尽可能的避免。 线程比事件更强大，但很少需要这种能力 线程比事件更难编程;仅供专家 使用事件作为主要的开发工具(gui和分布式系统) 只对性能关键的内核使用线程 Ref ref: Why Threads Are a Bad Idea\n","date":"2022-03-22T00:00:00Z","permalink":"https://MyLoveES.github.io/p/why-threads-are-a-bad-idea-for-most-purposes/","title":"Why threads are a bad idea (for most purposes)"},{"content":"\n背景 业务中碰到一个场景，服务需要处理一个Job，分为几个阶段：Pre Do After。实现业务逻辑有两种比较常见的模式:\n线程池 事件驱动 那这两种模式孰优孰劣？看看前人的见解：\nEventDriven-oriented or Procedure-oriented Who are they Event Driven 相对较小的进程数量，进程之间进行显式消息通信。针对于事件驱动，有多种实现形式，例如观察者模式或者发布订阅模式等。\n1 2 3 1. Small number of (relatively static) big processes 2. Explicit set of message channels 3. Limited amount of direct sharing of data in memory 观察者模式\n发布订阅模式 它的特点是事件的产生者并不关心具体的处理逻辑，每个环节的workers都能够专注于自己的工作\n1 2 3 4 5 6 事件驱动的优点： 1. 解耦：事件发布者、订阅者解耦，其中一个环节的变化，不会影响其他环境的进行 2. 异步：各个环节通过事件关联，所以服务间不会block。带来执行的可调度性，资源分配的动态性 3. 灵活：在服务间增加一些适配器（如日志、认证、版本、限流、降级、熔断等）相当容易 4. 独立：服务间的吞吐也被解开了，各个服务可以专注于自己的实现，按照自己的处理速度处理 5. 缓冲：利用 Broker 或队列的方式还可以达到把抖动的吞吐量变成均匀的吞吐量，这就是所谓的“削峰”，这对后端系统是个不错的保护 Procedure-oriented 大量快速变化的小进程和基于共享数据的进程同步机制\n它的特点是一次请求从一而终，由单一的线程（进程）执行\n1 2 3 4 1. Large number of very small processes 2. Rapid creation and deletion of processes 3. Communication by means of direct sharing of data in memory 1 2 3 4 面向过程的优点： 1. 架构简单明了，易于实现 2. 各环节在在同一线程/进程内完成，易于追踪(debug) 3. 实时性强，任务从一而终 The Duality Mapping 1 2 3 4 5 6 7 8 \u0026lt;On the Duality of Operating System Structures\u0026gt; 1. The two models are duals of each other. That is, a program or subsystem constructed strictly according to the primitives defined by one model can be mapped directly into a dual program or subsystem which fits the other model. 2. The dual programs or subsystems are logically identical to each other. They can also be made textually very similar, differing only in non-essential details. 3. The performance of a program or subsystem from one model, as reflected by its queue lengths, waiting times, service rates, etc. is identical to that of its dual system, given identical scheduling strategies. Furthermore, the primitive operations provided by the operating system of one model can be made as efficient as their duals of the other model. 1. 两类模型互为对偶，两者可以互相转换。 2. 逻辑上两者彼此相似，甚至可以在文本上表现得非常相似，除了非必要的细节上。 3. 在相同的调度策略下，两类模型的性能可以一样高（根据队列长度、等待时间、处理速度等方面观测）。除此之外，一个模型提供的基本操作可以使其效率和另一个模型一样高。 Event Driven: simpler concurrency model 原文 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 Messages and message identifiers. A message is a data structure meant for sending information from one process to another; it typically contains a small, fixed area for data which is passed by value and space for a pointer to larger data structures which must be passed by reference. A message identifier is a handle by which a particular message can be identified. Message channels and message ports. A message channel is an abstract structure which identifies the destination of a message. A message port is queue capable of holding messages of a certain class or type which might be received by a particular process. Each message channel must be bound to a particular message port before is can be used. A message port, however, may have more than one message channel bound to it. Four message transmission operations: 1. SendMessage[messageChannel, messageBody] returns [messageId] This operation simply queues a new message on the port bound to the the messageChannel named as parameter. The messageld returned is used as parameter to the following operation. 2. AwaitReply(messageId] returns [messageBody] The operation causes the process to wait for a reply to a specific message previously sent via SendMessage. 3. WaitForMessage[set of messagePort] returns [messageBody, messageld, messagePort] This operation allows a process to wait for a new (unsolicited) message on any one of the message ports named in the \u0026#39;parameter. The message which is first on the queue is returned, along with a message identifier for future reference and an indication of the port from which that message came. 4. SendReply[messageId, messageBody] This operation sends a reply to the particular message identified by the message identifier. SKIP FIGURE In this process, the kind of service requested is a function of which port the requesting message arrives on. It may or may not involve making requests of still other processes and/or sending a reply back to the requestor. It may also result in some circumstance, such as the exhaustion of a resource, which prevents further requests from being considered. These remain queued on their port until later, when the process is willing to listen on that port again. Note that if a whole system is built according to this style, then the sole means of interaction among the components of that system is by means of the message facility. Each process can operate in its own address space without interference from the others. Because of the serial way in which requests are handled, there is never any need to protect the state information of a process from multiple, simultaneous access and updating. 一些概念的定义： Messages: 数据结构，用于进程之间传递信息。\nMessage identifiers: 消息 id\nMessage channels: 消息目的地\nMessage ports: 接收端口\nSendMessage [messageChannel, messageBody] returns [messageId]\nAwaitReply [messageId] returns [messageBody] WaitForMessage [set of messagePort] returns [messageBody, messageld, messagePort] SendReply [messageId, messageBody]\n运行公式 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 begin m: messageBody; i: messageld; p: portid; s: set of portid; ... -local data and state information for this process initialize; do forever; [m, i, p] \u0026lt;- WaitForMessage[s]; case p of port1 =\u0026gt;...; -algorithm for port1 port2 =\u0026gt;... if resourceExhausted then s \u0026lt;- s - port2; SendReply[i, reply]; ...; -algorithm for port2 portk =\u0026gt;... s \u0026lt;- s + port2 ...; -algorithm for portk endcase; endloop; end. Procedure-oriented: simpler \u0026amp; natural programming style 原文 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 1. Procedures. A procedure is a piece of Mesa text containing algorithms, local data, parameters, and results. It always operates in the scope of a Mesa module and may access any global data declared in that module (as well as in any containing procedures). 2. Procedure call facilities, synchronous and asynchronous. The synchronous procedure call mechanism is just the ordinary Mesa procedure call statement, which may return results. This is very much like procedure or function calls in Algol, Pascal, etc. The asynchronous procedure call mechanism is represented by the FORK and JOIN statements, which are defmed as follows: 1) processld \u0026lt;- FORK procedureName[parameterList] - This statement starts the procedure executing as a new process with its own parameters. The procedure operates in the context of its declaration, just as if it had been called synchronously, but the process has its own call stack and state. The calling process continues executing from the statement following the FORK. The process identifier returned from FORK is used in the next statement 2) [resultList] \u0026lt;- JOIN processld - This statement causes the process executing it to synchronize itself with the termination of the process named by the process identifier. The results are retrieved from that process and returned to the calling process as if they had been returned from an ordinary procedure call. The JOlNed process is then destroyed and execution continues in the JOlNing process from the statement following the JOIN. 3. Modules and monitors. A module is the primitive Mesa unit of compilation and consists of a collection of procedures and data. The scope rules of the language determine which of these. procedures and data are accessible or callable from outside the module. A monitor is a special kind of Mesa module which has associated with it a lock to prevent more than one process from executing inside of it at any one time. It is based on and very similar to the monitor mechanism described by Hoare[6]. 4. Module instantiation. Modules (including monitor modules) may be instantiated in Mesa by means of the NEW and START statements. These cause a new context to be created for holding the module data, provide the binding from external procedure references within the module to procedures declared in other modules, and activate the initialization code of the module. 5. Condition variables. Condition variables are part of Hoare\u0026#39;s monitor mechanism an provide more flexible synchronization among events than mutual exclusion facility of the monitor lock or the process termination facility of the JOIN statement. In our model, a condition variable, must be contained within a monitor, has associated with it a queue of processes, and has two operations defined on it: 1) WAIT conditionVariable - This causes the process executing it to release the monitor lock, suspend execution, and join the queue associated with that condition variable. 2) SIGNAL condition Variable -- This causes a process which has previously WAITed on the condition variable to resume execution from its next statement when it is able to reclaim the monitor lock. Note that because the FORK and JOIN operations apply to procedures which are already declared and bound to the right context, these operations take the same order of magnitude of time to execute as do simple procedure calls and returns. Thus processes are very lightweight, and can be created and destroyed very frequently. Module and monitor instantiation, on the other hand, is more cumbersome and is usually done statically before the system is started. Note that this canonical model has no module deletion facility. SKIP FIGURE The attribute ENTRY is used to distinguish procedures which are called from outside the monitor, thus seizing the monitor lock, from those which are declared purely internal to the monitor. Any of the procedures in this module may, of course, call procedures declared in other modules for other system services before returning. Within the monitor, condition variables are used to control waiting for circumstances such as the availability of resources. These are used in this standard resource manager ,tp control the access of a process the procedure representing a particular kind of service. If a whole system is built in this style, then the sole means of interaction among its components is procedural. Processes move from one context to another by means of the procedure call facility across module boundaries, and they use asynchronous calls to stimulate concurrent activity. They depend upon monitor locks and condition variables to keep out of the way of each other. Thus no process can be associated with a single address space unless that space be the whole system. 一些概念的定义 Procedure\nProcedure call facilities, synchronous and asynchronous\n\u0026ndash; (asynchronous):\n\u0026mdash;- processld \u0026lt;- FORK procedureName[parameterList]\n\u0026mdash;- [resultList] \u0026lt;- JOIN processld\nModules: consists of a collection of procedures and data\nMonitors: 监控锁防止多进程在其中执行\nModule instantiation: 实例化\nCondition variables:\n\u0026ndash; WAIT condition Variable\n\u0026ndash; SIGNAL condition Variable\n运行公式 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ResourceManager: MONITOR = C: CONDITION; resourceExhausted: BOOLEAN; ... -global data and state information for this process proc1: ENTRY PROCEDURE[...] = ...; -algorithm for prod proc2: ENTRY PROCEDURE[...] RETURNS[...] = BEGIN IF resourceExhausted THEN WAIT c; •••; RETURN[results]; •••; END; -algorithm for proc2 procL: ENTRY PROCEDURE[...] = BEGIN resourceExhausted \u0026lt;- FALSE; SIGNAL C; ...; END; -algorithm for procL endloop; endloop; initialize; END The Duality Mapping Processes, CreateProcess Monitors, NEW/START Message Channels External Procedure identifiers Message Ports ENTRY procedure identifier SendMessage; AwaitReply (immediate) simple procedure call SendMessage; AwaitReply (delayed) fork;join SendReply RETURN (from procedure) main loop of standard resource manager, WaitForMessage statement, case statement monitor lock, ENTRY attribute arms of the case statement ENTRY procedure declaration selective waiting for messages condition variables, WAIT, SIGNAL 番外 在之后的另一篇文章，Why Events Are A Bad Idea 里面，作者的解释性对比\nevent handlers monitors events accepted by a handler functions exported by a module SendMessage、AwaitReply procedure call, or fork/join SendReply return from procedure waiting for messages waiting on condition variables 性能 程序本身执行时间 调用系统操作开销（可以理解为调用第三方开销？） 队列等待时间（反映的是阻塞、资源竞争、调度策略） 二元性变换使构成系统的程序主体不受影响。因此，所有的算法将以相同的速度计算，并且在每个数据结构中存储相同数量的信息。在每个系统中执行的代码数量相同。将执行相同数量的加法、乘法、比较和字符串操作。因此，如果基本的处理器特性没有改变，那么这些特性将需要相同数量的计算能力，并且系统性能的这个组件将保持不变。同样，对于系统调用开销和队列阻塞等待，两者会付出同样的代价。\n原文 1 2 3 4 5 6 7 8 The duality transformation leaves the main bodies of the programs comprising the system untouched. Thus the algorithms will all compute at the same speed, and the same amount of information will be stored in each data structure. The same amount of client code will be executed in each of the dual systems. The same number of additions, multiplications, comparisons, -and string operations will be performed. Therefore if basic processor characteristics are unchanged, then these will take precisely the same amount of computing power, and this component of the system performance will remain unchanged. The other component affecting the speed of execution of a single program is the time it takes to execute each of the primitive system operations it calls. We assert without proof that the facilities of each of our two canonical models can be made to execute as efficiently as the corresponding facilities of the other model. I.e., Sending a message, with its inherent need to allocate a message block and manipulate a queue and its possibility of forcing a context (process) switch, is a computation of the same complexity as that of calling or FOR King to an ENTRY procedure, which involves the same need to allocate, queue, and force a context switch. Leaving a monitor, with the possibility of having to unqueue a waiting process and re-enter it, is an operation of the same complexity as that of waiting for new messages. Process switching can be made equally fast in either system, and for similar machine architectures this means saving the same amount of state information. The same is true for the scheduling and dispatching of processes at the \u0026#39;microscopic\u0026#39; level. 14 ON THE DUALITY OF OPERATING SYSTEM STRUCTURES Virtual memory and paging or swapping can even be used with equal effectiveness in either model. But, who is better ? ref: Why Threads Are a Bad Idea ref: Why events are a bad idea\nRef ref: On the Duality of Operating System Structures\n","date":"2022-03-21T00:00:00Z","permalink":"https://MyLoveES.github.io/p/%E4%BA%8B%E4%BB%B6%E9%A9%B1%E5%8A%A8-or-%E9%9D%A2%E5%90%91%E8%BF%87%E7%A8%8B/","title":"事件驱动 or 面向过程"},{"content":"","date":"2022-03-10T00:00:00Z","permalink":"https://MyLoveES.github.io/p/%E7%BC%93%E5%AD%98%E7%AE%97%E6%B3%95-window-tinylfu/","title":"缓存算法 - Window-TinyLFU"},{"content":"\nWhat \u0026amp; Why Single entry point for all clients (面向客户端的单一入口) Common in microservice architectures （微服务需要业务聚合） Client insulation from services （客户端和服务隔离） protocol （协议转换） Security（安全） surgical routing（路由） Load Shedding （负载均衡） Rate Limit（限流） Cache（缓存） Fuse（熔断） Retry（重试） Auth（认证、鉴权） Log（日志收集） Monitor（监控） Tracing Analysis（链路追踪） Aspects to be considered Develepment（Programming Language） Community activity Performance Stability Update SOME GATEWAY(By programming language) Nginx+Lua：Open Resty、Kong、Orange、Abtesting Gateway等； Java：Zuul/Zuul 2、Spring Cloud Gateway、Kaazing KWG、gravitee、Dromara soul等； Go：Janus、fagongzi、Grpc-Gateway； .NET：Ocelot； Node.js：Express Gateway、MicroGateway。 使用范围、成熟度、活跃度等，主流有：Kong、Zuul/Zuul 2、Spring Cloud Gateway、apisix\nNginx Nginx 更偏向于作为流量网关。成熟，性能好，稳定。 但致命弱点是，不便于自定义开发（有一部分是开发语言的原因），需要 +Lua (例如下面的 OpenResty / Kong / Apisix)。\n1 Nginx 在启动后，会有一个 Master 进程和多个 Worker 进程，Master 进程和 Worker 进程之间是通过进程间通信进行交互的，如图所示。Worker 工作进程的阻塞点是在像 select()、epoll_wait() 等这样的 I/O 多路复用函数调用处，以等待发生数据可读 / 写事件。Nginx 采用了异步非阻塞的方式来处理请求，也就是说，Nginx 是可以同时处理成千上万个请求的。 OpenResty（Nginx + Lua） OpenResty nginx + Lua动态扩展。目前已经有基于Open Resty产生的gateway，例如 Kong，APISIX等。\n1 还可以将 Lua 嵌入到 Nginx 中，从而可以使用 Lua 来编写脚本，这样就可以使用 Lua 编写应用脚本，部署到 Nginx 中运行，即 Nginx 变成了一个 Web 容器；这样开发人员就可以使用 Lua 语言开发高性能Web应用了。在开发的时候使用 OpenResty 来搭建开发环境，OpenResty 将 Nginx 核心、LuaJIT、许多有用的 Lua 库和 Nginx 第三方模块打包在一起；这样只需要安装 OpenResty，不需要了解 Nginx 核心和写复杂的 C/C++ 模块就可以，只需要使用 Lua 语言进行 Web 应用开发了。 APISIX（Nginx + etcd） Develepment（Programming Language）: N Community activity: Y Performance: Y Stability: ？ Update: Y APISIX的性能更好，功能也更加丰富。 但APISIX很年轻，或许需要更多的“检验”，开发语言的差别也会增加问题排查、处理的代价。\n(可以尝试)\nKong（Nginx + postgres） Develepment（Programming Language）: Y Community activity: Y Performance: N (compared with apisix) Stability: Y Update: Y Kong和APISIX相比较而言，从性能、功能丰富度方面 都处于下风（毕竟APISIX更新）。而且开发语言的劣势同样具有。\nZuul (Java) Zuul-动态路由、监控、弹性和安全。基于Servlet，属于上一代产品，同步I/O、多线程。性能不好，针对于Zuul的替代方案已经有 Zuul2 以及 Spring Gateway 两类。因此Zuul不予考虑。\nPreDecorationFilter\nSendForwardFilter\nRibbonRoutingFilter\nSimpleHostRoutingFilter\nSendResponseFilter\nSendErrorFilter\nPROBLEMS? Large coupled filters Filter model/API is clunky\nZuul2 (Java) Develepment（Programming Language）: N Community activity: Y Performance: Y Stability: ？ Update: Y 服务发现、负载均衡、连接池、状态分类、重试、请求凭证、HTTP/2、TLS、代理协议、GZip、WebSocket\n基于 Netty，Zuul 2.x最大的改进就是基于Netty Server实现了异步I/O来接入请求，同时基于Netty Client实现了到后端业务服务API的请求。这样就可以实现更高的性能、更低的延迟。此外也调整了Filter类型，将原来的三个核心Filter显式命名为Inbound Filter、Endpoint Filter和Outbound Filter，如图7-8所示。\nMajor rewrite\nNot backwards compatible with Zuul 1\nNon-blocking/Netty\nNot released yet\nHTTP/2 and Websockets in the future\nReinvents many things from Spring\nSpring Cloud Gateway (Java) Develepment（Programming Language）: Y Community activity: Y Performance: Y Stability: Y Update: Y Java 8、Spring 5.0、Spring Boot 2.0、Reactor -\u0026gt; Spring family\nSpring Cloud紧密配合 目前使用了nacos作为配置中心，可以和 gateway 搭配。\nJava 8/Spring 5/Boot 2 WebFlux/Reactor HTTP/2 and Websockets Finchley Release Train\nSpring Cloud Gateway里明确地区分了Router和Filter，内置了非常多的开箱即用功能，并且都可以通过Spring Boot配置或手工编码链式调用来使用。\n内置了10种Router，直接配置就可以随心所欲地根据Header、Path、Host或Query来做路由。\n核心特性： 通过请求参数匹配路由； 通过断言和过滤器实现路由； 与Hystrix熔断集成； 与Spring Cloud DiscoveryClient集成； 非常方便地实现断言和过滤器； 请求限流； 路径重写。 Compare Zuul vs Zuul2 (Difference between sync and async): Zuul2 Win! Zuul(block) disadvantages Zuul 1 was built on the Servlet framework Blocking and mltithreaded, means they process requests by using one thread per connection. I/O oprations are done by choosing a worker threade from athread pool to execute the I/O, and the request thread is blocked until the worker thread completes, and the worker thread notifies the request thread when its work is complete. 1 This works well with modern multi-core AWS instances handling 100’s of concurrent connections each. But when things go wrong, like backend latency increases or device retries due to errors, the count of active connections and threads increases. When this happens, nodes get into trouble and can go into a death spiral where backed up threads spike server loads and overwhelm the cluster. To offset these risks, we built in throttling mechanisms and libraries (e.g., Hystrix) to help keep our blocking systems stable during these events. Zuul2(async) advantages Zuul2 async. One thread per CPU core handling all requests and responses. The lifecycle of the request and response is handled through events and callbacks. 1 Because there is not a thread for each request, the cost of connections is cheap. This is the cost of a file descriptor, and the addition of a listener. Whereas the cost of a connection in the blocking model is a thread and with heavy memory and system overhead. There are some efficiency gains because data stays on the same CPU, making better use of CPU level caches and requiring fewer context switches. The fallout of backend latency and “retry storms” (customers and devices retrying requests when problems occur) is also less stressful on the system because connections and increased events in the queue are far less expensive than piling up threads. Zuul2(async) disadvantages \u0026amp; Zuul(block) advantages 1 2 3 The advantages of async systems sound glorious, but the above benefits come at a cost to operations. Blocking systems are easy to grok and debug. A thread is always doing a single operation so the thread’s stack is an accurate snapshot of the progress of a request or spawned task; and a thread dump can be read to follow a request spanning multiple threads by following locks. An exception thrown just pops up the stack. A “catch-all” exception handler can cleanup everything that isn’t explicitly caught. Async, by contrast, is callback based and driven by an event loop. The event loop’s stack trace is meaningless when trying to follow a request. It is difficult to follow a request as events and callbacks are processed, and the tools to help with debugging this are sorely lacking in this area. Edge cases, unhandled exceptions, and incorrectly handled state changes create dangling resources resulting in ByteBuf leaks, file descriptor leaks, lost responses, etc. These types of issues have proven to be quite difficult to debug because it is difficult to know which event wasn’t handled properly or cleaned up appropriately. Kong vs APISIX Spring Cloud Gateway vs Zuul2 vs OpenResty vs Kong 网关 限流 鉴权 监控 易用性 可维护性 成熟度 Spring Cloud Gateway IP、用户、集群限流，提供相应接口扩展 普通鉴权、OAuth2.0 Gateway Metrics Filter 简单 Spring全家桶成员、扩展性强、易配置、易维护 成熟 Zuul2 配置文件，配置集群、单服务器限流、Filter扩展 Filter实现 Filter实现 - - - OpenResty Lua Lua 需要开发 简单易用，但需要Lua开发 可维护性差 成熟 Kong 根据时间、用户限流。可在源码基础上开发http://coding.idealworld.group/2021/05/26/reflections-of-middle-platform-api-gateway-selection/ 普通鉴权、OAuth2.0、Key Auth、HMAC 上报datadog，记录请求数量、数据量、应答数据量、接收发送时间间隔、状态码数量、Kong内耗时 简单易用，API转发通过管理接口配置。需要Lua开发 维护需要Lua 成熟 Finally Kong nginx + lua，开发不友好 仅支持PostgreSQL，增加运维成本，单点数据库 在不修改源码的情况下，无法自定义Nginx配置文件，重启后重新初始化所有变更的Nginx配置文件 Apisix nginx + lua，开发不友好 待检验 Zuul 性能差，已有替代品 Zuul2 性能好，但 Sprintg Cloud 没有兼容计划，与目前使用的技术栈结合的不是很好 还不太稳定 Spring Cloud Gateway 性能比较好（not best） 结论 综合比较下来，Spring Cloud Gateway 性能好，也比较成熟稳定，并且是Spring全家桶成员，和当前项目是用的技术栈更匹配。)\nRef ref: The author of spring cloud gateway\u0026rsquo;s PPT\nref: What is api gateway\nref: Netflix blog: Zuul2\nref: Netflix blog: Optimizing the Netflix API\nref: Microservice Architecture\nref: 百亿流量微服务网关的设计与实现\nref: 中台反思：云原生下API网关的选择\nref: API 网关 Apache APISIX 和 Kong 的选型对比\nref: API网关架构与技术选型\nref: API网关介绍及选型(kong)\nref: API网关选型调研\nref: 微服务下的网关如何选择(提到了soul)\nref: API 网关性能比较：NGINX vs. ZUUL vs. Spring Cloud Gateway vs. Linkerd\nref: API 网关选型及包含 BFF 的架构设计\nref: 5、微服务网关\nref: 如何评价 spring cloud gateway? 对比 zuul2.0 主要的优势是什么?\nref: 如何选择和设计微服务网关\nref: 有了 NGINX 和 Kong，为什么还需要 Apache APISIX\nref: 浅谈 k8s ingress controller 选型\n","date":"2022-03-10T00:00:00Z","permalink":"https://MyLoveES.github.io/p/%E7%BD%91%E5%85%B3%E9%80%89%E5%9E%8B/","title":"网关选型"},{"content":"1. 海选 Nginx，Openresty、Apisix、Kong、Zuul、Zuul2、Spring Cloud Gateway Nginx（淘汰）：功能不够丰富，且开发不便（需求自定义组件开发时, 需要+Lua）。应作为流量网关，而非API网关。 OpenResty（淘汰）：Nginx + Lua。是一种从Nginx扩展各种功能、插件的思路，Kong、Apisix皆基于此，我们没必要从它开始手动实现。 Kong（淘汰）：OpenResty + Lua，同样存在开发不便的问题。存储采用postgresql。对比来看，apisix其实是更优选（国产友好，性能更好，功能也更丰富）。 Zuul（淘汰）：上一代产品，基于BIO，性能不好。已有替代产物 - Zuul2、Spring Cloud Gateway。 2. 决赛圈：Spring Cloud Gateway、Zuul2、APISIX 我们的需求：\n与当前技术栈的适配（Spring Cloud、Nacos 等） 协议转换（http → rpc 等） 路由 限流 负载均衡 熔断 监控 auth 安全 服务发现 插件开发 动态路由 性能：Apisix \u0026gt; Zuul2 ≈ Spring Cloud Gateway 3. 结论 首先从成熟度来看，Zuul2的资料较少，很多还是停留在Zuul1阶段。在Zuul2和Spring Cloud Gateway同属Java开发的情况下，更倾向于Spring。因此之后的比较主要发生在Spring Cloud Gateway和APISIX之间。\n比较来看，apisix优势是性能更好，功能更丰富。劣势是开发语言不匹配，之后开发维护学习成本比较高。而且对于nacos目前也只是实验性，并不完全成熟稳定。除此之外，需要etcd支持。\nSpring Cloud Gateway虽性能稍劣，但技术栈匹配，后期开发难度较小。并且与当前使用的基础设施更契合，能够省却维护新的基础设施组件的成本。因此最终选择Spring Cloud Gateway作为API网关。\nSpring Cloud Gateway(Java) https://docs.spring.io/spring-cloud-gateway/docs/current/reference/html/\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 1. Spring 全家桶成员：✓ 2. http \u0026gt; rpc: x 3. 路由 提供 Route Predicate: 包含 Time、Cookie、Header、Host、Method、Path、Query、RemoteAddr、XForwarded Remote Addr https://docs.spring.io/spring-cloud-gateway/docs/current/reference/html/#gateway-request-predicates-factories Filters：request、response、oauth2、cache request https://docs.spring.io/spring-cloud-gateway/docs/current/reference/html/#gatewayfilter-factories 4. 限流 redis - 令牌桶 https://docs.spring.io/spring-cloud-gateway/docs/current/reference/html/#the-requestratelimiter-gatewayfilter-factory 5. 负载均衡 weight route: https://docs.spring.io/spring-cloud-gateway/docs/current/reference/html/#the-weight-route-predicate-factory 6. 熔断(By status code / ) circuit breaker: https://docs.spring.io/spring-cloud-gateway/docs/current/reference/html/#spring-cloud-circuitbreaker-filter-factory 可以将错误信息回调 7. 监控 Prometheus https://docs.spring.io/spring-cloud-gateway/docs/current/reference/html/#the-gateway-metrics-filter 8. auth / security / 插件开发 等通过各类Filter可以实现 11. 服务发现：nacos / eureka 13. 动态路由：可通过监听nacos \u0026amp; 代码更新 Zuul2(Java) https://github.com/Netflix/zuul/wiki/Getting-Started-2.0\nsample: https://github.com/dashprateek/zuul2-sample\nhttps://github.com/csh0711/zuul-2-sample\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 1.SpringCloud 不兼容Zuul2 2.http 转 rpc: x 3.路由: ✓ 4.限流: ✓ https://github.com/Netflix/zuul/wiki/Core-Features#origin-concurrency-protection 整体限流：max-requests Per server：MaxConnectionsPerHost If an origin exceeds overall concurrency or per-host concurrency, Zuul will return a 503 to the client. 5.负载均衡: ✓ roundrobin(轮询) https://github.com/Netflix/ribbon/wiki/Working-with-load-balancers#roundrobinrule AvailabilityFilteringRule(可用性过滤) https://github.com/Netflix/ribbon/wiki/Working-with-load-balancers#availabilityfilteringrule WeightedResponseTimeRule(根据响应时间加权重) https://github.com/Netflix/ribbon/wiki/Working-with-load-balancers#weightedresponsetimerule 6. 熔断（自定义）: ✓ https://www.javadoc.io/doc/org.springframework.cloud/spring-cloud-netflix-zuul/2.0.3.RELEASE/org/springframework/cloud/netflix/zuul/filters/route/FallbackProvider.html https://www.baeldung.com/spring-zuul-fallback-route 7.监控: prometheus、Statsd https://zuul-ci.org/docs/zuul/latest/monitoring.html 8.auth、安全: ✓ 11. 服务发现: ✓ 静态配置 / eureka 12.插件开发: Java 13.数据存储: JPA / redis APISIX(OpenResty + Lua) https://apisix.apache.org/docs/apisix/getting-started\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 1.Nginx + Lua: x 2.支持rpc: ✓ grpc: https://apisix.apache.org/zh/docs/apisix/plugins/grpc-transcode dubbo: https://apisix.apache.org/zh/docs/apisix/plugins/dubbo-proxy 3.路由: ✓ 4.限流: ✓ limit-req: 限制请求速度的插件，使用的是漏桶算法。https://apisix.apache.org/zh/docs/apisix/plugins/limit-req limit-conn: 限制并发请求（或并发连接）插件。https://apisix.apache.org/zh/docs/apisix/plugins/limit-conn limit-count: 在指定的时间范围内，限制总的请求个数。并且在 HTTP 响应头中返回剩余可以请求的个数。https://apisix.apache.org/zh/docs/apisix/plugins/limit-count 5.负载均衡: ✓ 策略：chash(一致性hash)，roundrobin(轮询)，priority，least_conn(最少连接数)，ewma(指数加权移动平均法) https://github.com/apache/apisix/tree/master/apisix/balancer 6.熔断（backoff）:✓ https://apisix.apache.org/zh/docs/apisix/plugins/api-breaker/ 7.监控: ✓ prometheus: https://apisix.apache.org/zh/docs/apisix/plugins/prometheus/ zipkin: https://apisix.apache.org/zh/docs/apisix/plugins/zipkin skywalking: https://apisix.apache.org/zh/docs/apisix/plugins/skywalking node-status: https://apisix.apache.org/zh/docs/apisix/plugins/node-status datadog: https://apisix.apache.org/zh/docs/apisix/plugins/datadog 8.auth: ✓ key-auth(k-v): https://apisix.apache.org/zh/docs/apisix/plugins/key-auth jwt-auth: https://apisix.apache.org/zh/docs/apisix/plugins/jwt-auth basic-auth: https://apisix.apache.org/zh/docs/apisix/plugins/basic-auth authz-keycloak: https://apisix.apache.org/zh/docs/apisix/plugins/authz-keycloak wolf-rbac: https://apisix.apache.org/zh/docs/apisix/plugins/wolf-rbac openid-connect: https://apisix.apache.org/zh/docs/apisix/plugins/openid-connect hmac-auth: https://apisix.apache.org/zh/docs/apisix/plugins/hmac-auth authz-casbin: https://apisix.apache.org/zh/docs/apisix/plugins/authz-casbin ldap-auth: https://apisix.apache.org/zh/docs/apisix/plugins/ldap-auth opa: https://apisix.apache.org/zh/docs/apisix/plugins/opa forward-auth: https://apisix.apache.org/zh/docs/apisix/plugins/forward-auth 9.安全: ✓ cors: https://apisix.apache.org/zh/docs/apisix/plugins/cors uri-blocker: 指定block_rules，拦截请求 https://apisix.apache.org/zh/docs/apisix/plugins/uri-blocker ip-restriction: ip 黑白名单 https://apisix.apache.org/zh/docs/apisix/plugins/ip-restriction ua-restriction: User-Agent 黑白名单 https://apisix.apache.org/zh/docs/apisix/plugins/ua-restriction referer-restriction: referer 黑白名单 https://apisix.apache.org/zh/docs/apisix/plugins/referer-restriction consumer-restriction: 自定义 https://apisix.apache.org/zh/docs/apisix/plugins/consumer-restriction 10.log: ✓ http / skywalking / tcp / kafka / rocketmq / udp / Syslog / log-rotate / error-log-logger / sls-logger / google-cloud / splunk-hec 11.服务发现: ✓ DNS consul_kv nacos(实验性) eureka 12.插件开发: ✓ lua: https://apisix.apache.org/zh/docs/apisix/plugin-develop external plugin: java: https://apisix.apache.org/zh/docs/apisix/external-plugin go: https://apisix.apache.org/docs/go-plugin-runner/getting-started/ python: https://apisix.apache.org/docs/python-plugin-runner/getting-started/ 13.数据存储/动态路由：etcd 番外 对dubbo集成 APISIX https://apisix.apache.org/zh/docs/apisix/plugins/dubbo-proxy/\nhttps://apisix.apache.org/zh/blog/2022/01/13/how-to-proxy-dubbo-in-apache-apisix/ dubbo-proxy 插件允许将 HTTP 请求代理到 dubbo，但是有所限制：参数和返回值都必须要是 Map\u0026lt;String, Object\u0026gt;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 curl http://127.0.0.1:9080/apisix/admin/upstreams/1 -H \u0026#39;X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\u0026#39; -X PUT -d \u0026#39; { \u0026#34;nodes\u0026#34;: { \u0026#34;127.0.0.1:20880\u0026#34;: 1 }, \u0026#34;type\u0026#34;: \u0026#34;roundrobin\u0026#34; }\u0026#39; curl http://127.0.0.1:9080/apisix/admin/routes/1 -H \u0026#39;X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\u0026#39; -X PUT -d \u0026#39; { \u0026#34;uris\u0026#34;: [ \u0026#34;/hello\u0026#34; ], \u0026#34;plugins\u0026#34;: { \u0026#34;dubbo-proxy\u0026#34;: { \u0026#34;service_name\u0026#34;: \u0026#34;org.apache.dubbo.sample.tengine.DemoService\u0026#34;, \u0026#34;service_version\u0026#34;: \u0026#34;0.0.0\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;tengineDubbo\u0026#34; } }, \u0026#34;upstream_id\u0026#34;: 1 }\u0026#39; 如果要满足复杂场景，需要在服务中额外增加一个HTTP TO DUBBO的service进行处理。通过 HTTP Request Body 描述要调用的 Service 和 Method 以及对应参数，再利用 Java 的反射机制实现目标方法的调用。最后将返回值序列化为 JSON，并写入到 HTTP Response Body 中.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 public class DubboInvocationParameter { private String type; private String value; } public class DubboInvocation { private String service; private String method; private DubboInvocationParameter[] parameters; } public interface HTTP2DubboService { Map\u0026lt;String, Object\u0026gt; invoke(Map\u0026lt;String, Object\u0026gt; context) throws Exception; } @Component public class HTTP2DubboServiceImpl implements HTTP2DubboService { @Autowired private ApplicationContext appContext; @Override public Map\u0026lt;String, Object\u0026gt; invoke(Map\u0026lt;String, Object\u0026gt; context) throws Exception { DubboInvocation invocation = JSONObject.parseObject((byte[]) context.get(\u0026#34;body\u0026#34;), DubboInvocation.class); Object[] args = new Object[invocation.getParameters().size()]; for (int i = 0; i \u0026lt; args.length; i++) { DubboInvocationParameter parameter = invocation.getParameters().get(i); args[i] = JSONObject.parseObject(parameter.getValue(), Class.forName(parameter.getType())); } Object svc = appContext.getBean(Class.forName(invocation.getService())); Object result = svc.getClass().getMethod(invocation.getMethod()).invoke(args); Map\u0026lt;String, Object\u0026gt; httpResponse = new HashMap\u0026lt;\u0026gt;(); httpResponse.put(\u0026#34;status\u0026#34;, 200); httpResponse.put(\u0026#34;body\u0026#34;, JSONObject.toJSONString(result)); return httpResponse; } } Spring Cloud Gateway https://www.cnblogs.com/zlt2000/p/13201326.html\n由于SpringCloudGateway不支持Http到RPC协议的转换，因此需要引入\u0026quot;web\u0026quot;层来作http→rpc。有两种实现方式\n通过web层来转换http → rpc 通过dubbo rest承接http 自定义auth（插件开发） apisix: https://apisix.apache.org/zh/blog/2021/09/07/how-to-use-apisix-auth/ spring cloud gateway: preFilter\n","date":"2022-03-10T00:00:00Z","permalink":"https://MyLoveES.github.io/p/%E7%BD%91%E5%85%B3%E9%80%89%E5%9E%8B%E7%BB%86%E8%8A%82/","title":"网关选型细节"},{"content":"背景 服务运行不起来，ps aux查看state始终处在 Dl+ 状态\n查看dmesg或者cat /var/log/messages，发现 trap invalid opcode 和 segfault at. 查过后，可能是指令集问题。\n解释 ip:c14490 sp:7f975324f790 error:0 in xxxx[400000+53a3d000]\n其中，\nip: 指令(内存)指针 sp: 堆栈指针地址（栈顶指针） [xxxx+yyyy] : 虚拟内存起始地址、大小 error number: 4 -\u0026gt; 100 bit2: 值为1表示是用户态程序内存访问越界，值为0表示是内核态程序内存访问越界\nbit1: 值为1表示是写操作导致内存访问越界，值为0表示是读操作导致内存访问越界\nbit0: 值为1表示没有足够的权限访问非法地址的内容，值为0表示访问的非法地址根本没有对应的页面，也就是无效地址\n出错地址： 0xc14490 - 0x400000 = 0x814490\n反汇编，执行(很不幸，我反编译失败了)\n1 objdump -D xxxx \u0026gt; obj 检索 0814490 尝试在老CPU上重新编译，运行无错。\nref: https://blog.csdn.net/wangtingting_100/article/details/83749504\nref: https://stackoverflow.com/questions/2549214/interpreting-segfault-messages\nref: https://utcc.utoronto.ca/~cks/space/blog/linux/KernelSegfaultMessageMeaning\n贴一下 starkoverflow 上的操作 If this were a program, not a shared library Run addr2line -e yourSegfaultingProgram c14490 (and repeat for the other instruction pointer valUes given) to see where the error is happening. Better, get a debug-instrumented build, and reproduce the problem under a debugger such as gdb.\nSince it\u0026rsquo;s a shared library You\u0026rsquo;re hosed, unfortunately; it\u0026rsquo;s not possible to know where the libraries were placed in memory by the dynamic linker after-the-fact. Reproduce the problem under gdb.\nWhat the error means Here\u0026rsquo;s the breakdown of the fields:\naddress (after the at) - the location in memory the code is trying to access (it\u0026rsquo;s likely that c14490 are offsets from a pointer we expect to be set to a valid value but which is instead pointing to 0) ip - instruction pointer, ie. where the code which is trying to do this lives (指令指针，即尝试执行此操作的代码所在的位置) sp - stack pointer (堆栈指针) error - An error code for page faults; see below for what this means on x86 link. 1 2 3 4 5 6 7 8 9 10 11 /* * Page fault error code bits: * * bit 0 == 0: no page found 1: protection fault * bit 1 == 0: read access 1: write access * bit 2 == 0: kernel-mode access 1: user-mode access * bit 3 == 1: use of reserved bit detected * bit 4 == 1: fault was an instruction fetch * bit 5 == 1: protection keys block access * bit 15 == 1: SGX MMU page-fault */ 贴另一个blog，更详尽的解释 What the Linux kernel\u0026rsquo;s messages about segfaulting programs mean on 64-bit x86 For quite a while the Linux kernel has had an option to log a kernel message about every faulting user program, and it probably defaults to on in your Linux distribution. I\u0026rsquo;ve seen these messages fly by for years, but for reasons beyond the scope of this entry I\u0026rsquo;ve recently wanted to understand what they mean in some moderate amount of detail.\n很长一段时间以来，Linux 内核都有一个选项来记录有关每个错误用户程序的内核消息，并且它可能在您的 Linux 发行版中默认为开启。多年来，我一直看到这些消息飞来飞去，但由于超出本文范围的原因，我最近想以适度的细节了解它们的含义。\nI\u0026rsquo;ll start with a straightforward and typical example, one that I see every time I build and test Go (as this is a test case that is supposed to crash):\n我将从一个简单而典型的示例开始，我每次构建和测试 Go 时都会看到这个示例（因为这是一个应该崩溃的测试用例）：\n1 testp[19288]: segfault at 0 ip 0000000000401271 sp 00007fff2ce4d210 error 4 in testp[400000+98000] The meaning of this is:\n\u0026rsquo;testp[19288]\u0026rsquo; is the faulting program and its PID \u0026lsquo;segfault at 0\u0026rsquo; tells us the memory address (in hex) that caused the segfault when the program tried to access it. Here the address is 0, so we have a null dereference of some sort.(告诉我们当程序试图访问它时导致段错误的内存地址（十六进制）。这里的地址是 0，所以我们有某种类型的空解引用。) \u0026lsquo;ip 0000000000401271\u0026rsquo; is the value of the instruction pointer at the time of the fault. This should be the instruction that attempted to do the invalid memory access. In 64-bit x86, this will be register %rip (useful for inspecting things in GDB and elsewhere).(是故障时指令指针的值。这应该是试图进行无效内存访问的指令。在 64 位 x86 中，这将是寄存器 %rip（用于检查 GDB 和其他地方的内容）) \u0026lsquo;sp 00007fff2ce4d210\u0026rsquo; is the value of the stack pointer. In 64-bit x86, this will be %rsp.(是堆栈指针的值。在 64 位 x86 中，这将是 %rsp。)(是来自traps.herror 4的十六进制页面错误代码位 ，和往常一样，几乎总是至少为 4（这意味着“用户模式访问”）。值 4 表示读取未映射区域，例如地址 0，而值 6 (4+2) 表示写入未映射区域) \u0026rsquo;error 4\u0026rsquo; is the page fault error code bits from traps.h in hex, as usual, and will almost always be at least 4 (which means \u0026lsquo;user-mode access\u0026rsquo;). A value of 4 means it was a read of an unmapped area, such as address 0, while a value of 6 (4+2) means it was a write of an unmapped area. \u0026lsquo;in testp[400000+98000]\u0026rsquo; tells us the specific virtual memory area that the instruction pointer is in, specifying which file it is (here it\u0026rsquo;s the executable), the starting address that VMA is mapped at (0x400000), and the size of the mapping (0x98000).(告诉我们指令指针所在的特定虚拟内存区域，指定它是哪个文件（这里是可执行文件），VMA 映射的起始地址（0x400000），以及映射的大小（0x98000）) With a faulting address of 0 and an error code of 4, we know this particular segfault is a read of a null pointer.\n错误地址为 0，错误代码为 4，我们知道这个特定的段错误是对空指针的读取。\nHere\u0026rsquo;s two more error messages:\n1 bash[12235]: segfault at 1054808 ip 000000000041d989 sp 00007ffec1f1cbd8 error 6 in bash[400000+f4000] \u0026lsquo;Error 6\u0026rsquo; means a write to an unmapped user address, here 0x1054808. “错误 6”表示写入未映射的用户地址，此处为 0x1054808。\n1 bash[11909]: segfault at 0 ip 00007f83c03db746 sp 00007ffccbeda010 error 4 in libc-2.23.so[7f83c0350000+1c0000] Error 4 and address 0 is a null pointer read but this time it\u0026rsquo;s in some libc function, not in bash\u0026rsquo;s own code, since it\u0026rsquo;s reported as \u0026lsquo;in libc-2.23.so[\u0026hellip;]\u0026rsquo;. Since I looked at the core dump, I can tell you that this was in strlen().\n错误 4 和地址 0 是读取的空指针，但这次是在某个 libc 函数中，而不是在 bash 自己的代码中，因为它被报告为“在 libc-2.23.so[\u0026hellip;]”中。由于我查看了核心转储，我可以告诉你这是在strlen().\nOn 64-bit x86 Linux, you\u0026rsquo;ll get a somewhat different message if the problem is actually with the instruction being executed, not the address it\u0026rsquo;s referencing. For example: 在 64 位 x86 Linux 上，如果问题实际上出在正在执行的指令上，而不是它所引用的地址上，那么您将收到一条稍微不同的消息。例如：\n1 bash[2848] trap invalid opcode ip:48db90 sp:7ffddc8879e8 error:0 in bash[400000+f4000] There are a number of such trap types set up in traps.c. Two notable additional ones are \u0026lsquo;divide error\u0026rsquo;, which you get if you do an integer division by zero, and \u0026lsquo;general protection\u0026rsquo;, which you can get for certain extremely wild pointers (one case I know of is when your 64-bit x86 address is not in \u0026lsquo;canonical form\u0026rsquo;). Although these fields are formatted slightly differently, most of them mean the same thing as in segfaults. The exception is \u0026rsquo;error:0\u0026rsquo;, which is not a page fault error code. I don\u0026rsquo;t understand the relevant kernel code enough to know what it means, but if I\u0026rsquo;m reading between the lines correctly in entry_64.txt, then it\u0026rsquo;s either 0 (the usual case) or an error code from the CPU. Here is one possible list of exceptions that get error codes. 在traps.c中设置了许多此类陷阱类型。两个值得注意的附加错误是“除法错误”，如果你将整数除以零，你会得到它，以及“一般保护”，你可以得到某些非常狂野的指针（我知道的一种情况是当你的 64 位 x86地址不是“规范形式”）。尽管这些字段的格式略有不同，但它们中的大多数与段错误中的含义相同。例外是“ error:0”，它不是页面错误错误代码。我对相关内核代码的理解不足以知道它的含义，但是如果我在entry_64.txt中的行之间正确读取，那么它要么是 0（通常情况），要么是来自 CPU 的错误代码。这里是获取错误代码的一个可能的异常列表。\nSometimes these messages can be a little bit unusual and surprising. Here is a silly sample program and the error it produces when run. The code: 有时这些消息可能有点不寻常和令人惊讶。这是一个愚蠢的示例程序以及它在运行时产生的错误。代码：\n1 2 3 4 5 6 #include \u0026lt;stdio.h\u0026gt; int main(int argc, char **argv) { int (*p)(); p = 0x0; return printf(\u0026#34;%d\\n\u0026#34;, (*p)()); } If compiled (without optimization is best) and run, this generates the kernel message: 如果编译（最好没有优化）并运行，这会生成内核消息：\n1 a.out[3714]: segfault at 0 ip (null) sp 00007ffe872aa418 error 14 in a.out[400000+1000] The \u0026lsquo;(null)\u0026rsquo; bit turns out to be expected; it\u0026rsquo;s what the general kernel printf() function generates when asked to print something as a pointer and it\u0026rsquo;s null (as seen here). In our case the instruction pointer is 0 (null) because we\u0026rsquo;ve made a subroutine call through a null pointer and thus we\u0026rsquo;re trying to execute code at address 0. I don\u0026rsquo;t know why the \u0026lsquo;in \u0026hellip;\u0026rsquo; portion says that we\u0026rsquo;re in the executable (although in this case the call actually was there).\nThe error code of 14 is in hex, which means that as bits it\u0026rsquo;s 010100. This is a user mode read of an unmapped area (our usual \u0026lsquo;4\u0026rsquo; case), but it\u0026rsquo;s an instruction fetch, not a normal data read or write. Any error 14s are a sign of some form of mangled function call or a return to a mangled address because the stack has been mashed.\n\u0026rsquo; (null)\u0026rsquo; 位是预期的；这是一般内核 printf() 函数在被要求将某些内容作为指针打印并且它为 null 时生成的内容（如此处所示）。在我们的例子中，指令指针为 0（空），因为我们通过空指针进行了子程序调用，因此我们试图在地址 0 处执行代码。我不知道为什么“in \u0026hellip;”部分表示我们在可执行文件中（尽管在这种情况下调用实际上在那里）。\n错误代码 14 是十六进制的，这意味着它的位是 010100。这是对未映射区域的用户模式读取（我们通常的“4”情况），但它是指令获取，而不是正常的数据读取或写入。任何错误 14s 都是某种形式的函数调用错误或返回到错误地址的标志，因为堆栈已被混合。\n(These bits turn out to come straight from the CPU\u0026rsquo;s page fault IDT.) .\nFor 64-bit x86 Linux kernels (and possibly for 32-bit x86 ones as well), the code you want to look at is show_signal_msg in fault.c, which prints the general \u0026lsquo;segfault at ..\u0026rsquo; message, do_trap and do_general_protection in traps.c, which print the \u0026rsquo;trap \u0026hellip;\u0026rsquo; messages, and print_vma_addr in memory.c, which prints the \u0026lsquo;in \u0026hellip;\u0026rsquo; portion for all of these messages.\nSidebar: The various error code bits as numbers +1 protection fault in a mapped area (eg writing to a read-only mapping) 映射区域中的保护故障（例如写入只读映射） +2 write (instead of a read) +4 user mode access (instead of kernel mode access) +8 use of reserved bits in the page table entry detected (the kernel will panic if this happens) use of reserved bits in the page table entry detected (the kernel will panic if this happens) +16 (+0x10) fault was an instruction fetch, not data read or write +32 (+0x20) \u0026lsquo;protection keys block access\u0026rsquo; (don\u0026rsquo;t ask me) Hex 0x14 is 0x10 + 4; (hex) 6 is 4 + 2. Error code 7 (0x7) is 4 + 2 + 1, a user-mode write to a read-only mapping, and is what you get if you attempt to write to a string constant in C:\n1 2 3 4 char *ex = \u0026#34;example\u0026#34;; int main(int argc, char **argv) { *ex = \u0026#39;E\u0026#39;; } Compile and run this and you will get:\n1 a.out[8832]: segfault at 400540 ip 0000000000400499 sp 00007ffce6831490 error 7 in a.out[400000+1000] It appears that the program code always gets loaded at 0x400000 for ordinary programs, although I believe that shared libraries can have their location randomized.\nPS: Per a comment in the kernel source, all accesses to addresses above the end of user space will be labeled as \u0026lsquo;protection fault in a mapped area\u0026rsquo; whether or not there are actual page table entries there. The kernel does this so you can\u0026rsquo;t work out where its memory pages are by looking at the error code.\n(I believe that user space normally ends around 0x07fffffffffff, per mm.txt, although see the comments about TASK_SIZE_MAX in processor.h and also page_64_types.h.) .\n","date":"2022-03-07T00:00:00Z","permalink":"https://MyLoveES.github.io/p/linux-avx512-%E9%80%A0%E6%88%90%E7%A8%8B%E5%BA%8F-crash/","title":"Linux - AVX512 造成程序 crash"},{"content":"自定义异常 简化 / 自定义打印结果 1 2 3 4 5 6 @Override public synchronized Throwable fillInStackTrace() { return this; } com.demo.test.xxxx.custom.exception.SimpleException: 自定义异常 ","date":"2022-02-23T17:52:13Z","permalink":"https://MyLoveES.github.io/p/customized-java-exception/","title":"Customized Java Exception"},{"content":"Docker Image nmtan/chevereto\nEnvironment variables The most essentials environments variables are listed below\nCHEVERETO_DB_HOST - Hostname of the Database machine that you wish to connect, default to db CHEVERETO_DB_PORT - The port of the Database machine to connect to, default to 3306 CHEVERETO_DB_USERNAME - Username to authenticate to MySQL database, default to chevereto CHEVERETO_DB_PASSWORD - Password of the user when connect to MySQL database, default to chevereto CHEVERETO_DB_NAME - Name of the database in MySQL server, default to chevereto CHEVERETO_DB_PREFIX - Table prefix (you can use this to run multiple instance of Chevereto using the same Database), default to chv_ For other environment variables, please consult the file settings.php and the section \u0026ldquo;Advanced configuration\u0026rdquo; below.\nPersistent storage Chevereto stores images uploaded by users in /var/www/html/images directory within the container.\nYou can mount a data volume at this location to ensure that you don\u0026rsquo;t lose your images if you relaunch/remove container.\nMax image size By default, PHP allow a maximum file upload to be 2MB. You can change such behaviour by updating the php.ini in your container, either by bind-mount the file, or build a new image with the updated file, that way you can reuse the image on demand.\nNote that by default, Chevereto set a file upload limit of 10MB, so after you modify your php.ini, you should also update this settings in Chevereto settings page (available at CHEVERETO_URL/dashboard/settings/image-upload)\nThe customized php.ini should set the values of upload_max_filesize, post_max_size and potentially memory_limit, as showed in the discussion from Chevereto Forum. Further details on these parameters are available from PHP documentation\nAn example of this is available in the examples/bigger-files directory\nStandalone 1 2 3 4 5 6 7 8 9 10 docker run -it --name chevereto -d \\ --link mysql:mysql \\ -p 80:80 \\ -v \u0026#34;$PWD/images\u0026#34;:/var/www/html/images \\ -e \u0026#34;CHEVERETO_DB_HOST=db\u0026#34; \\ -e \u0026#34;CHEVERETO_DB_USERNAME=chevereto\u0026#34; \\ -e \u0026#34;CHEVERETO_DB_PASSWORD=chevereto\u0026#34; \\ -e \u0026#34;CHEVERETO_DB_NAME=chevereto\u0026#34; \\ -e \u0026#34;CHEVERETO_DB_PREFIX=chv_\u0026#34; \\ nmtan/chevereto docker-compose 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 version: \u0026#39;3\u0026#39; services: db: image: mariadb container_name: chevereto_mariadb volumes: - ./database:/var/lib/mysql:rw restart: always environment: MYSQL_ROOT_PASSWORD: chevereto_root MYSQL_DATABASE: chevereto MYSQL_USER: chevereto MYSQL_PASSWORD: chevereto networks: - docker_default - docker_public chevereto: depends_on: - db image: nmtan/chevereto restart: always container_name: chevereto environment: CHEVERETO_DB_HOST: db CHEVERETO_DB_USERNAME: chevereto CHEVERETO_DB_PASSWORD: chevereto CHEVERETO_DB_NAME: chevereto CHEVERETO_DB_PREFIX: chv_ volumes: - ./images:/var/www/html/images:rw - ./php.ini:/usr/local/etc/php/php.ini:rw ports: - 8081:80 networks: - docker_default - docker_public networks: docker_default: external: true docker_public: external: true Nginx (使用了bitwarden的nginx) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 upstream chevereto { server chevereto:80; } server { listen 8080 default_server; listen [::]:8080 default_server; server_name chevereto.weasley.cn; proxy_set_header X-Forwarded-For $remote_addr; location / { proxy_set_header Host $host; proxy_pass http://chevereto; client_max_body_size 256m; } } php.ini 1 2 3 4 5 [PHP] max_execution_time = 60; memory_limit = 256M; upload_max_filesize = 256M; post_max_size = 256M; ","date":"2022-02-15T00:00:00Z","permalink":"https://MyLoveES.github.io/p/install-and-deploy-chevereto-linux/","title":"Install and deploy Chevereto - Linux"},{"content":"云服务器安装 bitwarden 官方文档 Install and Deploy - Linux 创建bitwarden本地用户和目录（SKIP） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 1. Create a bitwarden user: sudo adduser bitwarden 2. Set password for bitwarden user (strong password): sudo passwd bitwarden 3. Create a docker group (if it doesn’t already exist): sudo groupadd docker 4. Add the bitwarden user to the docker group: sudo usermod -aG docker bitwarden 5. Create a bitwarden directory: sudo mkdir /opt/bitwarden 6. Set permissions for the /opt/bitwarden directory: sudo chmod -R 700 /opt/bitwarden 7. Set the bitwarden user ownership of the /opt/bitwarden directory: sudo chown -R bitwarden:bitwarden /opt/bitwarden 安装 1. Download the Bitwarden installation script (bitwarden.sh) to your machine: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 curl -Lso bitwarden.sh https://go.btwrdn.co/bw-sh \u0026amp;\u0026amp; chmod 700 bitwarden.sh 全文： #!/usr/bin/env bash set -e cat \u0026lt;\u0026lt; \u0026#34;EOF\u0026#34; _ _ _ _ | |__ (_) |___ ____ _ _ __ __| | ___ _ __ | \u0026#39;_ \\| | __\\ \\ /\\ / / _` | \u0026#39;__/ _` |/ _ \\ \u0026#39;_ \\ | |_) | | |_ \\ V V / (_| | | | (_| | __/ | | | |_.__/|_|\\__| \\_/\\_/ \\__,_|_| \\__,_|\\___|_| |_| EOF cat \u0026lt;\u0026lt; EOF Open source password management solutions Copyright 2015-$(date +\u0026#39;%Y\u0026#39;), 8bit Solutions LLC https://bitwarden.com, https://github.com/bitwarden =================================================== EOF # Setup DIR=\u0026#34;$( cd \u0026#34;$( dirname \u0026#34;${BASH_SOURCE[0]}\u0026#34; )\u0026#34; \u0026amp;\u0026amp; pwd )\u0026#34; SCRIPT_NAME=$(basename \u0026#34;$0\u0026#34;) SCRIPT_PATH=\u0026#34;$DIR/$SCRIPT_NAME\u0026#34; OUTPUT=\u0026#34;$DIR/bwdata\u0026#34; if [ $# -eq 2 ] then OUTPUT=$2 fi SCRIPTS_DIR=\u0026#34;$OUTPUT/scripts\u0026#34; GITHUB_BASE_URL=\u0026#34;https://raw.githubusercontent.com/bitwarden/server/master\u0026#34; # Please do not create pull requests modifying the version numbers. COREVERSION=\u0026#34;1.45.2\u0026#34; WEBVERSION=\u0026#34;2.25.0\u0026#34; KEYCONNECTORVERSION=\u0026#34;1.0.0\u0026#34; echo \u0026#34;bitwarden.sh version $COREVERSION\u0026#34; docker --version docker-compose --version echo \u0026#34;\u0026#34; # Functions function downloadSelf() { if curl -s -w \u0026#34;http_code %{http_code}\u0026#34; -o $SCRIPT_PATH.1 $GITHUB_BASE_URL/scripts/bitwarden.sh | grep -q \u0026#34;^http_code 20[0-9]\u0026#34; then mv $SCRIPT_PATH.1 $SCRIPT_PATH chmod u+x $SCRIPT_PATH else rm -f $SCRIPT_PATH.1 fi } function downloadRunFile() { if [ ! -d \u0026#34;$SCRIPTS_DIR\u0026#34; ] then mkdir $SCRIPTS_DIR fi curl -s -o $SCRIPTS_DIR/run.sh $GITHUB_BASE_URL/scripts/run.sh chmod u+x $SCRIPTS_DIR/run.sh rm -f $SCRIPTS_DIR/install.sh } function checkOutputDirExists() { if [ ! -d \u0026#34;$OUTPUT\u0026#34; ] then echo \u0026#34;Cannot find a Bitwarden installation at $OUTPUT.\u0026#34; exit 1 fi } function checkOutputDirNotExists() { if [ -d \u0026#34;$OUTPUT/docker\u0026#34; ] then echo \u0026#34;Looks like Bitwarden is already installed at $OUTPUT.\u0026#34; exit 1 fi } function listCommands() { cat \u0026lt;\u0026lt; EOT Available commands: install start restart stop update updatedb updaterun updateself updateconf renewcert rebuild help See more at https://bitwarden.com/help/article/install-on-premise/#script-commands-reference EOT } # Commands case $1 in \u0026#34;install\u0026#34;) checkOutputDirNotExists mkdir -p $OUTPUT downloadRunFile $SCRIPTS_DIR/run.sh install $OUTPUT $COREVERSION $WEBVERSION $KEYCONNECTORVERSION ;; \u0026#34;start\u0026#34; | \u0026#34;restart\u0026#34;) checkOutputDirExists $SCRIPTS_DIR/run.sh restart $OUTPUT $COREVERSION $WEBVERSION $KEYCONNECTORVERSION ;; \u0026#34;update\u0026#34;) checkOutputDirExists downloadRunFile $SCRIPTS_DIR/run.sh update $OUTPUT $COREVERSION $WEBVERSION $KEYCONNECTORVERSION ;; \u0026#34;rebuild\u0026#34;) checkOutputDirExists $SCRIPTS_DIR/run.sh rebuild $OUTPUT $COREVERSION $WEBVERSION $KEYCONNECTORVERSION ;; \u0026#34;updateconf\u0026#34;) checkOutputDirExists $SCRIPTS_DIR/run.sh updateconf $OUTPUT $COREVERSION $WEBVERSION $KEYCONNECTORVERSION ;; \u0026#34;updatedb\u0026#34;) checkOutputDirExists $SCRIPTS_DIR/run.sh updatedb $OUTPUT $COREVERSION $WEBVERSION $KEYCONNECTORVERSION ;; \u0026#34;stop\u0026#34;) checkOutputDirExists $SCRIPTS_DIR/run.sh stop $OUTPUT $COREVERSION $WEBVERSION $KEYCONNECTORVERSION ;; \u0026#34;renewcert\u0026#34;) checkOutputDirExists $SCRIPTS_DIR/run.sh renewcert $OUTPUT $COREVERSION $WEBVERSION $KEYCONNECTORVERSION ;; \u0026#34;updaterun\u0026#34;) checkOutputDirExists downloadRunFile ;; \u0026#34;updateself\u0026#34;) downloadSelf \u0026amp;\u0026amp; echo \u0026#34;Updated self.\u0026#34; \u0026amp;\u0026amp; exit ;; \u0026#34;help\u0026#34;) listCommands ;; *) echo \u0026#34;No command found.\u0026#34; echo listCommands esac 2. Run the installer script. A ./bwdata directory will be created relative to the location of bitwarden.sh. 1 ./bitwarden.sh install 3. Complete the prompts in the installer Enter the domain name for your Bitwarden instance:\nTypically, this value should be the configured DNS record.\nDo you want to use Let\u0026rsquo;s Encrypt to generate a free SSL certificate? (y/n):\nSpecify y to generate a trusted SSL certificate using Let\u0026rsquo;s Encrypt. You will be prompted to enter an email address for expiration reminders from Let\u0026rsquo;s Encrypt. For more information, see Certificate Options. Alternatively, specify n and use the Do you have a SSL certificate to use? option.\nEnter your installation id:\nRetrieve an installation id using a valid email at https://bitwarden.com/host. For more information, see What are my installation id and installation key used for?.\nEnter your installation key:\nRetrieve an installation key using a valid email at https://bitwarden.com/host. For more information, see What are my installation id and installation key used for?.\nDo you have a SSL certificate to use? (y/n):\nIf you already have your own SSL certificate, specify y and place the necessary files in the ./bwdata/ssl/your.domain directory. You will be asked whether it is a trusted SSL certificate (y/n). For more information, see Certificate Options.\nAlternatively, specify n and use the self-signed SSL certificate? option, which is only recommended for testing purposes.\nDo you want to generate a self-signed SSL certificate? (y/n):\nSpecify y to have Bitwarden generate a self-signed certificate for you. This option is only recommended for testing. For more information, see Certificate Options.\nIf you specify n, your instance will not use an SSL certificate and you will be required to front your installation with a HTTPS proxy, or else Bitwarden applications will not function properly.\n4. Post-Install Configuration Environment Variables 1 2 3 4 5 6 7 8 9 ... globalSettings__mail__smtp__host=\u0026lt;placeholder\u0026gt; globalSettings__mail__smtp__port=\u0026lt;placeholder\u0026gt; globalSettings__mail__smtp__ssl=\u0026lt;placeholder\u0026gt; globalSettings__mail__smtp__username=\u0026lt;placeholder\u0026gt; globalSettings__mail__smtp__password=\u0026lt;placeholder\u0026gt; ... adminSettings__admins= ... Replacing globalSettings__mail__smtp...= placeholdesr will configure the SMTP Mail Server that will be used to send verification emails to new users and invitations to Organizations. Adding an email address to adminSettings__admins= will provision access to the Admin Portal.\nAfter editing global.override.env, run the following command to apply your changes:\n1 ./bitwarden.sh restart 5. Installation File The Bitwarden installation script uses settings in ./bwdata/config.yml to generate the necessary assets for installation. Some installation scenarios (e.g. installations behind a proxy with alternate ports) may require adjustments to config.yml that were not provided during standard installation.\nEdit config.yml as necessary and apply your changes by running:\n1 ./bitwarden.sh rebuild Start Bitwarden Once you\u0026rsquo;ve completed all previous steps, start your Bitwarden instance:\n1 ./bitwarden.sh start The first time you start Bitwarden it may take some time as it downloads all of the images from Docker Hub.\nVerify that all containers are running correctly:\n1 docker ps Congratulations! Bitwarden is now up and running at https://your.domain.com. Visit the web vault in your web browser to confirm that it\u0026rsquo;s working.\nYou may now register a new account and log in. You will need to have configured smtp environment variables (see Environment Variables) in order to verify the email for your new account.\nScript Commands Reference The Bitwarden installation script (bitwarden.sh or bitwarden.ps1) has the following commands available:\nPowerShell users will run the commands with a prefixed - (switch). For example .\\bitwarden.ps1 -start.\nCommand Description install Start the installer. start Start all containers. restart Restart all containers (same as start). stop Stop all containers. update Update all containers and the database. updatedb Update/initialize the database. updateself Update this main script. updateconf Update all containers without restarting the running instance. renewcert Renew certificates. rebuild Rebuild generated installation assets from config.yml. help List all commands. ","date":"2022-01-28T00:00:00Z","permalink":"https://MyLoveES.github.io/p/install-and-deploy-bitwarden-linux/","title":"Install and deploy bitwarden - Linux"},{"content":"脱离线程池，这些操作的替代方案 线程池 - 优 线程池 - 缺 方案 1. 非阻塞 2. 限流 3. 隔离 4. 任务状态监控、干预 ","date":"2021-11-25T18:40:00Z","permalink":"https://MyLoveES.github.io/p/code-without-theadpool/","title":"Code without TheadPool"},{"content":"根据优先级消费MQ消息 背景 MQ作为处理任务的消息中间件。由于任务来自于不同的场景，带有不同的优先级属性，因此消费者需要根据呀优先级对任务进行处理。\n方案 带有优先级的MQ：以 RabbitMQ 为例 1. 为什么 Kafka/Pulsar 不带有优先级概念 2. 两者的差别 Topic 优先级顺序消费 1. 通过单一任务队列限流（当前实现方案） 2. 为每个Topic分配队列，容量整体调控 ","date":"2021-11-20T18:38:00Z","permalink":"https://MyLoveES.github.io/p/mq-consume-messages-with-priority/","title":"MQ - Consume messages with Priority"},{"content":"Podman简介 podman\n和Docker的区别 安装 使用 使用 Nvidia-driver ","date":"2021-10-13T18:14:13Z","permalink":"https://MyLoveES.github.io/p/docker-substitute-podman/","title":"Docker Substitute Podman"},{"content":"如何在数据库中存储层次结构 常见场景 公司：公司 - 部门 - 子部门 人员：领导 - 员工 文件：根目录 - 文件夹 - 文件 关系：group - child 实例 {% asset_img CASE.png CASE %}\n转成树型 {% asset_img CASE_TREE.png CASE_TREE %}\n回头看 之前提到的几种方案（Adjacency_List, Path_Enumerations, Closure_Table）都能够一定程度地满足需求，但是各自具有不可避免的弊端。\nAdjacency_List: 层级查询 -\u0026gt; ∞\nPath_Enumerations: 深度限制\nClosure_Table: 空间消耗大，层级删改 -\u0026gt; ∞\nNested Sets 根据树的深度遍历对节点编号，记录首、末次访问到该节点的数字。通过比较数字或的层级结构关系。\n{% asset_img Nested_Sets_1.png Nested_Sets_1 %}\n{% asset_img Nested_Sets_2.png Nested_Sets_2 %}\nid name left right 1 DIR_A 1 30 2 DIR_B 2 11 3 DIR_C 12 13 4 file_x 24 25 5 file_y 26 27 6 file_z 28 29 7 DIR_E 3 6 8 file_o 7 8 9 file_p 9 10 10 DIR_D 14 19 11 file_m 20 21 12 file_n 22 23 13 file_l 4 5 14 file_j 15 16 15 file_k 17 18 各种情况的处理代价 增 {% asset_img ADD.jpg ADD %}\n代价：-\u0026gt; O(n)，更新会影响到其他子树\n输入：name. parent_id 执行：\n1 2 3 4 5 6 parent = $(select * from table where id = $id); insert into table(name, left, right) values($name, $parent.right, $parent.right + 1); update table set left = left + 2 where left \u0026gt;= $parent.right; update table set right = right + 2 where right \u0026gt;= $parent.right; 删 {% asset_img DEL.jpg DEL %}\n代价：-\u0026gt; O(n)，先删除节点以及子树，并对其他子树进行修改\n输入：id\n执行：\n1 2 3 4 5 6 7 current = $(select * from table where id = $id); delete from table where left \u0026gt;= $current.left and right \u0026lt;= $current.right; d = current.right - current.right + 1; update table set left = left - $d where left \u0026gt; $current.right; update table set right = right - $d where right \u0026gt; $current.right; 改 代价：-\u0026gt; O(1)\nid, other info\n执行：\n1 update table set info where id = $id; 查 查自己 代价：-\u0026gt; O(1)\n输入：id\n执行：\n1 select * from table where id = $id 查下一级 {% asset_img SEARCH_NEXT.jpg SEARCH_NEXT %}\n代价：-\u0026gt; O(n)\n输入：id 执行：\n1 2 3 4 5 6 7 select distinct Child.Node, Child.Left, Child.Right from table as child, table as parent where parent.left \u0026lt; child.left and parent.right \u0026gt; child.right -- associate Child Nodes with ancestors group by child.node, child.left, child.right having max(parent.left) = $parent.left -- Subset for those with the given Parent Node as the nearest ancestor 这类查询可以通过增加一列来简化。例如，增加depth列记录当前节点深度，或者parent_id列记录父节点（和Adjacency List混用）,但增加了维护成本 查所有子集 {% asset_img SEARCH_ALL.jpg SEARCH_ALL %}\n代价：-\u0026gt; O(n)\n输入：path\n执行：\n1 2 3 4 5 select * from table where left \u0026gt; $parent.left and right \u0026lt; $parent.right order by left asc; 移动 {% asset_img MOVE1.jpg MOVE1 %} {% asset_img MOVE2.jpg MOVE2 %}\n代价：-\u0026gt; O(n)\n输入：id, new_parent_id 执行：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 # 先执行 DEL，再执行 ADD current = $( select * from table where id = $id ); # 查询父节点 old_parent = $( select * from table where left \u0026lt; $current.left and right \u0026gt; $current.right order by left desc limit 1 ); -- 查询移动节点以及子节点的总数 node_count = $( select count(*) from table where left \u0026gt;= $current.left and right \u0026lt;= $current.right ); update table set left = left - node_count*2, where left \u0026gt; $current.right; update table set right = right - node_count*2 where right \u0026gt; $current.right; # 更新子节点 update table set left = left + ($new_parent.right - $current.left), right = right + ($new_parent.right - $current.left) where left \u0026gt;= $current.left and right \u0026lt;= $current.right $current.left = $new_parent.right; $current.right = $new_parent.right + 1; update table set left = left + node_count*2, where left \u0026gt; $current.right; update table set right = right + node_count*2 where right \u0026gt; $current.right; 总结 可以和邻接表同时使用 优点 : 消除递归操作,实现无限分组 缺点 : 增 删 改影响范围大 适用 : 强要求无限层级深度\n比较 T 增 删 改 查自己 查下一级 查所有子集 移动 适用 Adjacency List O(1) ∞ O(1) O(1) O(n) O(n) O(1) 不涉及“所有子集”操作，严格按照层级一层层地查询 Closure Table O(n) O(n) O(1) O(1) O(n) O(n) O(n) 有固定的层级深度，并且层级不多 Path O(1) O(n) O(n) O(1) O(n) O(n) O(n) 对层级深度有一定限制，并且需求对所有子集进行操作 Netsted Sets O(n) O(n) O(1) O(1) O(n) O(n) O(n) 强要求无限层级深度 ","date":"2021-08-23T18:51:00Z","permalink":"https://MyLoveES.github.io/p/storing-hierarchical-data-by-nested-sets/","title":"Storing Hierarchical data by 【Nested Sets】"},{"content":"如何在数据库中存储层次结构 常见场景 公司：公司 - 部门 - 子部门 人员：领导 - 员工 文件：根目录 - 文件夹 - 文件 关系：group - child 实例 {% asset_img CASE.png CASE %}\n转成树型 {% asset_img CASE_TREE.png CASE_TREE %}\nPath Enumerations 每条记录存整个tree path经过的node枚举\nid name path 1 DIR_A /DIR_A 2 DIR_B /DIR_A/DIR_B 3 DIR_C /DIR_A/DIR_C 4 file_x /DIR_A/file_x 5 file_y /DIR_A/file_y 6 file_z /DIR_A/file_z 7 DIR_D /DIR_A/DIR_B/DIR_D 8 file_o /DIR_A/DIR_B/file_o 9 file_p /DIR_A/DIR_B/file_p 10 DIR_E /DIR_A/DIR_C/DIR_E 11 file_m /DIR_A/DIR_C/file_m 12 file_n /DIR_A/DIR_C/file_n 13 file_l /DIR_A/DIR_B/DIR_D/file_l 14 file_j /DIR_A/DIR_C/DIR_E/file_j 15 file_k /DIR_A/DIR_C/DIR_E/file_k 各种情况的处理代价 增 代价：-\u0026gt; O(1)\n输入：name, path\n执行：\n1 insert into table(name, path) values($name, $path); id name path 13 file_l /DIR_A/DIR_B/DIR_D/file_l 14 file_j /DIR_A/DIR_C/DIR_E/file_j 15 file_k /DIR_A/DIR_C/DIR_E/file_k 16(add) file_ADD /DIR_A/DIR_C/DIR_E/file_ADD 删 {% asset_img DEL.jpg DEL %}\n代价：-\u0026gt; O(n)\n输入：path\n执行：\n1 delete from table where path like CONCAT($path, \u0026#39;%\u0026#39;) ; 改 {% asset_img UPDATE.jpg UPDATE %}\n代价：-\u0026gt; O(n)\n输入：path, other info\n执行：\n1 2 3 4 5 6 7 8 9 pre: 需要分割、拼接拿到old_path和new_path mysql 5.7: update table set info where path = $path update table set path = REPLACE( CONCAT(\u0026#39;^\u0026#39;, path), CONCAT(\u0026#39;^\u0026#39;, $old_path), $new_path) where path like CONCAT($old_path, \u0026#39;%\u0026#39;) 查 查自己 代价：-\u0026gt; O(1)\n输入：path\n执行：\n1 select * from table where path = $path 查下一级 {% asset_img SEARCH_NEXT.jpg SEARCH_NEXT %}\n代价：-\u0026gt; O(n)\n输入：id\n执行：\n1 select * from table where path regexp CONCAT(\u0026#39;^\u0026#39;, $path, \u0026#39;/.+\u0026#39;, \u0026#39;((?!/).)\u0026#39;) 查所有子集 {% asset_img SEARCH_ALL.jpg SEARCH_ALL %}\n代价：-\u0026gt; O(n)\n输入：path\n执行：\n1 select * from table where path like CONCAT($path, \u0026#39;/%\u0026#39;) 移动 {% asset_img MOVE.jpg MOVE %}\n代价：-\u0026gt; O(n)\n输入：path, new_path\n执行：\n1 2 3 pre: old_path和new_path UPDATE # 执行上面的改操作 总结 可以和邻接表同时使用\n优点 : 增加、查询、修改方便\n缺点 : 需要大量用到正则、模糊匹配；需要控制层级深度(致命)\n适用 : 对层级深度有一定限制，并且需求对所有子集进行操作的场景\n","date":"2021-08-23T18:51:00Z","permalink":"https://MyLoveES.github.io/p/storing-hierarchical-data-by-path-enumerations/","title":"Storing Hierarchical data by 【Path Enumerations】"},{"content":"如何在数据库中存储层次结构 常见场景 公司：公司 - 部门 - 子部门 人员：领导 - 员工 文件：根目录 - 文件夹 - 文件 关系：group - child 实例 {% asset_img CASE.png CASE %}\n转成树型 {% asset_img CASE_CUSTOM.png CASE_CUSTOM %}\n{% asset_img CASE_TREE.png CASE_TREE %}\nClosure Table 维护一个表，所有的tree path作为记录进行保存。\nid name 1 DIR_A 2 DIR_B 3 DIR_C 4 file_x 5 file_y 6 file_z 7 DIR_E 8 file_o 9 file_p 10 DIR_D 11 file_m 12 file_n 13 file_l 14 file_j 15 file_k current_id ancestor_id distance 2 1 1 7 1 2 7 2 1 13 1 3 13 2 2 13 7 1 \u0026hellip; \u0026hellip; \u0026hellip; 各种情况的处理代价 增 {% asset_img ADD.jpg ADD %}\n代价：-\u0026gt; O(n) 如果层级非常深，代价 -\u0026gt; ∞\n输入：name, parent_id\n执行：添加到 DIR_D 下\n1 2 3 4 5 6 7 8 9 insert into node(name) values($name); # 查所有父节点，建立关系 ids[] = {$parent_id} ids[] += select id from node where current_id=$parent_id; distance = ids.length; for (ancestor_id : ids) { insert into relation(current_id, ancestor_id, distance) values ($id, $ancestor_id, $distance); distance--; } id name 13 file_l 14 file_j 15 file_k 16(add) file_ADD current_id ancestor_id distance 16 1 4 16 3 3 16 10 2 16 13 1 删 {% asset_img DEL.jpg DEL %}\n代价：-\u0026gt; O(n)\n输入：id\n执行：\n1 2 3 4 ids[] = {$id} ids += $(select id from relation where ancestor_id = ${id}) delete from relation where ancestor_id in ${ids} or current_id in ${ids} delete from node where id in ${ids} 改 代价：-\u0026gt; O(1)\n输入：id, other info\n执行：\n1 update node set info where id = $id 查 查自己 代价：-\u0026gt; O(1)\n输入：id\n执行：\n1 select * from node where id = $id 查下一级 {% asset_img SEARCH_NEXT.jpg SEARCH_NEXT %}\n代价：-\u0026gt; O(n)\n输入：id\n执行：\n1 2 3 select * from relation left join node on node.id = relation.current_id where relation.ancestor_id = $id and distance = 1 查所有子集 {% asset_img SEARCH_ALL.jpg SEARCH_ALL %}\n代价：-\u0026gt; O(n)\n输入：id\n执行：\n1 2 3 select * from relation left join node on node.id = relation.current_id where relation.ancestor_id = $id 移动 {% asset_img MOVE.jpg MOVE %}\n代价：-\u0026gt; O(n) 输入：id, new_parent_id 执行：\n1 2 3 4 5 6 7 8 old_parent_id = select ancestor_id from relation where current_id = $id and distance = 1; DEL # 执行上面的删除操作 objects = ${object(id)} objects += $(select current_id, distance from relation where ancestor = $id) for (object : $objects) { INSERT # 执行上面的插入操作 } 总结 优点 : 修改、查询简便，效率高；\n缺点 : 空间换时间；进行删除、移动代价较大；层级深度很大的时候空间消耗巨大\n适用 : 有固定的层级深度，并且层级不多的场景\n","date":"2021-08-23T18:50:00Z","permalink":"https://MyLoveES.github.io/p/storing-hierarchical-data-by-closure-table/","title":"Storing Hierarchical data by 【Closure Table】"},{"content":"如何在数据库中存储层次结构 常见场景 公司：公司 - 部门 - 子部门 人员：领导 - 员工 文件：根目录 - 文件夹 - 文件 关系：group - child 实例 {% asset_img CASE.png CASE %}\n转成树型 {% asset_img CASE_TREE.png CASE_TREE %}\nAdjacency List 邻接表 存储当前节点的父节点信息（parent_id），通过 parent_id 相关联\nid name parent_id 1 DIR_A root 2 DIR_B 1 3 DIR_C 1 4 file_x 1 5 file_y 1 6 file_z 1 7 DIR_E 2 8 file_o 2 9 file_p 2 10 DIR_D 3 11 file_m 3 12 file_n 3 13 file_l 7 14 file_j 10 15 file_k 10 各种情况的处理代价 1 2 3 O(1)：有限的操作次数，有限的影响范围 O(n)：有限的操作次数，无限的影响范围 ∞：无限的操作次数 增 {% asset_img ADD.jpg ADD %}\n代价：-\u0026gt; O(1)\n输入：name, parent_id\n执行：\n1 insert into table(name, parent_id) values($name, $parent_id); id name parent_id 13 file_l 7 14 file_j 10 15 file_k 10 16(add) file_ADD 1 删 {% asset_img DEL.jpg DEL %}\n需要递归删除\n代价：-\u0026gt; ∞\n输入：id\n递归执行：\n1 2 3 4 5 ids[] = $id while (ids is not empty) { delete from table where id in $ids; ids = $(select id from table where parent_id in $ids); } 当然也可以只删除直接下级，但会留下“悬浮节点”\n改 代价：-\u0026gt; O(1)\n输入：id, other info\n执行：\n1 update table set info where id = $id 查 查自己 代价：-\u0026gt; O(1)\n输入：id\n执行：\n1 select * from table where id = $id 查下一级 {% asset_img SEARCH_NEXT.jpg SEARCH_NEXT %}\n代价：-\u0026gt; O(n)\n输入：id\n执行：\n1 select * from table where parent_id = $id 查所有子集 {% asset_img SEARCH_ALL.jpg SEARCH_ALL %}\n代价：-\u0026gt; ∞\n输入：id\n执行：\n1 2 3 4 5 6 ids[] = $id sub_ids[] = $id while (sub_ids is not empty) { sub_ids = $(select id from table where id in sub_ids); ids += sub_ids; } 移动 {% asset_img MOVE.jpg MOVE %}\n代价：-\u0026gt; O(1)\n输入：id, new_parent_id\n执行：\n1 update table set parent_id = $new_parent_id where id = $id; 总结 优点 : 进行增加、修改、移动时代价很低; 查询自己、直接下级非常方便\n缺点 : 如若需要使用层级结构，例如获取所有子目录，所有下级，代价趋近∞（致命）\n适用 : 不涉及“所有子集”，严格按照层级一层层地查询\n","date":"2021-08-07T23:38:00Z","permalink":"https://MyLoveES.github.io/p/storing-hierarchical-data-by-adjacency-list/","title":"Storing Hierarchical data by 【Adjacency List】"},{"content":"线段树 何为线段树 构造线段树 查询线段树 更新线段树 LEETCODE PROBLEM ","date":"2021-07-20T15:44:10Z","permalink":"https://MyLoveES.github.io/p/algorithm-%E7%BA%BF%E6%AE%B5%E6%A0%91/","title":"Algorithm - 线段树"},{"content":"1818. 绝对差值和 排序 \u0026amp; 二分查找，找到差距最大的那一项\n改变前后的差值为： |nums1[i] - nums2[i]| - |nums1[j] - nums2[i]|\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class Solution { public int minAbsoluteSumDiff(int[] nums1, int[] nums2) { final int MOD = 1000000007; int n = nums1.length; int[] rec = new int[n]; System.arraycopy(nums1, 0, rec, 0, n); Arrays.sort(rec); int sum = 0, maxGap = 0; for (int i = 0; i \u0026lt; n; i++) { int gap = Math.abs(nums1[i] - nums2[i]); sum = (gap + sum) % MOD; int nearOne = getHigherPos(rec, nums2[i]); if (nearOne \u0026lt; n) { maxGap = Math.max(maxGap, gap - (rec[nearOne] - nums2[i])); } if (nearOne \u0026gt; 0) { maxGap = Math.max(maxGap, gap - (nums2[i] - rec[nearOne - 1])); } } return (sum - maxGap + MOD) % MOD; } private int getHigherPos(int[] arr, int target) { int low = 0, high = arr.length - 1; if (arr[high] \u0026lt; target) { return high + 1; } else { while (low \u0026lt; high) { int mid = (high - low) / 2 + low; if (arr[mid] \u0026gt; target) { high = mid; } else { low = mid + 1; } } } return low; } } ","date":"2021-07-15T22:04:20Z","permalink":"https://MyLoveES.github.io/p/leetcode_1818_minimum_absolute_sum_difference/","title":"Leetcode_1818_Minimum_Absolute_Sum_Difference"},{"content":"274. H-Index Given an array of integers citations where citations[i] is the number of citations a researcher received for their ith paper, return compute the researcher\u0026rsquo;s h-index.\nAccording to the definition of h-index on Wikipedia: A scientist has an index h if h of their n papers have at least h citations each, and the other n − h papers have no more than h citations each.\nIf there are several possible values for h, the maximum one is taken as the h-index.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Solution { public int hIndex(int[] citations) { int pageCount = citations.length; int[] pageRef = new int[pageCount + 1]; for (int i = 0; i \u0026lt; pageCount; i++) { if (citations[i] \u0026gt; pageCount) { pageRef[pageCount]++; } else { pageRef[citations[i]]++; } } int totlePage = 0; for (int i = pageCount; i \u0026gt;= 0; i--) { totlePage += pageRef[i]; if (totlePage \u0026gt;= i) { return i; } } return 0; } } ","date":"2021-07-11T21:41:00Z","permalink":"https://MyLoveES.github.io/p/leetcode_274_h_index/","title":"Leetcode_274_H_Index"},{"content":"981. Time Based Key-Value Store Design a time-based key-value data structure that can store multiple values for the same key at different time stamps and retrieve the key\u0026rsquo;s value at a certain timestamp.\nImplement the TimeMap class:\nTimeMap() Initializes the object of the data structure. void set(String key, String value, int timestamp) Stores the key key with the value value at the given time timestamp. String get(String key, int timestamp) Returns a value such that set was called previously, with timestamp_prev \u0026lt;= timestamp. If there are multiple such values, it returns the value associated with the largest timestamp_prev. If there are no values, it returns \u0026ldquo;\u0026rdquo;. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class TimeMap { HashMap\u0026lt;String, TreeMap\u0026lt;Integer, String\u0026gt;\u0026gt; map; /** Initialize your data structure here. */ public TimeMap() { map = new HashMap(); } public void set(String key, String value, int timestamp) { map.computeIfAbsent(key, k -\u0026gt; new TreeMap\u0026lt;\u0026gt;()).put(timestamp, value); } public String get(String key, int timestamp) { if (!map.containsKey(key)) { return \u0026#34;\u0026#34;; } Integer time = map.get(key).floorKey(timestamp); if (time == null) { return \u0026#34;\u0026#34;; } return map.get(key).get(time); } } ","date":"2021-07-10T17:30:10Z","permalink":"https://MyLoveES.github.io/p/leetcode_981_time_based_key_value_store.md/","title":"Leetcode_981_Time_Based_Key_Value_Store.md"},{"content":"451. Sort Characters By Frequency Given a string s, sort it in decreasing order based on the frequency of characters, and return the sorted string.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Solution { public String frequencySort(String s) { Map\u0026lt;Character, Integer\u0026gt; cm = new HashMap(); for (int i = 0; i \u0026lt; s.length(); i++) { char c = s.charAt(i); int count = cm.getOrDefault(c, 0) + 1; cm.put(c, count); } List\u0026lt;Character\u0026gt; keyList = new ArrayList(cm.keySet()); Collections.sort(keyList, (a,b) -\u0026gt; cm.get(b) - cm.get(a)); StringBuilder sb = new StringBuilder(); for (Character c : keyList) { int freq = cm.get(c); for (int i = 0; i \u0026lt; freq; i++) { sb.append(c); } } return sb.toString(); } } ","date":"2021-07-02T21:44:10Z","permalink":"https://MyLoveES.github.io/p/leetcode_645_set_mismatch/","title":"Leetcode_645_Set_Mismatch"},{"content":"451. Sort Characters By Frequency Given a string s, sort it in decreasing order based on the frequency of characters, and return the sorted string.\n排序 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Solution { public String frequencySort(String s) { Map\u0026lt;Character, Integer\u0026gt; cm = new HashMap\u0026lt;Character, Integer\u0026gt;(); int maxFreq = 0; for (int i = 0; i \u0026lt; s.length(); i++) { char c = s.charAt(i); int freq = cm.getOrDefault(c, 0) + 1; cm.put(c, freq); maxFreq = Math.max(freq, maxFreq); } List\u0026lt;Character\u0026gt; list = new ArrayList\u0026lt;Character\u0026gt;(cm.keySet()); Collections.sort(list, (a, b) -\u0026gt; cm.get(b) - cm.get(a)); StringBuilder s = new StringBuilder(); for (Character c : list) { int count = cm.get(c); for (int i = 0; i \u0026lt; count; i++) { s.append(c); } } return s.toString(); } } 桶排序 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class Solution { public String frequencySort(String s) { Map\u0026lt;Character, Integer\u0026gt; cm = new HashMap\u0026lt;Character, Integer\u0026gt;(); int maxFreq = 0; for (int i = 0; i \u0026lt; s.length(); i++) { char c = s.charAt(i); int freq = cm.getOrDefault(c, 0) + 1; cm.put(c, freq); maxFreq = Math.max(freq, maxFreq); } StringBuffer[] buckets = new StringBuffer[maxFreq + 1]; for (int i = 0; i \u0026lt;= maxFreq; i++) { buckets[i] = new StringBuffer(); } for (Map.Entry\u0026lt;Character, Integer\u0026gt; entry : cm.entrySet()) { buckets[entry.getValue()].append(entry.getKey()); } StringBuilder sb = new StringBuilder(); for (int i = maxFreq; i \u0026gt; 0 ; i--) { String str = buckets[i].toString(); for (int j = 0; j \u0026lt; str.length(); j++) { for (int k = 0; k \u0026lt; i; k++) { sb.append(str.charAt(j)); } } } return sb.toString(); } } ","date":"2021-07-02T10:52:10Z","permalink":"https://MyLoveES.github.io/p/leetcode_451_sort_characters_by_frequency/","title":"Leetcode_451_Sort_Characters_By_Frequency"},{"content":"191. 位1的个数 方法二：位运算优化 思路及解法\n观察这个运算：n \u0026amp; (n−1)，其运算结果恰为把 n 的二进制位中的最低位的 1 变为 0 之后的结果。 ​ ，运算结果 4 即为把 6 的二进制位中的最低位的 1 变为 0 之后的结果。\n这样我们可以利用这个位运算的性质加速我们的检查过程，在实际代码中，我们不断让当前的 n 与 n - 1 做与运算，直到 n 变为 0 即可。因为每次运算会使得 n 的最低位的 1 被翻转，因此运算次数就等于 n 的二进制位中 1 的个数。\n1 2 3 4 5 6 7 8 9 10 11 public class Solution { // you need to treat n as an unsigned value public int hammingWeight(int n) { int r = 0; while (n != 0) { n \u0026amp;= n - 1; r++; } return r; } } ","date":"2021-06-25T23:28:10Z","permalink":"https://MyLoveES.github.io/p/leetcode_203_remove_linked_list_elements/","title":"Leetcode_203_Remove_Linked_List_Elements"},{"content":"多重 背包问题 1. 多重背包 有N种物品和一个容量为T的背包，第i种物品最多有M[i]件可用，价值为P[i]，体积为V[i]，求解：选哪些物品放入背包，可以使得这些物品的价值最大，并且体积总和不超过背包容量。\n对比一下完全背包，其实只是多了一个限制条件，完全背包问题中，物品可以选择任意多件，只要你装得下，装多少件都行。但多重背包就不一样了，每种物品都有指定的数量限制，所以不是你想装，就能一直装的。举个栗子，有A、B、C三种物品，相应的数量、价格和占用空间如下图：\n{% asset_img package_multi_init.png package_multi_init %}\n2. 解决 分治 递推公式：\nks(i,t) = max{ks(i-1, t - V[i] * k) + P[i] * k}; （0 \u0026lt;= k \u0026lt;= M[i] \u0026amp;\u0026amp; 0 \u0026lt;= k * V[i] \u0026lt;= t）\n完全背包递推公式：\nks(i,t) = max{ks(i-1, t - V[i] * k) + P[i] * k}; （ 0 \u0026lt;= k * V[i] \u0026lt;= t）\n可见两者之间只差了一个限制条件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 public int solution(int [] m, int[] w, int[] v, int c) { return cal(m, w, v, w.length, c); } public int cal(int[] m int[] w, int[] v, int i, int j) { if (i \u0026lt; 0 || j \u0026lt;= 0) { return 0; } int result = 0; if (w[i] \u0026gt; j) { result = cal(w, v, i - 1, j); } else if (i \u0026lt; 0) { result = v[i] * Math.floor(j / w[i]); } else { for (int k = 0; k \u0026lt;= m[i] \u0026amp;\u0026amp; k * w[i] \u0026lt; j; k++) { int tmp = cal(w, v, i - 1, j - k * w[i]) + k * v[i]; result = Math.max(result, tmp); } } return result; } 动态规划 递归 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 public int solution(int[] m, int[] w, int[] v, int c) { int[][] result = new int[][]; return cal(m, w, v, result, w.length, c); } public int cal(int[] m, int[] w, int[] v, int[][] result, int i, int j) { if (result[i][j] != null) { return result[i][j]; } if (i \u0026lt; 0 || j \u0026lt;= 0) { return 0; } else { if (w[i] \u0026gt; j) { result[i][j] = cal(w, v, result, i - 1, j); } else { for (int k = 0; k \u0026lt;= m[i] \u0026amp;\u0026amp; k * w[i] \u0026lt;= j; k++) { int tmp = cal(w, v, result, i, j - k * w[i]); result[i][j] = Math.max(result[i][j], tmp); } } } } 迭代 1 2 3 4 5 6 7 8 9 10 11 12 public int solution(int[] m, int[] w, int[] v, int c) { // 初始化为 r = 0 / r[i][0] = 0 int[][] result = new int[][]; for (int i = 1; i \u0026lt; w.length; i++) { for (int j = 0; j \u0026lt;= c; j++) { for (int k = 0; k \u0026lt;= m[i] \u0026amp;\u0026amp; k * w[i] \u0026lt;= j; k++) { result[i][j] = Math.max(result[i][j], result[i - 1][j - k * w[i]] + k * v[i]); } } } return result[w.length][c]; } 空间优化 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public int solution(int w[], int v[], int c) { // 初始化为 r = 0 / r[0] = 0 int[] result = new int[]; for (int i = 0; i \u0026lt; w.length; i++) { if (m[i] * w[i] \u0026gt;= c) { for (int j = w[i]; j \u0026lt;= c; j++) { result[j] = Math.max(result[j], result[j - w[i]] + v[i]); } } else { for (int j = c; j \u0026gt;= w[i]; j--) { for (int k = 1; k \u0026lt;= m[i] \u0026amp;\u0026amp; k * w[i] \u0026lt;= j) { result[j] = Math.max(result[j], result[j - w[i] * k] + v[i] * k); } } } } return result[c]; } 这里有一个较大的不同点，在第二层循环中，需要分两种情况考虑，如果 M[m] * V[m] \u0026gt;= T ，那么第m个物品就可以当做完全背包问题来考虑，而如果 M[m] * V[m] \u0026lt; T，则每次选择时，需要从 newResults[n-V[m]*k] + P[m] * k（0 \u0026lt;= k \u0026lt;= M[m]）中找到最大值。\n{% asset_img package_multi_space.png package_multi_space %}\n多重背包问题同样也可以转化成01背包问题来求解，因为第i件物品最多选 M[i] 件，于是可以把第i种物品转化为M[i]件体积和价值相同的物品，然后再来求解这个01背包问题。\n原文作者：弗兰克的猫\n原文链接：https://www.cnblogs.com/mfrank/p/10533701.html\n","date":"2021-06-18T14:39:13Z","permalink":"https://MyLoveES.github.io/p/algorithm-%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98_%E5%A4%9A%E9%87%8D%E8%83%8C%E5%8C%85/","title":"Algorithm-背包问题_多重背包"},{"content":"完全 背包问题 1. 完全背包 有N种物品和一个容量为T的背包，每种物品都就可以选择任意多个，第i种物品的价值为P[i]，体积为V[i]，求解：选哪些物品放入背包，可卡因使得这些物品的价值最大，并且体积总和不超过背包容量。\n跟01背包一样，完全背包也是一个很经典的动态规划问题，不同的地方在于01背包问题中，每件物品最多选择一件，而在完全背包问题中，只要背包装得下，每件物品可以选择任意多件。从每件物品的角度来说，与之相关的策略已经不再是选或者不选了，而是有取0件、取1件、取2件\u0026hellip;直到取⌊T/Vi⌋（向下取整）件。\n2. 分析 a. 贪心？ 根据性价比排序，性价比从高到底取。\n结论：不可行\n反例：如果一个背包的容量为10，有三件物品可供选择，分别是\n物品 价值 体积 A 5 5 B 8 7 按照性价比，必然取一个B。但是取两个A明显收益更高。因此贪心不可行。 3. 解决 分治 和之前相似，直接递归即可\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 public int solution(int[] w, int[] v, int c) { return cal(w, v, w.length, c); } public int cal(int w[], int v[], int i, int j) { if (i \u0026lt; 0 || j \u0026lt;= 0) { return 0; } int result = 0; if (w[i] \u0026gt; j) { result = cal(w, v, i - 1, j); } else if (i \u0026lt; 0) { result = v[i] * Math.floor(j / w[i]); } else { for (int k = 0; k * w[i] \u0026lt; j; k++) { int tmp = cal(w, v, i - 1, j - k * w[i]) + k * v[i]; result = Math.max(result, tmp); } } return result; } 动态规划 递归 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 public int solution(int w[], int v[], int c) { int[][] result = new int[][]; return cal(w, v, result, w.length, c); } public int cal(int w[], int v[], int[][] result, int i, int j) { if (result[i][j] != null) { return result[i][j]; } if (i \u0026lt; 0 || j \u0026lt;= 0) { return 0; } else { if (w[i] \u0026gt; j) { result[i][j] = cal(w, v, result, i - 1, j); } else { for (int k = 0; k * w[i] \u0026lt;= j; k++) { int tmp = cal(w, v, result, i, j - k * w[i]); result[i][j] = Math.max(result[i][j], tmp); } } } } 迭代 1 2 3 4 5 6 7 8 9 10 11 12 public int solution(int w[], int v[], int c) { // 初始化为 r = 0 / r[i][0] = 0 int[][] result = new int[][]; for (int i = 1; i \u0026lt; w.length; i++) { for (int j = 0; j \u0026lt;= c; j++) { for (int k = 0; k * w[i] \u0026lt;= j; k++) { result[i][j] = Math.max(result[i][j], result[i - 1][j - k * w[i]] + k * v[i]); } } } return result[w.length][c]; } 空间优化 1 2 3 4 5 6 7 8 9 10 public int solution(int w[], int v[], int c) { // 初始化为 r = 0 / r[0] = 0 int[] result = new int[]; for (int i = 0; i \u0026lt; w.length; i++) { for (int j = w[i]; j \u0026lt;= c; j++) { result[j] = Math.max(result[j], result[j - w[i]] + v[i]); } } return result[c]; } 原文作者：弗兰克的猫\n原文链接：https://www.cnblogs.com/mfrank/p/10533701.html\n","date":"2021-06-08T17:52:13Z","permalink":"https://MyLoveES.github.io/p/algorithm-%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98_%E5%AE%8C%E5%85%A8%E8%83%8C%E5%8C%85/","title":"Algorithm-背包问题_完全背包"},{"content":"0-1 背包问题 1. 动态规划问题，最好具备两个前提： 最优化原理 最优化原理指的最优策略具有这样的性质：不论过去状态和决策如何，对前面的决策所形成的状态而言，余下的诸决策必须构成最优策略。简单来说就是一个最优策略的子策略也是必须是最优的，而所有子问题的局部最优解将导致整个问题的全局最优。如果一个问题能满足最优化原理，就称其具有最优子结构性质。\n无后效性 无后效性指的是某状态下决策的收益，只与状态和决策相关，与到达该状态的方式无关。某个阶段的状态一旦确定，则此后过程的演变不再受此前各种状态及决策的影响。换句话说，未来与过去无关，当前状态是此前历史状态的完整总结，此前历史决策只能通过影响当前的状态来影响未来的演变。再换句话说，过去做的选择不会影响现在能做的最优选择，现在能做的最优选择只与当前的状态有关，与经过如何复杂的决策到达该状态的方式无关。\n2. 分析 你只有一个容量有限的背包，总容量为c，有n个可待选择的物品，每个物品只有一件，它们都有各自的重量和价值，你需要从中选择合适的组合来使得你背包中的物品总价值最大。\n抽象问题，可以描述为：\n有一个容量为c的背包，总共有n个物品，每个物品的体积为wi，价值为vi，求该背包能容下的最大价值。（每个物品只取一次）\n使用xi来标志第i个物品是否放到背包里，则：\nx1w1 + x2w2 + x3w3 + \u0026hellip; + xnwn \u0026lt;= c Max(x1v1 + x2v2 + x3v3 + \u0026hellip; + xnvn) = result 根据上面所说的两个前提，可以得到一个公式：\nif wi \u0026gt; c, result[i, j] = result[i-1, j] else result[i, j] = max(result[i-1, j], result[i, j-w[i]] + v[i]) 3. 解决 a. 分治 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 public int solution(int[] w, int[] v, int c) { return cal(w, v, w.length, c); } /** * 当背包容量为j时，第i件物品的决策 */ public int cal(int[] w, int[] v, int i, int j) { // 首先定义返回时机 if (i \u0026lt; 0 || j \u0026lt;= 0 ) { return 0; } int result = 0; if (w[i] \u0026gt; j) { result = cal(w, v, i - 1, j); } else { result = Math.max(cal(w, v, i - 1, j), cal(w, v, i - 1, j - w[i]) + v[i]); } } b. 动态规划 递归 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 public int solution(int[] w, int[] v, int c) { // 初始化：r = 0 / (必须恰好装满) r[i][0] = 0 int [][] r = new int [][]; return cal(w, v, w.length, c, r); } /** * 当背包容量为j时，第i件物品的决策 */ public int cal(int[] w, int[] v, int i, int j, int[][] r) { if (i \u0026lt; 0 || j \u0026lt;= 0) { return 0; } // 首先定义返回时机 if (r[i][j] != null) { return r[i][j]; } int result = 0; // 装不下，则存储不装该物品的最大值 if (w[i] \u0026gt; j) { result = cal(w, v, i - 1, j); } else { // 否则，存储装或不装的最大值 result = Math.max(cal(w, v, i - 1, j), cal(w, v, i - 1, j - w[i]) + v[i]); } r[i][j] = result; return result; } 迭代 1 2 3 4 5 6 7 8 9 10 11 12 13 14 public int solution(int[] w, int[] v, int c) { // 初始化：r = 0 / (必须恰好装满) r[i][0] = 0 int [][] r = new int [][]; for (int i = 1; i \u0026lt; w.length; i++) { for (int j = 0; j \u0026lt; c; j++) { if (w[i] \u0026gt; j) { r[i][j] = r[i - 1][j]; } else { r[i][j] = Math.max(r[i - 1][j], r[i][j - w[i]] + v[i]); } } } return r[w.length][c]; } 空间优化 result[j] = max(result[j], result[j - w[i]] + v[i]); 不过此处要注意，需要从后往前遍历。原因是，如果从前往后遍历的话，可能会导致原值被改变，后面再取用的时候，就会导致错误。\n例如，当计算 i=3 的时候，r[5] = Math.max(r[5], r[5 - w[3]] + v[3])。当进行后续计算的时候，如果再用到r[5]，那就不再是从 i=2 计算过来的原值了，发生错误。\n1 2 3 4 5 6 7 8 9 10 public int solution(int[] w, int[] v, int c) { // 初始化：r = 0 / (必须恰好装满) r[0] = 0 int[] r = new int[]; for (int i = 0; i \u0026lt; w.length; i++) { for (int j = c; j \u0026gt;= w[i]; j--) { r[j] = Math.max(r[j], r[j - w[i]] + v[i]); } } return r[r.length - 1]; } “恰好” 装满 区别在于初始值（对于一维二维都是）。如果需要恰好装满，那么除了j=0以外，初始值应该是负无穷大。如果不需要恰好装满，初始值都为0即可。\n因为如果要求恰好装满，那么只有j=0的初始状态是合法的。从别的通道得出的依然会是负无穷大。\n{% asset_img package01.jpeg package01 %}\n原文作者：弗兰克的猫\n原文链接：https://www.cnblogs.com/mfrank/p/10533701.html\n","date":"2021-06-06T17:52:13Z","permalink":"https://MyLoveES.github.io/p/algorithm-%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98_0-1/","title":"Algorithm-背包问题_0-1"},{"content":"203. 移除链表元素 1. des 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 给你一个链表的头节点 head 和一个整数 val ，请你删除链表中所有满足 Node.val == val 的节点，并返回 新的头节点 。 示例 1： 输入：head = [1,2,6,3,4,5,6], val = 6 输出：[1,2,3,4,5] 示例 2： 输入：head = [], val = 1 输出：[] 示例 3： 输入：head = [7,7,7,7], val = 7 输出：[] 提示： 列表中的节点在范围 [0, 104] 内 1 \u0026lt;= Node.val \u0026lt;= 50 0 \u0026lt;= k \u0026lt;= 50 2. code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 /** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode() {} * ListNode(int val) { this.val = val; } * ListNode(int val, ListNode next) { this.val = val; this.next = next; } * } */ class Solution { public ListNode removeElements(ListNode head, int val) { if (head == null) { return head; } while (head != null \u0026amp;\u0026amp; head.val = val) { head = head.next; } ListNode node = head; while (node != null) { if (node.next != null \u0026amp;\u0026amp; node.next.val = val) { node.next = node.next.next; } node = node.next; } return head; } } class Solution { public ListNode removeElements(ListNode head, int val) { if (head == null) { return head; } head.next = removeElements(head.next, val); return head.val == val ? head.next : head; } } ","date":"2021-06-06T17:02:13Z","permalink":"https://MyLoveES.github.io/p/leetcode_203_%E7%A7%BB%E9%99%A4%E9%93%BE%E8%A1%A8%E5%85%83%E7%B4%A0/","title":"LeetCode_203_移除链表元素"},{"content":"Jayway JsonPath A Java DSL for reading JSON documents.\nGetting Started JsonPath is available at the Central Maven Repository. Maven users add this to your POM.\n1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.jayway.jsonpath\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;json-path\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.6.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; If you need help ask questions at Stack Overflow. Tag the question \u0026lsquo;jsonpath\u0026rsquo; and \u0026lsquo;java\u0026rsquo;.\nJsonPath expressions always refer to a JSON structure in the same way as XPath expression are used in combination with an XML document. The \u0026ldquo;root member object\u0026rdquo; in JsonPath is always referred to as $ regardless if it is an object or array.\nJsonPath expressions can use the dot–notation\n$.store.book[0].title\nor the bracket–notation\n$['store']['book'][0]['title']\n1 2 [ dot-notation 点语法 ] [ bracket–notation 括号语法 ] Operators Operator Description $ The root element to query. This starts all path expressions. @ The current node being processed by a filter predicate. * Wildcard. Available anywhere a name or numeric are required. .. Deep scan. Available anywhere a name is required. .\u0026lt;name\u0026gt; Dot-notated child ['\u0026lt;name\u0026gt;' (, '\u0026lt;name\u0026gt;')] Bracket-notated child or children [\u0026lt;number\u0026gt; (, \u0026lt;number\u0026gt;)] Array index or indexes [start:end] Array slice operator [?(\u0026lt;expression\u0026gt;)] Filter expression. Expression must evaluate to a boolean value. Functions Functions can be invoked at the tail end of a path - the input to a function is the output of the path expression. The function output is dictated by the function itself.\n函数可以在path的尾端调用，函数的输入是path表达式的输出，函数输出由函数本身决定。\nFunction Description Output type min() Provides the min value of an array of numbers Double max() Provides the max value of an array of numbers Double avg() Provides the average value of an array of numbers Double stddev() Provides the standard deviation value of an array of numbers （标准差） Double length() Provides the length of an array Integer sum() Provides the sum value of an array of numbers Double keys() Provides the property keys (An alternative for terminal tilde ~) Set\u0026lt;E\u0026gt; concat(X) Provides a concatinated version of the path output with a new item like input append(X) add an item to the json path output array like input Filter Operators Filters are logical expressions used to filter arrays. A typical filter would be [?(@.age \u0026gt; 18)] where @ represents the current item being processed. More complex filters can be created with logical operators \u0026amp;\u0026amp; and ||. String literals must be enclosed by single or double quotes ([?(@.color == 'blue')] or [?(@.color == \u0026quot;blue\u0026quot;)]).\nOperator Description == left is equal to right (note that 1 is not equal to \u0026lsquo;1\u0026rsquo;) != left is not equal to right \u0026lt; left is less than right \u0026lt;= left is less or equal to right \u0026gt; left is greater than right \u0026gt;= left is greater than or equal to right =~ left matches regular expression [?(@.name =~ /foo.*?/i)] in left exists in right [?(@.size in [\u0026lsquo;S\u0026rsquo;, \u0026lsquo;M\u0026rsquo;])] nin left does not exists in right subsetof left is a subset of right [?(@.sizes subsetof [\u0026lsquo;S\u0026rsquo;, \u0026lsquo;M\u0026rsquo;, \u0026lsquo;L\u0026rsquo;])] anyof left has an intersection with right [?(@.sizes anyof [\u0026lsquo;M\u0026rsquo;, \u0026lsquo;L\u0026rsquo;])] noneof left has no intersection with right [?(@.sizes noneof [\u0026lsquo;M\u0026rsquo;, \u0026lsquo;L\u0026rsquo;])] size size of left (array or string) should match right empty left (array or string) should be empty 1 2 A has an intersection with B; A 与 B 有交集 Path Examples Given the json\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 { \u0026#34;store\u0026#34;: { \u0026#34;book\u0026#34;: [ { \u0026#34;category\u0026#34;: \u0026#34;reference\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Nigel Rees\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Sayings of the Century\u0026#34;, \u0026#34;price\u0026#34;: 8.95 }, { \u0026#34;category\u0026#34;: \u0026#34;fiction\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Evelyn Waugh\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Sword of Honour\u0026#34;, \u0026#34;price\u0026#34;: 12.99 }, { \u0026#34;category\u0026#34;: \u0026#34;fiction\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Herman Melville\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Moby Dick\u0026#34;, \u0026#34;isbn\u0026#34;: \u0026#34;0-553-21311-3\u0026#34;, \u0026#34;price\u0026#34;: 8.99 }, { \u0026#34;category\u0026#34;: \u0026#34;fiction\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;J. R. R. Tolkien\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;The Lord of the Rings\u0026#34;, \u0026#34;isbn\u0026#34;: \u0026#34;0-395-19395-8\u0026#34;, \u0026#34;price\u0026#34;: 22.99 } ], \u0026#34;bicycle\u0026#34;: { \u0026#34;color\u0026#34;: \u0026#34;red\u0026#34;, \u0026#34;price\u0026#34;: 19.95 } }, \u0026#34;expensive\u0026#34;: 10 } JsonPath (click link to try) Result $.store.book[*].author The authors of all books $..author All authors $.store.* All things, both books and bicycles $.store..price The price of everything $..book[2] The third book $..book[-2] The second to last book $..book[0,1] The first two books $..book[:2] All books from index 0 (inclusive) until index 2 (exclusive) $..book[1:2] All books from index 1 (inclusive) until index 2 (exclusive) $..book[-2:] Last two books $..book[2:] Book number two from tail $..book[?(@.isbn)] All books with an ISBN number $.store.book[?(@.price \u0026lt; 10)] All books in store cheaper than 10 $..book[?(@.price \u0026lt;= $[\u0026rsquo;expensive\u0026rsquo;])] All books in store that are not \u0026ldquo;expensive\u0026rdquo; $..book[?(@.author =~ /.*REES/i)] All books matching regex (ignore case) $..* Give me every thing $..book.length() The number of books Reading a Document The simplest most straight forward way to use JsonPath is via the static read API.\n使用 JsonPath 最简单最直接的方法是通过静态读取 API。\n1 2 3 String json = \u0026#34;...\u0026#34;; List\u0026lt;String\u0026gt; authors = JsonPath.read(json, \u0026#34;$.store.book[*].author\u0026#34;); If you only want to read once this is OK. In case you need to read an other path as well this is not the way to go since the document will be parsed every time you call JsonPath.read(\u0026hellip;). To avoid the problem you can parse the json first.\n如果您只想阅读一次，这是可以的。如果您还需要读取其他路径，这不是可行的方法，因为每次调用 JsonPath.read(\u0026hellip;) 时都会解析文档。为了避免这个问题，你可以先解析json。\n1 2 3 4 5 String json = \u0026#34;...\u0026#34;; Object document = Configuration.defaultConfiguration().jsonProvider().parse(json); String author0 = JsonPath.read(document, \u0026#34;$.store.book[0].author\u0026#34;); String author1 = JsonPath.read(document, \u0026#34;$.store.book[1].author\u0026#34;); JsonPath also provides a fluent API. This is also the most flexible one.\nJsonPath 还提供了流畅的 API。这也是最灵活的一种。\n1 2 3 4 5 6 7 8 9 10 11 String json = \u0026#34;...\u0026#34;; ReadContext ctx = JsonPath.parse(json); List\u0026lt;String\u0026gt; authorsOfBooksWithISBN = ctx.read(\u0026#34;$.store.book[?(@.isbn)].author\u0026#34;); List\u0026lt;Map\u0026lt;String, Object\u0026gt;\u0026gt; expensiveBooks = JsonPath .using(configuration) .parse(json) .read(\u0026#34;$.store.book[?(@.price \u0026gt; 10)]\u0026#34;, List.class); What is Returned When? When using JsonPath in java its important to know what type you expect in your result. JsonPath will automatically try to cast the result to the type expected by the invoker.\n在 java 中使用 JsonPath 时，重要的是要知道您在结果中期望什么类型。 JsonPath 将自动尝试将结果转换为调用者期望的类型。\n1 2 3 4 5 //Will throw an java.lang.ClassCastException List\u0026lt;String\u0026gt; list = JsonPath.parse(json).read(\u0026#34;$.store.book[0].author\u0026#34;) //Works fine String author = JsonPath.parse(json).read(\u0026#34;$.store.book[0].author\u0026#34;) When evaluating a path you need to understand the concept of when a path is definite. A path is indefinite if it contains:\n你需要理解何为绝对，何为相对\n.. - a deep scan operator ?(\u0026lt;expression\u0026gt;) - an expression [\u0026lt;number\u0026gt;, \u0026lt;number\u0026gt; (, \u0026lt;number\u0026gt;)] - multiple array indexes Indefinite paths always returns a list (as represented by current JsonProvider).\n相对path常常返回list\nBy default a simple object mapper is provided by the MappingProvider SPI. This allows you to specify the return type you want and the MappingProvider will try to perform the mapping. In the example below mapping between Long and Date is demonstrated.\n你可以指定期望的返回类型，mapper会尝试进行映射\n1 2 3 String json = \u0026#34;{\\\u0026#34;date_as_long\\\u0026#34; : 1411455611975}\u0026#34;; Date date = JsonPath.parse(json).read(\u0026#34;$[\u0026#39;date_as_long\u0026#39;]\u0026#34;, Date.class); If you configure JsonPath to use JacksonMappingProvider or GsonMappingProvider you can even map your JsonPath output directly into POJO\u0026rsquo;s.\n1 Book book = JsonPath.parse(json).read(\u0026#34;$.store.book[0]\u0026#34;, Book.class); To obtain full generics type information, use TypeRef.\n要获取完整的泛型类型信息，请使用 TypeRef。\n1 2 3 TypeRef\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; typeRef = new TypeRef\u0026lt;List\u0026lt;String\u0026gt;\u0026gt;() {}; List\u0026lt;String\u0026gt; titles = JsonPath.parse(JSON_DOCUMENT).read(\u0026#34;$.store.book[*].title\u0026#34;, typeRef); Predicates # 判断 There are three different ways to create filter predicates in JsonPath.\nInline Predicates Inline predicates are the ones defined in the path.\n1 2 List\u0026lt;Map\u0026lt;String, Object\u0026gt;\u0026gt; books = JsonPath.parse(json) .read(\u0026#34;$.store.book[?(@.price \u0026lt; 10)]\u0026#34;); You can use \u0026amp;\u0026amp; and || to combine multiple predicates [?(@.price \u0026lt; 10 \u0026amp;\u0026amp; @.category == 'fiction')] , [?(@.category == 'reference' || @.price \u0026gt; 10)].\nYou can use ! to negate a predicate [?(!(@.price \u0026lt; 10 \u0026amp;\u0026amp; @.category == 'fiction'))].\nFilter Predicates Predicates can be built using the Filter API as shown below:\n1 2 3 4 5 6 7 8 9 10 11 12 import static com.jayway.jsonpath.JsonPath.parse; import static com.jayway.jsonpath.Criteria.where; import static com.jayway.jsonpath.Filter.filter; ... ... Filter cheapFictionFilter = filter( where(\u0026#34;category\u0026#34;).is(\u0026#34;fiction\u0026#34;).and(\u0026#34;price\u0026#34;).lte(10D) ); List\u0026lt;Map\u0026lt;String, Object\u0026gt;\u0026gt; books = parse(json).read(\u0026#34;$.store.book[?]\u0026#34;, cheapFictionFilter); Notice the placeholder ? for the filter in the path. When multiple filters are provided they are applied in order where the number of placeholders must match the number of provided filters. You can specify multiple predicate placeholders in one filter operation [?, ?], both predicates must match.\n\u0026lsquo;?\u0026lsquo;的数目 必须与过滤器的数目相匹配\nFilters can also be combined with \u0026lsquo;OR\u0026rsquo; and \u0026lsquo;AND\u0026rsquo;\n1 2 3 4 5 6 7 Filter fooOrBar = filter( where(\u0026#34;foo\u0026#34;).exists(true)).or(where(\u0026#34;bar\u0026#34;).exists(true) ); Filter fooAndBar = filter( where(\u0026#34;foo\u0026#34;).exists(true)).and(where(\u0026#34;bar\u0026#34;).exists(true) ); Roll Your Own Third option is to implement your own predicates\n1 2 3 4 5 6 7 8 9 Predicate booksWithISBN = new Predicate() { @Override public boolean apply(PredicateContext ctx) { return ctx.item(Map.class).containsKey(\u0026#34;isbn\u0026#34;); } }; List\u0026lt;Map\u0026lt;String, Object\u0026gt;\u0026gt; books = reader.read(\u0026#34;$.store.book[?].isbn\u0026#34;, List.class, booksWithISBN); Path vs Value In the Goessner implementation a JsonPath can return either Path or Value. Value is the default and what all the examples above are returning. If you rather have the path of the elements our query is hitting this can be achieved with an option.\n可以指定option来返回path（默认返回value）\n1 2 3 4 5 6 7 8 9 10 Configuration conf = Configuration.builder() .options(Option.AS_PATH_LIST).build(); List\u0026lt;String\u0026gt; pathList = using(conf).parse(json).read(\u0026#34;$..author\u0026#34;); assertThat(pathList).containsExactly( \u0026#34;$[\u0026#39;store\u0026#39;][\u0026#39;book\u0026#39;][0][\u0026#39;author\u0026#39;]\u0026#34;, \u0026#34;$[\u0026#39;store\u0026#39;][\u0026#39;book\u0026#39;][1][\u0026#39;author\u0026#39;]\u0026#34;, \u0026#34;$[\u0026#39;store\u0026#39;][\u0026#39;book\u0026#39;][2][\u0026#39;author\u0026#39;]\u0026#34;, \u0026#34;$[\u0026#39;store\u0026#39;][\u0026#39;book\u0026#39;][3][\u0026#39;author\u0026#39;]\u0026#34;); Set a value The library offers the possibility to set a value.\n1 String newJson = JsonPath.parse(json).set(\u0026#34;$[\u0026#39;store\u0026#39;][\u0026#39;book\u0026#39;][0][\u0026#39;author\u0026#39;]\u0026#34;, \u0026#34;Paul\u0026#34;).jsonString(); Tweaking Configuration Options When creating your Configuration there are a few option flags that can alter the default behaviour.\nDEFAULT_PATH_LEAF_TO_NULL This option makes JsonPath return null for missing leafs. Consider the following json\n不存在的key返回NULL\n1 2 3 4 5 6 7 8 9 [ { \u0026#34;name\u0026#34; : \u0026#34;john\u0026#34;, \u0026#34;gender\u0026#34; : \u0026#34;male\u0026#34; }, { \u0026#34;name\u0026#34; : \u0026#34;ben\u0026#34; } ] 1 2 3 4 5 6 7 8 9 10 11 12 13 Configuration conf = Configuration.defaultConfiguration(); //Works fine String gender0 = JsonPath.using(conf).parse(json).read(\u0026#34;$[0][\u0026#39;gender\u0026#39;]\u0026#34;); //PathNotFoundException thrown String gender1 = JsonPath.using(conf).parse(json).read(\u0026#34;$[1][\u0026#39;gender\u0026#39;]\u0026#34;); Configuration conf2 = conf.addOptions(Option.DEFAULT_PATH_LEAF_TO_NULL); //Works fine String gender0 = JsonPath.using(conf2).parse(json).read(\u0026#34;$[0][\u0026#39;gender\u0026#39;]\u0026#34;); //Works fine (null is returned) String gender1 = JsonPath.using(conf2).parse(json).read(\u0026#34;$[1][\u0026#39;gender\u0026#39;]\u0026#34;); ALWAYS_RETURN_LIST This option configures JsonPath to return a list even when the path is definite.\n永远返回List\n1 2 3 4 5 6 7 8 9 Configuration conf = Configuration.defaultConfiguration(); //ClassCastException thrown List\u0026lt;String\u0026gt; genders0 = JsonPath.using(conf).parse(json).read(\u0026#34;$[0][\u0026#39;gender\u0026#39;]\u0026#34;); Configuration conf2 = conf.addOptions(Option.ALWAYS_RETURN_LIST); //Works fine List\u0026lt;String\u0026gt; genders0 = JsonPath.using(conf2).parse(json).read(\u0026#34;$[0][\u0026#39;gender\u0026#39;]\u0026#34;); SUPPRESS_EXCEPTIONS This option makes sure no exceptions are propagated from path evaluation. It follows these simple rules:\n路径计算不返回异常\nIf option ALWAYS_RETURN_LIST is present an empty list will be returned If option ALWAYS_RETURN_LIST is NOT present null returned REQUIRE_PROPERTIES This option configures JsonPath to require properties defined in path when an indefinite path is evaluated.\n必须绝对路径\n1 2 3 4 5 6 7 8 9 Configuration conf = Configuration.defaultConfiguration(); //Works fine List\u0026lt;String\u0026gt; genders = JsonPath.using(conf).parse(json).read(\u0026#34;$[*][\u0026#39;gender\u0026#39;]\u0026#34;); Configuration conf2 = conf.addOptions(Option.REQUIRE_PROPERTIES); //PathNotFoundException thrown List\u0026lt;String\u0026gt; genders = JsonPath.using(conf2).parse(json).read(\u0026#34;$[*][\u0026#39;gender\u0026#39;]\u0026#34;); JsonProvider SPI JsonPath is shipped with five different JsonProviders:\nJsonSmartJsonProvider (default) JacksonJsonProvider JacksonJsonNodeJsonProvider GsonJsonProvider JsonOrgJsonProvider Changing the configuration defaults as demonstrated should only be done when your application is being initialized. Changes during runtime is strongly discouraged, especially in multi threaded applications.\n仅在初始化应用程序时才应按照演示更改配置默认值。强烈建议不要在运行时进行更改，尤其是在多线程应用程序中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Configuration.setDefaults(new Configuration.Defaults() { private final JsonProvider jsonProvider = new JacksonJsonProvider(); private final MappingProvider mappingProvider = new JacksonMappingProvider(); @Override public JsonProvider jsonProvider() { return jsonProvider; } @Override public MappingProvider mappingProvider() { return mappingProvider; } @Override public Set\u0026lt;Option\u0026gt; options() { return EnumSet.noneOf(Option.class); } }); Note that the JacksonJsonProvider requires com.fasterxml.jackson.core:jackson-databind:2.4.5 and the GsonJsonProvider requires com.google.code.gson:gson:2.3.1 on your classpath.\nCache SPI In JsonPath 2.1.0 a new Cache SPI was introduced. This allows API consumers to configure path caching in a way that suits their needs. The cache must be configured before it is accesses for the first time or a JsonPathException is thrown. JsonPath ships with two cache implementations\ncom.jayway.jsonpath.spi.cache.LRUCache (default, thread safe) com.jayway.jsonpath.spi.cache.NOOPCache (no cache) If you want to implement your own cache the API is simple.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 CacheProvider.setCache(new Cache() { //Not thread safe simple cache private Map\u0026lt;String, JsonPath\u0026gt; map = new HashMap\u0026lt;String, JsonPath\u0026gt;(); @Override public JsonPath get(String key) { return map.get(key); } @Override public void put(String key, JsonPath jsonPath) { map.put(key, jsonPath); } }); ","date":"2021-06-04T17:02:13Z","permalink":"https://MyLoveES.github.io/p/jsonpath-guide-read/","title":"JSONPATH guide read"},{"content":"Shell 基础 variables variable meaning $$ Shell本身的PID（ProcessID） $! Shell最后运行的后台Process的PID $? 最后运行的命令的结束代码（返回值）,上一个命令执行后的返回结果。是显示最后命令的退出状态，0表示没有错误，其他表示有错误 $- 使用Set命令设定的Flag一览 $* 所有参数列表。是以一个单字符串显示所有向脚本传递的参数，与位置变量不同，参数可超过9个.如\u0026quot;$*\u0026ldquo;用「\u0026quot;」括起来的情况、以\u0026rdquo;$1 $2 … $n\u0026quot;的形式输出所有参数。 $@ 所有参数列表。如\u0026quot;$@\u0026ldquo;用「\u0026quot;」括起来的情况、以\u0026rdquo;$1\u0026quot; \u0026ldquo;$2\u0026rdquo; … \u0026ldquo;$n\u0026rdquo; 的形式输出所有参数。 $# 添加到Shell的参数个数 $0 Shell本身的文件名 $1～$n 添加到Shell的各参数值。$1是第1参数、$2是第2参数…。 symnbols meaning -eq 等于 equal -ne 不等于 not equal -gt 大于 greater -lt 小于 less -ge 大于等于 greater or equal -le 小于等于 less or equal ","date":"2021-02-20T12:00:00Z","permalink":"https://MyLoveES.github.io/p/hi-linux-shell/","title":"Hi, Linux shell"},{"content":"layout: post title: \u0026ldquo;Hi, Linux commands\u0026rdquo; subtitle: \u0026quot; \u0026quot;Hi, Linux commands\u0026quot;\u0026quot; date: 2021-02-21 17:00:00 author: \u0026ldquo;ruili\u0026rdquo; header-img: \u0026ldquo;img/post-bg-2015.jpg\u0026rdquo; catalog: true categories:\n\u0026ldquo;技术\u0026rdquo; tags: \u0026ldquo;设计模式\u0026rdquo; \u0026ldquo;命令模式\u0026rdquo; 常用 Shell 命令 1. 文本检索 wc wc -c the byte counts -m the character counts -l the newline counts -L the maximum display width -w the word counts 2. 重定向 在shell脚本中，默认情况下，总是有三个文件处于打开状态：\n标准输入(键盘输入)、标准输出（输出到屏幕）、标准错误（也是输出到屏幕），它们分别对应的文件描述符是0，1，2\n\u0026gt; 默认为标准输出重定向，与 1\u0026gt; 相同\n2\u0026gt;\u0026amp;1 意思是把 标准错误输出 重定向到 标准输出.\n\u0026amp;\u0026gt;file 意思是把标准输出 和 标准错误输出 都重定向到文件file中\n/dev/null是一个文件，这个文件比较特殊，所有传给它的东西它都丢弃掉\n3. 运算符 =~ 其中 ~ 其实是对后面的正则表达式表示匹配的意思，如果匹配就输出1，不匹配就输出0. == 是否相等 例子： [[ $test =~ ^[0-9]+ ]] \u0026amp;\u0026amp; echo 1 || echo 0\n4. awk -F fs or \u0026ndash;field-separator fs\n指定输入文件折分隔符，fs是一个字符串或者是一个正则表达式，如-F:。 -v var=value or \u0026ndash;asign var=value 赋值一个用户定义变量。 -f scripfile or \u0026ndash;file scriptfile\n从脚本文件中读取awk命令。 -mf nnn and -mr nnn\n对nnn值设置内在限制，-mf选项限制分配给nnn的最大块数目；-mr选项限制记录的最大数目。这两个功能是Bell实验室版awk的扩展功能，在标准awk中不适用。 -W compact or \u0026ndash;compat, -W traditional or \u0026ndash;traditional 在兼容模式下运行awk。所以gawk的行为和标准的awk完全一样，所有的awk扩展都被忽略。 -W copyleft or \u0026ndash;copyleft, -W copyright or \u0026ndash;copyright 打印简短的版权信息。 -W help or \u0026ndash;help, -W usage or \u0026ndash;usage 打印全部awk选项和每个选项的简短说明。 -W lint or \u0026ndash;lint 打印不能向传统unix平台移植的结构的警告。 -W lint-old or \u0026ndash;lint-old 打印关于不能向传统unix平台移植的结构的警告。 -W posix 打开兼容模式。但有以下限制，不识别：/x、函数关键字、func、换码序列以及当fs是一个空格时，将新行作为一个域分隔符；操作符和=不能代替^和^=；fflush无效。 -W re-interval or \u0026ndash;re-inerval 允许间隔正则表达式的使用，参考(grep中的Posix字符类)，如括号表达式[[:alpha:]]。 -W source program-text or \u0026ndash;source program-text 使用program-text作为源代码，可与-f命令混用。 -W version or \u0026ndash;version 打印bug报告信息的版本。 用法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 2 this is a test 3 Are you like awk This\u0026#39;s a test 10 There are orange,apple,mongo 1. awk \u0026#39;{[pattern] action}\u0026#39; {filenames} # 行匹配语句 awk \u0026#39;\u0026#39; 只能用单引号 $ awk \u0026#39;{print $1,$4}\u0026#39; log.txt --------------------------------------------- 2 a 3 like This\u0026#39;s 10 orange,apple,mongo # 格式化输出 $ awk \u0026#39;{printf \u0026#34;%-8s %-10s\\n\u0026#34;,$1,$4}\u0026#39; log.txt --------------------------------------------- 2 a 3 like This\u0026#39;s 10 orange,apple,mongo 2. awk -F #-F相当于内置变量FS, 指定分割字符 # 使用\u0026#34;,\u0026#34;分割 $ awk -F, \u0026#39;{print $1,$2}\u0026#39; log.txt --------------------------------------------- 2 this is a test 3 Are you like awk This\u0026#39;s a test 10 There are orange apple # 或者使用内建变量 $ awk \u0026#39;BEGIN{FS=\u0026#34;,\u0026#34;} {print $1,$2}\u0026#39; log.txt --------------------------------------------- 2 this is a test 3 Are you like awk This\u0026#39;s a test 10 There are orange apple # 使用多个分隔符.先使用空格分割，然后对分割结果再使用\u0026#34;,\u0026#34;分割 $ awk -F \u0026#39;[ ,]\u0026#39; \u0026#39;{print $1,$2,$5}\u0026#39; log.txt --------------------------------------------- 2 this test 3 Are awk This\u0026#39;s a 10 There apple 3. awk -v # 设置变量 $ awk -va=1 \u0026#39;{print $1,$1+a}\u0026#39; log.txt --------------------------------------------- 2 3 3 4 This\u0026#39;s 1 10 11 $ awk -va=1 -vb=s \u0026#39;{print $1,$1+a,$1b}\u0026#39; log.txt --------------------------------------------- 2 3 2s 3 4 3s This\u0026#39;s 1 This\u0026#39;ss 10 11 10s 4. awk -f {awk脚本} {文件名} awk -f cal.awk log.txt 5. 过滤第一列大于2并且第二列等于\u0026#39;Are\u0026#39;的行 $ awk \u0026#39;$1\u0026gt;2 \u0026amp;\u0026amp; $2==\u0026#34;Are\u0026#34; {print $1,$2,$3}\u0026#39; log.txt #命令 #输出 3 Are you 6. 计算大小 awk \u0026#39;{sum+=$5} END {print sum}\u0026#39; 7. 从文件中找出长度大于 80 的行： awk \u0026#39;length\u0026gt;80\u0026#39; log.txt nc 1 2 3 4 5 6 7 8 9 10 通过 nc 传输文件和目录 目录： 接收端：nc -l 7777 | tar xf - 发送端：tar cf - ${dir_name} | nc 192.168.1.1 7777 文件： 往10.105.135.50 上传输文件： 接收端：nc -l 9999 \u0026gt; 123 发送端：nc 10.105.135.50 9999 \u0026lt; 123 ","date":"0001-01-01T00:00:00Z","permalink":"https://MyLoveES.github.io/p/","title":""}]